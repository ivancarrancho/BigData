[ns_server:info,2019-07-04T11:52:27.698Z,nonode@nohost:<0.89.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2019-07-04T11:52:27.713Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10},
 {ipv6,false}]
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.714Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:warn,2019-07-04T11:52:27.715Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter ipv6, which is given from command line
[error_logger:info,2019-07-04T11:52:27.740Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.138.0>},
                       {name,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:27.750Z,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,9,125}
Runtime info: [{otp_release,"R16B03-1"},
               {erl_version,"5.10.4.0.0.1"},
               {erl_version_long,
                   "Erlang R16B03-1 (erts-5.10.4.0.0.1) [source-6d69bef] [64-bit] [smp:2:2] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2019,7,4},{11,52,27}}},
               {memory,
                   [{total,110192240},
                    {processes,9349168},
                    {processes_used,9348080},
                    {system,100843072},
                    {atom,339441},
                    {atom_used,322567},
                    {binary,51608},
                    {code,7796127},
                    {ets,2241496}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,path_config,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user',io_lib_fread,'ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',otp_internal,
                    ns_log_sink,ale_disk_sink,misc,couch_util,ns_server,
                    filelib,cpu_sup,memsup,disksup,os_mon,io,release_handler,
                    overload,alarm_handler,sasl,timer,tftp_sup,httpd_sup,
                    httpc_handler_sup,httpc_cookie,inets_trace,httpc_manager,
                    httpc,httpc_profile_sup,httpc_sup,ftp_sup,inets_sup,
                    inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    tls_connection_sup,ssl_session_cache,ssl_pkix_db,
                    ssl_manager,ssl_sup,ssl_app,crypto_server,crypto_sup,
                    crypto_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_validator,beam_z,
                    beam_flatten,beam_trim,beam_receive,beam_bsm,beam_peep,
                    beam_dead,beam_split,beam_type,beam_bool,beam_except,
                    beam_clean,beam_utils,beam_block,beam_jump,beam_a,
                    v3_codegen,v3_life,v3_kernel,sys_core_dsetel,erl_bifs,
                    sys_core_fold,cerl_trees,sys_core_inline,core_lib,cerl,
                    v3_core,erl_bits,erl_expand_records,sys_pre_expand,sofs,
                    erl_internal,sets,ordsets,erl_lint,compile,
                    dynamic_compile,ale_utils,io_lib_pretty,io_lib_format,
                    io_lib,ale_codegen,dict,ale,ale_dynamic_sup,ale_sup,
                    ale_app,epp,ns_bootstrap,child_erlang,file_io_server,
                    orddict,erl_eval,file,c,kernel_config,user_io,user_sup,
                    supervisor_bridge,standard_error,code_server,unicode,
                    hipe_unified_loader,gb_sets,ets,binary,code,file_server,
                    net_kernel,global_group,erl_distribution,filename,
                    inet_gethost_native,os,inet_parse,inet,inet_udp,
                    inet_config,inet_db,global,gb_trees,rpc,supervisor,kernel,
                    application_master,sys,application,gen_server,erl_parse,
                    proplists,erl_scan,lists,application_controller,proc_lib,
                    gen,gen_event,error_logger,heart,error_handler,
                    erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {os_mon,"CPO  CXC 138 46","2.2.14"},
                    {public_key,"Public key infrastructure","0.21"},
                    {asn1,"The Erlang ASN1 compiler version 2.0.4","2.0.4"},
                    {kernel,"ERTS  CXC 138 10","2.16.4"},
                    {ale,"Another Logger for Erlang","6.0.0-1693-community"},
                    {inets,"INETS  CXC 138 49","5.9.8"},
                    {ns_server,"Couchbase server","6.0.0-1693-community"},
                    {crypto,"CRYPTO version 2","3.2"},
                    {ssl,"Erlang/OTP SSL application","5.3.3"},
                    {sasl,"SASL  CXC 138 11","2.3.4"},
                    {stdlib,"ERTS  CXC 138 10","1.19.4"}]},
               {pre_loaded,
                   [erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {process_count,105},
               {node,nonode@nohost},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,code_server,ale_stats_events,
                    ns_server_cluster_sup,lhttpc_manager,
                    application_controller,ale,'sink-ns_log',httpd_sup,
                    release_handler,kernel_safe_sup,standard_error,ale_sup,
                    overload,error_logger,'sink-disk_json_rpc',alarm_handler,
                    ale_dynamic_sup,'sink-disk_metakv',timer_server,
                    standard_error_sup,'sink-disk_access_int',
                    'sink-disk_access',crypto_server,'sink-disk_reports',
                    crypto_sup,sasl_safe_sup,'sink-disk_stats',tftp_sup,
                    'sink-disk_xdcr',inet_db,init,os_mon_sup,rex,
                    'sink-disk_debug',tls_connection_sup,user,ssl_sup,
                    kernel_sup,cpu_sup,'sink-disk_error',global_name_server,
                    memsup,disksup,'sink-disk_default',httpc_sup,
                    file_server_2,ssl_manager,local_tasks,global_group,
                    httpc_profile_sup,httpc_manager,httpc_handler_sup,ftp_sup,
                    sasl_sup,erl_prim_loader,inets_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,2}]
[ns_server:info,2019-07-04T11:52:27.762Z,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"d200fc0f68695d4aef1fad5c3c8cc55f8c033014\" upstream=\"refs/tags/0.9.7\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"a5567811193b0cc3571fe94e42fc1b8a6a80bc5b\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"10233dcde760b61f4ffac0479bc3a8cabff73beb\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"faa4390d57116ccbdcfc8f00e3affd3044a890cc\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"9e3465560240ffb242b50a47cb7f19251a12ee42\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"055db35bf221ccdc62363f1c4ad88eaac2b892ab\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"f551b6d4f32bb920a83dd28c705bddd5de0d03b2\" />",
 "  <project name=\"blevex\" path=\"godeps/src/github.com/blevesearch/blevex\" remote=\"blevesearch\" revision=\"4b158bb555a3297565afecf6fae675c74f1e47df\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"523d6077a2ec14038605cf8a1feeecaa29c44deb\" upstream=\"alice\">",
 "    <annotation name=\"RELEASE\" value=\"alice\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"1693\" />",
 "    <annotation name=\"VERSION\" value=\"6.0.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbaselabs/cbas\" revision=\"b1f12f65c27f72f582a291ef5a4b72c7a5bf1af0\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"0f3911da8789ba9436962fac63e6928c60f46e6c\" upstream=\"alice\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"78fd5f8ce545e6082271e74cd9f85aa8b8fbbe0d\" upstream=\"alice\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"0df84c7e3c6d95ff435c12a3c08c6f064db11e97\" />",
 "  <project name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"80d2ad8892d806f5103f602fec0d80adaa4b628f\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"a33ad7b7000a9d8d237ba273c47cc100401a0fb0\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"0a94f40b9080e0ecb11d3b7531a58c5e6a4a4465\" upstream=\"master\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbq-gui\" path=\"goproj/src/github.com/couchbase/cbq-gui\" remote=\"couchbase-priv\" revision=\"19fecfe58921c162a31c156781bd2a512711f14d\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"dbfa1c0d73f0e49f6f04e390f03de8f9a6cee769\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"dcae66272b24600ae0005fa06b511cfae8914d3d\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project name=\"couchbase-cli\" revision=\"d04a2983f3f014442d2ec1132bb505aa6c025dc3\" upstream=\"alice\" />",
 "  <project name=\"couchdb\" revision=\"45731d9f42d8046f8ba9ccb754657eb2996b5c4a\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"c545a8563778bfc40284caf9213c7925488e633a\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"3b4c35d79a35756c26ae547e0759b8ef08aa8438\" upstream=\"vulcan\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"f23ba3a5ee43012fcb4b92e1a2a405a92554f4f2\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"05067021a042a1b63e100a486afd7ebddab4c535\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"e84dd5be6c2b899e77bd3fc24a930cc2bcf9188d\" upstream=\"alice\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"kv\" name=\"forestdb\" revision=\"562366039e50730282548b02c1a30d73f97cba27\" upstream=\"vulcan\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"2a0e73f43451045f157640eec59ced72da18471f\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"9b3739952a0900be7628424082559d41dc3cd0d1\" upstream=\"alice\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"3b0453ce6faae42ab4d8cdb9ac1f93919c9d8d69\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"47fc4e5e9153645da45af6a86a5bce95e63a0f9e\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d2f15a425a9c8e4d8447e5f5b89ce14845f7fa05\" upstream=\"vulcan\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb\" revision=\"699b13a51af5dd4f80ff3deedf41bba60debad32\" upstream=\"refs/tags/v1.3.7\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"710456e087a6d497e87f41d0a9d98d6a75672186\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"a0d26c2d6f5de912499d35a5aba573006e5e036f\" upstream=\"refs/tags/v7.1.7\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"aecdbe5a5a91f0688df7bdf260ca962178c06828\" upstream=\"vulcan\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"0da75df145308b9a4e6704d762ca9d9b77752efc\" upstream=\"vulcan\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"1e3589e665a728ec9a2c64b516fd26f52ac2663a\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"f98adca8eb365032cab838ef4d99453931afa112\" upstream=\"vulcan\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"a465a37b72784b21dde0290235a9066147fbb12f\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"28aa45915e50214577e4a7810a2a508c1d17934b\" upstream=\"alice\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"d0c17cc8a803812c2d2a304479cc3a0b200c9aba\" upstream=\"alice\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"081e8b16b991bf706eb77f8243935c6fba31b895\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"3681c2a912330352991ecdd642f257efe5b85518\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"956632ec1bc3e28276d00ee2f22c3202f06efb12\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"abd3b58b422dbc2e9463a589d0f3d93441726e23\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project groups=\"kv\" name=\"moxi\" revision=\"cd8da46b9b953800d430c8b0aa4667790727ed6f\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"62685c2d7ca23c807425dca88b11a3e2323dab41\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"f3bef3551997be504612a2d05a8b324b3bfdfe1b\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"43a2cac6976489bb79896f09695f2af2d9b53857\" upstream=\"alice\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"96501c57bb0fd61c85cba6f63101aed2bcf41d38\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"30badd9e911df0e6dd28e4eec2949fe144c0235c\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"2fbe5179a2673a9275cd0906daa4b1cab38a3eb5\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"55e45187ca8943aa8910e9ae2b59b41242d14386\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"655cdfa588ea190e901bc5590e65d5621688847c\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"df2438af8bb18ba35e599caa1a7afe2eab2c5137\" upstream=\"alice\" />",
 "  <project name=\"query-ui\" revision=\"15a6461f437fe810e411a6613ec7c143991cd1c6\" upstream=\"alice\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"fe09428be4c233d726797a1380f7438f4f71a31a\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"73353fe6dad8f3d67409feefb9b17f90f6de917b\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"4fba14c79f356ae48d6141c561bf9fd7ba33fabd\" upstream=\"refs/tags/v0.14.0\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"c30c3d4c250e68e81c57aa1e8ae91ffd21243cdb\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"9d4e42a20653790449273b3c85e67d6d8bae6e2e\" />",
 "  <project name=\"testrunner\" revision=\"d8f6c71dd26932f304c281645267c31146ef3e1c\" upstream=\"alice\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"601048ad6acbab6cedd582db09b8c4839ff25b15\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"b277d99e18b0ad625405c4cf1ec79af8c94710c7\" upstream=\"alice\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"0ceea4a37442f76199b9259840baf48d17af3c1a\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"5d12c10af88bec4e655ca358aaa3e6d60193a082\" upstream=\"alice\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2019-07-04T11:52:27.817Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.139.0>},
                       {name,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:27.822Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:86]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2019-07-04T11:52:27.823Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:86]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2019-07-04T11:52:27.824Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:init:163]ip config not found. Looks like we're brand new node
[error_logger:info,2019-07-04T11:52:27.824Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.143.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2019-07-04T11:52:27.826Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.142.0>},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:28.246Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:bringup:215]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2019-07-04T11:52:28.259Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.145.0>},
                       {name,erl_epmd},
                       {mfargs,{erl_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.260Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.146.0>},
                       {name,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.261Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.147.0>},
                       {name,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames]]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.261Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.144.0>},
                       {name,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:28.261Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:configure_net_kernel:259]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2019-07-04T11:52:28.266Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:save_node:147]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2019-07-04T11:52:28.273Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:bringup:229]Attempted to save node name to disk: ok
[ns_server:debug,2019-07-04T11:52:28.273Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:wait_for_node:236]Waiting for connection to node 'babysitter_of_ns_1@127.0.0.1' to be established
[error_logger:info,2019-07-04T11:52:28.274Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:28.285Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:wait_for_node:248]Observed node 'babysitter_of_ns_1@127.0.0.1' to come up
[error_logger:info,2019-07-04T11:52:28.286Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.141.0>},
                       {name,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.292Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.152.0>},
                       {name,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.299Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.153.0>},
                       {name,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:28.303Z,ns_1@127.0.0.1:ns_config_sup<0.154.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2019-07-04T11:52:28.303Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.155.0>},
                       {name,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.303Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.156.0>},
                       {name,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:28.378Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1095]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2019-07-04T11:52:28.385Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1109]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2019-07-04T11:52:28.386Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1114]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2019-07-04T11:52:28.398Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1117]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2019-07-04T11:52:28.404Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1138]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   false]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {port,11211},
   {verbosity,[]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1024},
 {fts_memory_quota,256},
 {memory_quota,319},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {maxconn,maxconn},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {maxconn,dedicated_port_maxconn},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {maxconn,maxconn},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]}]}},
     {ssl_cipher_list,{"~s",[ssl_cipher_list]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,{memcached_config_mgr,is_enabled,[[5,0]]}},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {port,11210},
   {dedicated_port,11209},
   {ssl_port,undefined},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {maxconn,30000},
   {dedicated_port_maxconn,5000},
   {ssl_cipher_list,"HIGH"},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,false},
   {datatype_snappy,true}]},
 {memcached,[]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {read_only_user_creds,null},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912}">>},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {port,8091},
   {port_meta,global}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {rest,[{port,8091}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,10},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@127.0.0.1',ldap_enabled},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   {5,5,3}]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   <<"2a8ae539a5cab1af4159b9b57e0098ee">>]}]
[error_logger:info,2019-07-04T11:52:28.416Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.157.0>},
                       {name,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.439Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.163.0>},
                       {name,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.445Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.164.0>},
                       {name,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.445Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.154.0>},
                       {name,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:28.452Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.166.0>},
                       {name,vbucket_filter_changes_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               [vbucket_filter_changes_registry,
                                [{terminate_command,shutdown}]]}},
                       {restart_type,permanent},
                       {shutdown,100},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.456Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.167.0>},
                       {name,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:28.475Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.170.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:28.479Z,ns_1@127.0.0.1:menelaus_barrier<0.171.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2019-07-04T11:52:28.479Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.171.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.480Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.172.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.486Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.173.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.489Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.175.0>},
                       {name,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.500Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.174.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:28.503Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.178.0>},
                       {name,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.513Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.180.0>},
                       {name,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:28.517Z,ns_1@127.0.0.1:users_replicator<0.180.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:52:28.522Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_storage:anounce_startup:69]Announce my startup to <0.180.0>
[ns_server:debug,2019-07-04T11:52:28.522Z,ns_1@127.0.0.1:users_replicator<0.180.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <0.181.0>
[ns_server:debug,2019-07-04T11:52:28.528Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:open:212]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2019-07-04T11:52:28.529Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.181.0>},
                       {name,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.529Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.179.0>},
                       {name,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:28.550Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:init:44]Starting versioned cache compiled_roles_cache
[error_logger:info,2019-07-04T11:52:28.550Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.183.0>},
                       {name,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:28.550Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.177.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:28.552Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.186.0>},
                       {name,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:28.553Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.187.0>},
                       {name,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:28.576Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:convert_docs_to_55_in_dets:243]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2019-07-04T11:52:28.577Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,'$1','_','$2'},
                                      [],
                                      [{{'$1','$2'}}]}],
                                    100}
[ns_server:debug,2019-07-04T11:52:28.578Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:init_after_ack:204]Loading 0 items, 299 words took 1ms
[ns_server:debug,2019-07-04T11:52:28.595Z,ns_1@127.0.0.1:users_replicator<0.180.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2019-07-04T11:52:28.607Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.190.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:28.607Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.191.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:141]Waiting for ns_couchdb node to start
[error_logger:info,2019-07-04T11:52:28.608Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:28.609Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:52:28.609Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.194.0>,shutdown}}
[error_logger:info,2019-07-04T11:52:28.609Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:28.810Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:28.813Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.197.0>,shutdown}}
[error_logger:info,2019-07-04T11:52:28.813Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:28.813Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:52:29.014Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:29.018Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.200.0>,shutdown}}
[error_logger:info,2019-07-04T11:52:29.018Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:29.018Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:52:29.219Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:29.221Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.203.0>,shutdown}}
[error_logger:info,2019-07-04T11:52:29.221Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:29.221Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:52:29.423Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:29.425Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.206.0>,shutdown}}
[error_logger:info,2019-07-04T11:52:29.425Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:29.425Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:52:29.627Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:29.629Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.209.0>,shutdown}}
[error_logger:info,2019-07-04T11:52:29.629Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:29.629Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:52:29.830Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:29.832Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.212.0>,shutdown}}
[ns_server:debug,2019-07-04T11:52:29.832Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:52:29.832Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:52:30.033Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:52:30.048Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:52:30.249Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:52:30.451Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:52:30.653Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:52:30.855Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:52:31.058Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:52:31.260Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[error_logger:info,2019-07-04T11:52:31.848Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.224.0>},
                       {name,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:32.049Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: Apache CouchDB v4.5.1-108-g45731d9 (LogLevel=info) is starting.

[error_logger:info,2019-07-04T11:52:32.678Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.191.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.96617950>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:32.702Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:ns_storage_conf:setup_db_and_ix_paths:47]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:info,2019-07-04T11:52:32.711Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.190.0>: 168: Booted. Waiting for shutdown request

[error_logger:info,2019-07-04T11:52:32.728Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.227.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:32.733Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.228.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:32.757Z,ns_1@127.0.0.1:ns_server_sup<0.226.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2019-07-04T11:52:32.757Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.229.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:32.760Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.230.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:52:32.765Z,ns_1@127.0.0.1:ns_log<0.231.0>:ns_log:read_logs:92]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first startup): {error,
                                                                                                 enoent}
[error_logger:info,2019-07-04T11:52:32.765Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.231.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:32.765Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.232.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:32.804Z,ns_1@127.0.0.1:memcached_passwords<0.233.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-07-04T11:52:32.837Z,ns_1@127.0.0.1:memcached_passwords<0.233.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-07-04T11:52:33.222Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2019-07-04T11:52:33.222Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.233.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:52:33.251Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:33.252Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:52:33.256Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:52:33.258Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:52:33.292Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2019-07-04T11:52:33.293Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.236.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:52:33.293Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:info,2019-07-04T11:52:33.293Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.239.0>},
                       {name,ns_log_events},
                       {mfargs,{gen_event,start_link,[{local,ns_log_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.293Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:52:33.326Z,ns_1@127.0.0.1:ns_node_disco<0.242.0>:ns_node_disco:init:130]Initting ns_node_disco with []
[error_logger:info,2019-07-04T11:52:33.326Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.241.0>},
                       {name,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.327Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[user:info,2019-07-04T11:52:33.327Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_init:83]Initial otp cookie generated: {sanitized,
                                  <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:52:33.327Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460353}}]}]
[ns_server:debug,2019-07-04T11:52:33.328Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
otp ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460353}}]},
 {cookie,{sanitized,<<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}}]
[ns_server:debug,2019-07-04T11:52:33.330Z,ns_1@127.0.0.1:<0.243.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:52:33.335Z,ns_1@127.0.0.1:<0.243.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[error_logger:info,2019-07-04T11:52:33.335Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.242.0>},
                       {name,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.340Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.245.0>},
                       {name,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.348Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.246.0>},
                       {name,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.354Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:init:69]init pulling
[error_logger:info,2019-07-04T11:52:33.354Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.247.0>},
                       {name,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.354Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:init:71]init pushing
[ns_server:debug,2019-07-04T11:52:33.367Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:init:75]init reannouncing
[ns_server:debug,2019-07-04T11:52:33.367Z,ns_1@127.0.0.1:ns_config_events<0.155.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2019-07-04T11:52:33.367Z,ns_1@127.0.0.1:ns_config_events<0.155.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2019-07-04T11:52:33.367Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              3359036101},
                                                                             false,
                                                                             []}
[ns_server:debug,2019-07-04T11:52:33.368Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[ns_server:debug,2019-07-04T11:52:33.368Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[ns_server:debug,2019-07-04T11:52:33.369Z,ns_1@127.0.0.1:<0.256.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:52:33.369Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
otp ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460353}}]},
 {cookie,{sanitized,<<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}}]
[ns_server:debug,2019-07-04T11:52:33.370Z,ns_1@127.0.0.1:<0.257.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:52:33.371Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2019-07-04T11:52:33.371Z,ns_1@127.0.0.1:<0.256.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:52:33.372Z,ns_1@127.0.0.1:<0.257.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:52:33.372Z,ns_1@127.0.0.1:memcached_passwords<0.233.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-07-04T11:52:33.371Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2019-07-04T11:52:33.373Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2019-07-04T11:52:33.374Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2019-07-04T11:52:33.374Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:52:33.374Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[],{configs,[]}]
[ns_server:debug,2019-07-04T11:52:33.374Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cbas_memory_quota ->
1024
[ns_server:debug,2019-07-04T11:52:33.374Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
fts_memory_quota ->
256
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
max_bucket_count ->
10
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memcached ->
[]
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memory_quota ->
319
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
nodes_wanted ->
['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
quorum_nodes ->
['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
read_only_user_creds ->
null
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
remote_clusters ->
[]
[ns_server:debug,2019-07-04T11:52:33.375Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2019-07-04T11:52:33.376Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest ->
[{port,8091}]
[ns_server:debug,2019-07-04T11:52:33.376Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest_creds ->
null
[ns_server:debug,2019-07-04T11:52:33.376Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
secure_headers ->
[]
[ns_server:debug,2019-07-04T11:52:33.376Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2019-07-04T11:52:33.397Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2019-07-04T11:52:33.397Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2019-07-04T11:52:33.397Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2019-07-04T11:52:33.398Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912}">>
[ns_server:debug,2019-07-04T11:52:33.398Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2019-07-04T11:52:33.398Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2019-07-04T11:52:33.398Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]
[ns_server:debug,2019-07-04T11:52:33.398Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8092]
[ns_server:debug,2019-07-04T11:52:33.398Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9110]
[ns_server:debug,2019-07-04T11:52:33.398Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9113]
[ns_server:debug,2019-07-04T11:52:33.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9112]
[ns_server:debug,2019-07-04T11:52:33.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9111]
[ns_server:debug,2019-07-04T11:52:33.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9115]
[ns_server:debug,2019-07-04T11:52:33.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9114]
[ns_server:debug,2019-07-04T11:52:33.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9116]
[ns_server:debug,2019-07-04T11:52:33.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|-1]
[ns_server:debug,2019-07-04T11:52:33.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8095]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9118]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9119]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9121]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9122]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9120]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9117]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2019-07-04T11:52:33.400Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 {5,5,3}]
[ns_server:debug,2019-07-04T11:52:33.401Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9140]
[ns_server:debug,2019-07-04T11:52:33.401Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8096]
[ns_server:debug,2019-07-04T11:52:33.401Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:52:33.401Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8094]
[ns_server:debug,2019-07-04T11:52:33.401Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:52:33.401Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9100]
[ns_server:debug,2019-07-04T11:52:33.401Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9102]
[ns_server:debug,2019-07-04T11:52:33.402Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:52:33.402Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9101]
[ns_server:debug,2019-07-04T11:52:33.402Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9104]
[ns_server:debug,2019-07-04T11:52:33.402Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9103]
[ns_server:debug,2019-07-04T11:52:33.404Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9105]
[ns_server:debug,2019-07-04T11:52:33.404Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|false]
[ns_server:debug,2019-07-04T11:52:33.404Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2019-07-04T11:52:33.404Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ldap_enabled} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|true]
[ns_server:debug,2019-07-04T11:52:33.404Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 active]
[ns_server:debug,2019-07-04T11:52:33.405Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {port,11210},
 {dedicated_port,11209},
 {ssl_port,undefined},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2019-07-04T11:52:33.436Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {maxconn,maxconn},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {maxconn,dedicated_port_maxconn},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {maxconn,maxconn},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]}]}},
   {ssl_cipher_list,{"~s",[ssl_cipher_list]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,{memcached_config_mgr,is_enabled,[[5,0]]}},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}}]}]
[ns_server:debug,2019-07-04T11:52:33.436Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {maxconn,30000},
 {dedicated_port_maxconn,5000},
 {ssl_cipher_list,"HIGH"},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,false},
 {datatype_snappy,true}]
[ns_server:debug,2019-07-04T11:52:33.437Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {port,11211},
 {verbosity,[]}]
[ns_server:debug,2019-07-04T11:52:33.437Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2019-07-04T11:52:33.437Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]
[ns_server:debug,2019-07-04T11:52:33.437Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9999]
[ns_server:debug,2019-07-04T11:52:33.437Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8093]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 <<"2a8ae539a5cab1af4159b9b57e0098ee">>]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9998]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|false]
[ns_server:debug,2019-07-04T11:52:33.438Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460353}}]}]
[error_logger:info,2019-07-04T11:52:33.440Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.248.0>},
                       {name,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.441Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.240.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:33.440Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,
                               read_only_user_creds,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port},
                               {node,'ns_1@127.0.0.1',indexer_http_port},
                               {node,'ns_1@127.0.0.1',indexer_https_port},
                               {node,'ns_1@127.0.0.1',indexer_scan_port},
                               {node,'ns_1@127.0.0.1',indexer_stcatchup_port},
                               {node,'ns_1@127.0.0.1',indexer_stinit_port}]..)
[error_logger:info,2019-07-04T11:52:33.445Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.264.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.478Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.266.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.478Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.269.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.478Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.270.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.528Z,ns_1@127.0.0.1:ns_log_events<0.239.0>:ns_mail_log:init:44]ns_mail_log started up
[error_logger:info,2019-07-04T11:52:33.528Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_mail_sup}
             started: [{pid,<0.272.0>},
                       {name,ns_mail_log},
                       {mfargs,{ns_mail_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.528Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.271.0>},
                       {name,ns_mail_sup},
                       {mfargs,{ns_mail_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:33.528Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.273.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.274.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.586Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.276.0>},
                       {name,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.587Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.280.0>},
                       {name,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.587Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.275.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:33.594Z,ns_1@127.0.0.1:ns_heart<0.276.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2019-07-04T11:52:33.594Z,ns_1@127.0.0.1:ns_heart<0.276.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[error_logger:info,2019-07-04T11:52:33.608Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.285.0>},
                       {name,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.651Z,ns_1@127.0.0.1:<0.283.0>:restartable:start_child:98]Started child process <0.284.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2019-07-04T11:52:33.651Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.287.0>},
                       {name,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.652Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.283.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:33.652Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.290.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.686Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.291.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.687Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.292.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.688Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.293.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.691Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.294.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.705Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.296.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.731Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.298.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.731Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.300.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.751Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.303.0>},
                       {name,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.751Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.305.0>},
                       {name,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.763Z,ns_1@127.0.0.1:ns_heart<0.276.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2019-07-04T11:52:33.767Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.306.0>},
                       {name,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.773Z,ns_1@127.0.0.1:ns_heart<0.276.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:45]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2019-07-04T11:52:33.780Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.308.0>},
                       {name,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.787Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.309.0>},
                       {name,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.788Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.310.0>},
                       {name,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:33.795Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.280.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,239}]}]}}

[ns_server:debug,2019-07-04T11:52:33.796Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.280.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:info,2019-07-04T11:52:33.797Z,ns_1@127.0.0.1:menelaus_sup<0.301.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts

[ns_server:debug,2019-07-04T11:52:33.799Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2019-07-04T11:52:33.800Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.280.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:info,2019-07-04T11:52:33.800Z,ns_1@127.0.0.1:menelaus_sup<0.301.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql

[ns_server:debug,2019-07-04T11:52:33.800Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.280.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:45]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:warn,2019-07-04T11:52:33.800Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:33.801Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:52:33.867Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.328.0>},
                       {name,menelaus_web},
                       {mfargs,{menelaus_web,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.872Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.345.0>},
                       {name,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.877Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.346.0>},
                       {name,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.882Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.347.0>},
                       {name,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.888Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.348.0>},
                       {name,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2019-07-04T11:52:33.889Z,ns_1@127.0.0.1:ns_server_sup<0.226.0>:menelaus_sup:start_link:46]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.0.0-1693-community".
[error_logger:info,2019-07-04T11:52:33.889Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.301.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:33.889Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.354.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.894Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.358.0>},
                       {name,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:33.895Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.359.0>},
                       {name,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.31986353>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:33.896Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.357.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:33.901Z,ns_1@127.0.0.1:ns_ports_setup<0.354.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:info,2019-07-04T11:52:33.930Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: working as port

[error_logger:info,2019-07-04T11:52:34.000Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.361.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.001Z,ns_1@127.0.0.1:ns_audit_cfg<0.362.0>:ns_audit_cfg:write_audit_json:265]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json" : [{descriptors_path,
                                                                                <<"/opt/couchbase/etc/security">>},
                                                                               {version,
                                                                                1},
                                                                               {auditd_enabled,
                                                                                false},
                                                                               {disabled,
                                                                                []},
                                                                               {log_path,
                                                                                <<"/opt/couchbase/var/lib/couchbase/logs">>},
                                                                               {rotate_interval,
                                                                                86400},
                                                                               {rotate_size,
                                                                                20971520},
                                                                               {sync,
                                                                                []}]
[ns_server:debug,2019-07-04T11:52:34.014Z,ns_1@127.0.0.1:ns_audit_cfg<0.362.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2019-07-04T11:52:34.014Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.362.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:52:34.017Z,ns_1@127.0.0.1:<0.365.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2019-07-04T11:52:34.040Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.366.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.044Z,ns_1@127.0.0.1:memcached_config_mgr<0.367.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:52:34.044Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.367.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:34.050Z,ns_1@127.0.0.1:<0.368.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2019-07-04T11:52:34.050Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.368.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.060Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.369.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.064Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.371.0>},
                       {name,ns_bucket_worker},
                       {mfargs,{work_queue,start_link,[ns_bucket_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.070Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.373.0>},
                       {name,buckets_observing_subscription},
                       {mfargs,{ns_bucket_sup,subscribe_on_config_events,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.070Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.372.0>},
                       {name,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.070Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.370.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.078Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.374.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.083Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.378.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.103Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.380.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.109Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.381.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.110Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.383.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.114Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.384.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.114Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.386.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.127Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.387.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.130Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.389.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.131Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.391.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.136Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.392.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.140Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.394.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.141Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.394.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:52:34.142Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.394.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2019-07-04T11:52:34.146Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.398.0>},
                       {name,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.151Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.400.0>},
                       {name,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.153Z,ns_1@127.0.0.1:ns_ports_setup<0.354.0>:ns_ports_setup:set_children:90]Monitor ns_child_ports_sup <12395.72.0>
[ns_server:debug,2019-07-04T11:52:34.155Z,ns_1@127.0.0.1:memcached_config_mgr<0.367.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:34.176Z,ns_1@127.0.0.1:memcached_config_mgr<0.367.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:info,2019-07-04T11:52:34.181Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.403.0>},
                       {name,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.185Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.406.0>},
                       {name,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.204Z,ns_1@127.0.0.1:memcached_config_mgr<0.367.0>:memcached_config_mgr:init:79]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[error_logger:info,2019-07-04T11:52:34.204Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.410.0>},
                       {name,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.205Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.399.0>},
                       {name,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:34.207Z,ns_1@127.0.0.1:memcached_config_mgr<0.367.0>:memcached_config_mgr:init:82]activated memcached port server
[error_logger:info,2019-07-04T11:52:34.205Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.413.0>},
                       {name,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.41280346>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.211Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.397.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.228Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.415.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.246Z,ns_1@127.0.0.1:<0.418.0>:new_concurrency_throttle:init:113]init concurrent throttle process, pid: <0.418.0>, type: kv_throttle# of available token: 1
[ns_server:warn,2019-07-04T11:52:34.256Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:34.256Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:52:34.256Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2019-07-04T11:52:34.256Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:52:34.257Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2019-07-04T11:52:34.257Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:52:34.257Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_master. Rescheduling compaction.
[error_logger:info,2019-07-04T11:52:34.257Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.416.0>},
                       {name,compaction_new_daemon},
                       {mfargs,{compaction_new_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.257Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2019-07-04T11:52:34.261Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.420.0>},
                       {name,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.261Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.419.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.276Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.421.0>},
                       {name,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:52:34.310Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:34.310Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:52:34.326Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.425.0>},
                       {name,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.332Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.426.0>},
                       {name,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.332Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.424.0>},
                       {name,leader_leases_sup},
                       {mfargs,
                           {leader_services_sup,start_link,
                               [leader_leases_sup]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.333Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.428.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.386Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.429.0>},
                       {name,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.387Z,ns_1@127.0.0.1:leader_registry_sup<0.427.0>:mb_master:check_master_takeover_needed:133]Sending master node question to the following nodes: []
[ns_server:debug,2019-07-04T11:52:34.387Z,ns_1@127.0.0.1:leader_registry_sup<0.427.0>:mb_master:check_master_takeover_needed:135]Got replies: []
[ns_server:debug,2019-07-04T11:52:34.387Z,ns_1@127.0.0.1:leader_registry_sup<0.427.0>:mb_master:check_master_takeover_needed:141]Was unable to discover master, not going to force mastership takeover
[user:info,2019-07-04T11:52:34.388Z,ns_1@127.0.0.1:mb_master<0.432.0>:mb_master:init:86]I'm the only node, so I'm the master.
[ns_server:debug,2019-07-04T11:52:34.388Z,ns_1@127.0.0.1:leader_registry<0.429.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2019-07-04T11:52:34.408Z,ns_1@127.0.0.1:leader_lease_acquirer<0.435.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2019-07-04T11:52:34.408Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.435.0>},
                       {name,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.412Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.437.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2019-07-04T11:52:34.412Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.437.0>},
                       {name,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:34.420Z,ns_1@127.0.0.1:mb_master_sup<0.434.0>:misc:start_singleton:756]start_singleton(gen_server, ns_tick, [], []): started as <0.439.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-07-04T11:52:34.420Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.439.0>},
                       {name,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.429Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.442.0>},
                       {name,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:34.432Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.441.0>:misc:start_singleton:756]start_singleton(gen_server, auto_reprovision, [], []): started as <0.443.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-07-04T11:52:34.432Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.443.0>},
                       {name,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.468Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,0]}]

[ns_server:info,2019-07-04T11:52:34.468Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [4,1]
[ns_server:debug,2019-07-04T11:52:34.468Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,1]},
 {set,{service_map,n1ql},[]},
 {set,{service_map,index},[]}]

[ns_server:info,2019-07-04T11:52:34.468Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [4,5]
[ns_server:debug,2019-07-04T11:52:34.468Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,5]},
 {set,{service_map,fts},[]},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>}]

[ns_server:info,2019-07-04T11:52:34.469Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [4,6]
[ns_server:debug,2019-07-04T11:52:34.469Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,6]}]

[ns_server:info,2019-07-04T11:52:34.469Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,0]
[ns_server:debug,2019-07-04T11:52:34.469Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]},
 {delete,roles_definitions},
 {delete,users_upgrade},
 {delete,read_only_user_creds},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2019-07-04T11:52:34.470Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2019-07-04T11:52:34.470Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2019-07-04T11:52:34.476Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2019-07-04T11:52:34.477Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@127.0.0.1']},
 {set,scramsha_fallback_salt,<<131,49,2,131,220,38,244,3,105,90,168,156>>}]

[ns_server:info,2019-07-04T11:52:34.526Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2019-07-04T11:52:34.551Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]},
 {set,audit_decriptors,
      [{20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]}]}]

[ns_server:debug,2019-07-04T11:52:34.554Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,3359036101},
                                                                false,[]} to {[6,
                                                                               0],
                                                                              {0,
                                                                               3359036101},
                                                                              false,
                                                                              []}
[ns_server:debug,2019-07-04T11:52:34.555Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.437.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2019-07-04T11:52:34.556Z,ns_1@127.0.0.1:leader_lease_acquirer<0.435.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2019-07-04T11:52:34.556Z,ns_1@127.0.0.1:menelaus_ui_auth<0.303.0>:token_server:handle_cast:202]Purge tokens []
[ns_server:debug,2019-07-04T11:52:34.561Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:52:34.562Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit_decriptors ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]}]
[ns_server:debug,2019-07-04T11:52:34.563Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 <<131,49,2,131,220,38,244,3,105,90,168,156>>]
[ns_server:debug,2019-07-04T11:52:34.563Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{rbac_upgrade,[5,5]} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:52:34.563Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2019-07-04T11:52:34.563Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>]
[ns_server:debug,2019-07-04T11:52:34.563Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
client_cert_auth ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2019-07-04T11:52:34.563Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
users_upgrade ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
roles_definitions ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,fts} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}]
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,index} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}]
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,n1ql} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}]
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cluster_compat_version ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{8,63729460354}}]},6,0]
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]}]
[ns_server:debug,2019-07-04T11:52:34.564Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{3,63729460354}}],{configs,[]}]
[ns_server:debug,2019-07-04T11:52:34.565Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
quorum_nodes ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:52:34.565Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
read_only_user_creds ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:52:34.565Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2019-07-04T11:52:34.565Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460354}}]}]
[ns_server:debug,2019-07-04T11:52:34.555Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,quorum_nodes,
                               read_only_user_creds,roles_definitions,
                               scramsha_fallback_salt,users_upgrade,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {rbac_upgrade,[5,5]},
                               {service_map,fts},
                               {service_map,index},
                               {service_map,n1ql}]..)
[ns_server:debug,2019-07-04T11:52:34.578Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.236.0>
[ns_server:warn,2019-07-04T11:52:34.578Z,ns_1@127.0.0.1:<0.447.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:34.579Z,ns_1@127.0.0.1:<0.402.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.367.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.579Z,ns_1@127.0.0.1:<0.409.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.367.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.579Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:handle_call:116]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2019-07-04T11:52:34.580Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:handle_call:122]Fully synchronized config in 49 us
[user:warn,2019-07-04T11:52:34.580Z,ns_1@127.0.0.1:<0.444.0>:ns_orchestrator:consider_switching_compat_mode_dont_exit:925]Changed cluster compat mode from undefined to [6,0]
[ns_server:debug,2019-07-04T11:52:34.580Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:52:34.580Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.236.0>
[error_logger:error,2019-07-04T11:52:34.580Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.447.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:info,2019-07-04T11:52:34.580Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.441.0>:misc:start_singleton:756]start_singleton(gen_fsm, ns_orchestrator, [], []): started as <0.444.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2019-07-04T11:52:34.581Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.437.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2019-07-04T11:52:34.581Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.437.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2019-07-04T11:52:34.593Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:warn,2019-07-04T11:52:34.593Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:34.593Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:error,2019-07-04T11:52:34.599Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.446.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.600Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.367.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.600Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.367.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: [do_check,do_check]
    links: [<0.226.0>,<0.409.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 23528
  neighbours:

[error_logger:info,2019-07-04T11:52:34.600Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.444.0>},
                       {name,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.600Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.441.0>},
                       {name,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:34.620Z,ns_1@127.0.0.1:<0.469.0>:auto_failover:init:211]init auto_failover.
[user:info,2019-07-04T11:52:34.620Z,ns_1@127.0.0.1:<0.469.0>:auto_failover:handle_call:242]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2019-07-04T11:52:34.662Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.440.0>:misc:start_singleton:756]start_singleton(gen_server, auto_failover, [], []): started as <0.469.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2019-07-04T11:52:34.662Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{3,63729460354}}]}]
[error_logger:info,2019-07-04T11:52:34.663Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.469.0>},
                       {name,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.663Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]}]
[error_logger:info,2019-07-04T11:52:34.663Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.440.0>},
                       {name,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:34.663Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[error_logger:info,2019-07-04T11:52:34.664Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.432.0>},
                       {name,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:34.664Z,ns_1@127.0.0.1:<0.422.0>:restartable:start_child:98]Started child process <0.423.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2019-07-04T11:52:34.664Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.427.0>},
                       {name,leader_registry_sup},
                       {mfargs,
                           {leader_services_sup,start_link,
                               [leader_registry_sup]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.664Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.422.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.664Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.479.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.664Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.480.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.682Z,ns_1@127.0.0.1:leader_lease_agent<0.426.0>:leader_lease_agent:do_handle_acquire_lease:147]Granting lease to {lease_holder,<<"2e0765d644fd2a31b0632a30fdfa32a4">>,
                                'ns_1@127.0.0.1'} for 15000ms
[error_logger:info,2019-07-04T11:52:34.690Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.483.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:52:34.696Z,ns_1@127.0.0.1:<0.471.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"2e0765d644fd2a31b0632a30fdfa32a4">>)
[error_logger:info,2019-07-04T11:52:34.699Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.484.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.721Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.487.0>},
                       {name,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.722Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.489.0>},
                       {name,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.724Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.490.0>},
                       {name,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.81875396>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.751Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.498.0>},
                       {name,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.755Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.506.0>},
                       {name,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:52:34.756Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.486.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:error,2019-07-04T11:52:34.756Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.367.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[ns_server:debug,2019-07-04T11:52:34.756Z,ns_1@127.0.0.1:memcached_config_mgr<0.508.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:34.756Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.367.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:34.756Z,ns_1@127.0.0.1:memcached_config_mgr<0.508.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:info,2019-07-04T11:52:34.756Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.508.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[ns_server:debug,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:menelaus_barrier<0.171.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.169.0>
[error_logger:info,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.226.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:<0.168.0>:restartable:start_child:98]Started child process <0.169.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.168.0>},
                       {name,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2019-07-04T11:52:34.757Z,ns_1@127.0.0.1:<0.2.0>:child_erlang:child_loop:130]134: Entered child_loop
[ns_server:debug,2019-07-04T11:52:34.759Z,ns_1@127.0.0.1:memcached_config_mgr<0.508.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.772Z,ns_1@127.0.0.1:memcached_config_mgr<0.508.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.775Z,ns_1@127.0.0.1:memcached_config_mgr<0.508.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.797Z,ns_1@127.0.0.1:<0.512.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.797Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.512.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:34.798Z,ns_1@127.0.0.1:<0.510.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.508.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.798Z,ns_1@127.0.0.1:<0.509.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.508.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.798Z,ns_1@127.0.0.1:memcached_config_mgr<0.513.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-07-04T11:52:34.798Z,ns_1@127.0.0.1:memcached_config_mgr<0.513.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:34.800Z,ns_1@127.0.0.1:memcached_config_mgr<0.513.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.802Z,ns_1@127.0.0.1:memcached_config_mgr<0.513.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[error_logger:error,2019-07-04T11:52:34.804Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.511.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.804Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.508.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-07-04T11:52:34.805Z,ns_1@127.0.0.1:memcached_config_mgr<0.513.0>:memcached_config_mgr:init:85]found memcached port to be already active
[error_logger:error,2019-07-04T11:52:34.805Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.508.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.510.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16457
  neighbours:

[error_logger:error,2019-07-04T11:52:34.805Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.508.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.806Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.508.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.806Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.513.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:52:34.807Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:34.807Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:52:34.808Z,ns_1@127.0.0.1:<0.517.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.808Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.517.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:34.809Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.516.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.809Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.513.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.809Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.513.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.515.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16467
  neighbours:

[error_logger:error,2019-07-04T11:52:34.809Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.513.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.810Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.513.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:34.810Z,ns_1@127.0.0.1:<0.515.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.513.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.810Z,ns_1@127.0.0.1:<0.514.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.513.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.810Z,ns_1@127.0.0.1:memcached_config_mgr<0.518.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:52:34.810Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.518.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.810Z,ns_1@127.0.0.1:memcached_config_mgr<0.518.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:34.812Z,ns_1@127.0.0.1:memcached_config_mgr<0.518.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.814Z,ns_1@127.0.0.1:memcached_config_mgr<0.518.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.816Z,ns_1@127.0.0.1:memcached_config_mgr<0.518.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.839Z,ns_1@127.0.0.1:<0.522.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.839Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.522.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:34.840Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.521.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.840Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.518.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.840Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.518.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.520.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16463
  neighbours:

[error_logger:error,2019-07-04T11:52:34.841Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.518.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.841Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.518.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:34.841Z,ns_1@127.0.0.1:<0.520.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.518.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.841Z,ns_1@127.0.0.1:<0.519.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.518.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.841Z,ns_1@127.0.0.1:memcached_config_mgr<0.523.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:52:34.841Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.523.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.842Z,ns_1@127.0.0.1:memcached_config_mgr<0.523.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:34.843Z,ns_1@127.0.0.1:memcached_config_mgr<0.523.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.847Z,ns_1@127.0.0.1:memcached_config_mgr<0.523.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.850Z,ns_1@127.0.0.1:memcached_config_mgr<0.523.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.852Z,ns_1@127.0.0.1:<0.527.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.852Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.527.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:34.853Z,ns_1@127.0.0.1:<0.524.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.523.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.853Z,ns_1@127.0.0.1:<0.525.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.523.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.854Z,ns_1@127.0.0.1:memcached_config_mgr<0.528.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:34.854Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.526.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-07-04T11:52:34.854Z,ns_1@127.0.0.1:memcached_config_mgr<0.528.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:34.854Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.523.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.855Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.523.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.525.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 16264
  neighbours:

[error_logger:error,2019-07-04T11:52:34.855Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.523.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.855Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.523.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.855Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.528.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.857Z,ns_1@127.0.0.1:memcached_config_mgr<0.528.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.859Z,ns_1@127.0.0.1:memcached_config_mgr<0.528.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.861Z,ns_1@127.0.0.1:memcached_config_mgr<0.528.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.863Z,ns_1@127.0.0.1:<0.532.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.863Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.532.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:34.863Z,ns_1@127.0.0.1:<0.529.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.528.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.863Z,ns_1@127.0.0.1:<0.530.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.528.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.864Z,ns_1@127.0.0.1:memcached_config_mgr<0.533.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:34.864Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.531.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-07-04T11:52:34.864Z,ns_1@127.0.0.1:memcached_config_mgr<0.533.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:34.864Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.528.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.865Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.528.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.530.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16467
  neighbours:

[error_logger:error,2019-07-04T11:52:34.865Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.528.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.865Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.528.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.865Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.533.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.878Z,ns_1@127.0.0.1:memcached_config_mgr<0.533.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.883Z,ns_1@127.0.0.1:memcached_config_mgr<0.533.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.885Z,ns_1@127.0.0.1:memcached_config_mgr<0.533.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.887Z,ns_1@127.0.0.1:<0.537.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.887Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.537.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:34.888Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.536.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.888Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.533.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.888Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.533.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.535.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16463
  neighbours:

[error_logger:error,2019-07-04T11:52:34.889Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.533.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.889Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.533.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:34.889Z,ns_1@127.0.0.1:<0.535.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.533.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.889Z,ns_1@127.0.0.1:<0.534.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.533.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.890Z,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-07-04T11:52:34.890Z,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:info,2019-07-04T11:52:34.890Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.538.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.892Z,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.894Z,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.909Z,ns_1@127.0.0.1:memcached_config_mgr<0.538.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.910Z,ns_1@127.0.0.1:<0.542.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:34.911Z,ns_1@127.0.0.1:<0.540.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.538.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-07-04T11:52:34.911Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.542.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:34.912Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.541.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.912Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.538.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.912Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.538.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.540.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16400
  neighbours:

[error_logger:error,2019-07-04T11:52:34.913Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.538.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[ns_server:debug,2019-07-04T11:52:34.913Z,ns_1@127.0.0.1:memcached_config_mgr<0.543.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:34.913Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.538.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.913Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.543.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.913Z,ns_1@127.0.0.1:<0.539.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.538.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.914Z,ns_1@127.0.0.1:memcached_config_mgr<0.543.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:34.915Z,ns_1@127.0.0.1:memcached_config_mgr<0.543.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.925Z,ns_1@127.0.0.1:memcached_config_mgr<0.543.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.928Z,ns_1@127.0.0.1:memcached_config_mgr<0.543.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.929Z,ns_1@127.0.0.1:<0.547.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.930Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.547.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:34.930Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.546.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.930Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.543.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.931Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.543.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.545.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16404
  neighbours:

[error_logger:error,2019-07-04T11:52:34.931Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.543.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.931Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.543.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:34.932Z,ns_1@127.0.0.1:<0.545.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.543.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.932Z,ns_1@127.0.0.1:<0.544.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.543.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.932Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:52:34.932Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.548.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.932Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:34.934Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.939Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.942Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.944Z,ns_1@127.0.0.1:<0.552.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.944Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.552.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:34.944Z,ns_1@127.0.0.1:<0.549.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.548.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.944Z,ns_1@127.0.0.1:<0.550.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.548.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.945Z,ns_1@127.0.0.1:memcached_config_mgr<0.553.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:34.945Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.551.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-07-04T11:52:34.945Z,ns_1@127.0.0.1:memcached_config_mgr<0.553.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:34.945Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.548.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.946Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.548.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.550.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16467
  neighbours:

[error_logger:error,2019-07-04T11:52:34.947Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.548.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.947Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.548.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.947Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.553.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.949Z,ns_1@127.0.0.1:memcached_config_mgr<0.553.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.950Z,ns_1@127.0.0.1:memcached_config_mgr<0.553.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.956Z,ns_1@127.0.0.1:memcached_config_mgr<0.553.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.959Z,ns_1@127.0.0.1:<0.557.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:34.959Z,ns_1@127.0.0.1:<0.554.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.553.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.959Z,ns_1@127.0.0.1:<0.555.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.553.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-07-04T11:52:34.960Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.556.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-07-04T11:52:34.960Z,ns_1@127.0.0.1:memcached_config_mgr<0.558.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:34.960Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.557.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:34.960Z,ns_1@127.0.0.1:memcached_config_mgr<0.558.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:34.960Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.553.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.961Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.553.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.555.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 16264
  neighbours:

[error_logger:error,2019-07-04T11:52:34.961Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.553.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.962Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.553.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.962Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.558.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.963Z,ns_1@127.0.0.1:memcached_config_mgr<0.558.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.967Z,ns_1@127.0.0.1:memcached_config_mgr<0.558.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.970Z,ns_1@127.0.0.1:memcached_config_mgr<0.558.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.972Z,ns_1@127.0.0.1:<0.563.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.972Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.563.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:34.973Z,ns_1@127.0.0.1:<0.559.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.558.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.973Z,ns_1@127.0.0.1:<0.561.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.558.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:34.974Z,ns_1@127.0.0.1:memcached_config_mgr<0.564.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:34.974Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.562.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-07-04T11:52:34.974Z,ns_1@127.0.0.1:memcached_config_mgr<0.564.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:34.974Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.558.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:34.975Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.558.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.561.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16408
  neighbours:

[error_logger:error,2019-07-04T11:52:34.975Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.558.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.975Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.558.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.977Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.564.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:34.979Z,ns_1@127.0.0.1:memcached_config_mgr<0.564.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.981Z,ns_1@127.0.0.1:memcached_config_mgr<0.564.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:34.984Z,ns_1@127.0.0.1:memcached_config_mgr<0.564.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:34.985Z,ns_1@127.0.0.1:<0.568.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:34.985Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.568.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:34.986Z,ns_1@127.0.0.1:<0.565.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.564.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:34.986Z,ns_1@127.0.0.1:<0.566.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.564.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-07-04T11:52:34.986Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.567.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:34.987Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.564.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-07-04T11:52:34.988Z,ns_1@127.0.0.1:memcached_config_mgr<0.569.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-07-04T11:52:34.988Z,ns_1@127.0.0.1:memcached_config_mgr<0.569.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:34.989Z,ns_1@127.0.0.1:memcached_config_mgr<0.569.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:34.996Z,ns_1@127.0.0.1:memcached_config_mgr<0.569.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[error_logger:error,2019-07-04T11:52:34.997Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.564.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.566.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 16264
  neighbours:

[error_logger:error,2019-07-04T11:52:34.997Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.564.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:34.997Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.564.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:34.998Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.569.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.016Z,ns_1@127.0.0.1:memcached_config_mgr<0.569.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:35.020Z,ns_1@127.0.0.1:<0.365.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:52:35.021Z,ns_1@127.0.0.1:<0.573.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:35.021Z,ns_1@127.0.0.1:<0.570.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.569.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:35.021Z,ns_1@127.0.0.1:<0.571.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.569.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:35.022Z,ns_1@127.0.0.1:memcached_config_mgr<0.574.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-07-04T11:52:35.022Z,ns_1@127.0.0.1:memcached_config_mgr<0.574.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:35.022Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.573.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:35.023Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.572.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:35.023Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.569.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.024Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.569.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.571.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16357
  neighbours:

[error_logger:error,2019-07-04T11:52:35.024Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.569.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.024Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.569.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:35.024Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.574.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.026Z,ns_1@127.0.0.1:memcached_config_mgr<0.574.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:35.049Z,ns_1@127.0.0.1:memcached_config_mgr<0.574.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:35.054Z,ns_1@127.0.0.1:memcached_config_mgr<0.574.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:35.055Z,ns_1@127.0.0.1:<0.578.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:35.056Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.578.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:35.056Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.577.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:35.057Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.574.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.057Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.574.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.058Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.574.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.576.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16394
  neighbours:

[error_logger:error,2019-07-04T11:52:35.060Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.574.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:35.060Z,ns_1@127.0.0.1:<0.576.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.574.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:35.060Z,ns_1@127.0.0.1:memcached_config_mgr<0.579.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-07-04T11:52:35.061Z,ns_1@127.0.0.1:memcached_config_mgr<0.579.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:info,2019-07-04T11:52:35.060Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.579.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.062Z,ns_1@127.0.0.1:memcached_config_mgr<0.579.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:35.064Z,ns_1@127.0.0.1:memcached_config_mgr<0.579.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:35.069Z,ns_1@127.0.0.1:memcached_config_mgr<0.579.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:debug,2019-07-04T11:52:35.065Z,ns_1@127.0.0.1:<0.575.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.574.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:warn,2019-07-04T11:52:35.076Z,ns_1@127.0.0.1:<0.583.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:35.077Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.583.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:35.077Z,ns_1@127.0.0.1:<0.580.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.579.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:35.077Z,ns_1@127.0.0.1:<0.581.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.579.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:35.078Z,ns_1@127.0.0.1:memcached_config_mgr<0.584.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:35.078Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.582.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-07-04T11:52:35.078Z,ns_1@127.0.0.1:memcached_config_mgr<0.584.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:35.078Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.579.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.579.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.581.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16392
  neighbours:

[error_logger:error,2019-07-04T11:52:35.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.579.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.579.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:35.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.584.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.080Z,ns_1@127.0.0.1:memcached_config_mgr<0.584.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:35.082Z,ns_1@127.0.0.1:memcached_config_mgr<0.584.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:35.084Z,ns_1@127.0.0.1:memcached_config_mgr<0.584.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:debug,2019-07-04T11:52:35.101Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:warn,2019-07-04T11:52:35.116Z,ns_1@127.0.0.1:<0.588.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:35.117Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.588.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:35.117Z,ns_1@127.0.0.1:<0.585.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.584.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:35.117Z,ns_1@127.0.0.1:<0.586.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.584.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:35.118Z,ns_1@127.0.0.1:memcached_config_mgr<0.589.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-07-04T11:52:35.118Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.587.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-07-04T11:52:35.118Z,ns_1@127.0.0.1:memcached_config_mgr<0.589.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:35.118Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.584.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.119Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.584.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.586.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16463
  neighbours:

[error_logger:error,2019-07-04T11:52:35.119Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.584.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.119Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.584.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:35.119Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.589.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.122Z,ns_1@127.0.0.1:memcached_config_mgr<0.589.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:35.124Z,ns_1@127.0.0.1:memcached_config_mgr<0.589.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:35.142Z,ns_1@127.0.0.1:memcached_config_mgr<0.589.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:35.145Z,ns_1@127.0.0.1:<0.593.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:35.145Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.593.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:35.146Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.592.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:35.146Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.589.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.147Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.589.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.591.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 27
    reductions: 16492
  neighbours:

[error_logger:error,2019-07-04T11:52:35.147Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.589.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.147Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.589.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:35.148Z,ns_1@127.0.0.1:<0.591.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.589.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:35.148Z,ns_1@127.0.0.1:memcached_config_mgr<0.594.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:52:35.148Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.594.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.148Z,ns_1@127.0.0.1:memcached_config_mgr<0.594.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:35.150Z,ns_1@127.0.0.1:memcached_config_mgr<0.594.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:35.152Z,ns_1@127.0.0.1:memcached_config_mgr<0.594.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:35.152Z,ns_1@127.0.0.1:<0.590.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.589.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:35.177Z,ns_1@127.0.0.1:memcached_config_mgr<0.594.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:35.180Z,ns_1@127.0.0.1:<0.598.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:35.180Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.598.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:35.181Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.597.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:35.181Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.594.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.182Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.594.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.596.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16392
  neighbours:

[error_logger:error,2019-07-04T11:52:35.182Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.594.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.182Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.594.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:52:35.182Z,ns_1@127.0.0.1:<0.596.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.594.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:35.182Z,ns_1@127.0.0.1:<0.595.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.594.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:35.183Z,ns_1@127.0.0.1:memcached_config_mgr<0.599.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:52:35.183Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.599.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.183Z,ns_1@127.0.0.1:memcached_config_mgr<0.599.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:35.184Z,ns_1@127.0.0.1:memcached_config_mgr<0.599.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:35.191Z,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.601.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.601.0>
[ns_server:debug,2019-07-04T11:52:35.217Z,ns_1@127.0.0.1:memcached_config_mgr<0.599.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:35.221Z,ns_1@127.0.0.1:memcached_config_mgr<0.599.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:35.223Z,ns_1@127.0.0.1:<0.605.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-07-04T11:52:35.224Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.605.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-07-04T11:52:35.224Z,ns_1@127.0.0.1:memcached_config_mgr<0.606.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-07-04T11:52:35.224Z,ns_1@127.0.0.1:<0.603.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.599.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-07-04T11:52:35.224Z,ns_1@127.0.0.1:<0.600.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.599.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:35.224Z,ns_1@127.0.0.1:memcached_config_mgr<0.606.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-07-04T11:52:35.227Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.604.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:35.227Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.599.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.228Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.599.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.603.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16463
  neighbours:

[error_logger:error,2019-07-04T11:52:35.228Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.599.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.228Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.599.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:52:35.228Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.606.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:35.229Z,ns_1@127.0.0.1:memcached_config_mgr<0.606.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:35.235Z,ns_1@127.0.0.1:memcached_config_mgr<0.606.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:35.247Z,ns_1@127.0.0.1:memcached_config_mgr<0.606.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-07-04T11:52:35.249Z,ns_1@127.0.0.1:<0.610.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:35.250Z,ns_1@127.0.0.1:<0.607.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.606.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-07-04T11:52:35.250Z,ns_1@127.0.0.1:<0.608.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.606.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-07-04T11:52:35.251Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.610.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-07-04T11:52:35.252Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.609.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-07-04T11:52:35.252Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.606.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-07-04T11:52:35.253Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.606.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.226.0>,<0.608.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16484
  neighbours:

[error_logger:error,2019-07-04T11:52:35.253Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.606.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-07-04T11:52:35.253Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.606.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:warn,2019-07-04T11:52:35.258Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:35.258Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:52:35.313Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:35.313Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:52:35.372Z,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.612.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.612.0>
[ns_server:debug,2019-07-04T11:52:35.372Z,ns_1@127.0.0.1:menelaus_cbauth<0.348.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"goxdcr-cbauth",<0.612.0>} started
[ns_server:debug,2019-07-04T11:52:35.387Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:warn,2019-07-04T11:52:35.423Z,ns_1@127.0.0.1:<0.616.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:52:35.596Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:35.596Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:52:35.622Z,ns_1@127.0.0.1:<0.469.0>:auto_failover_logic:log_master_activity:170]Transitioned node {'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>} state new -> up
[ns_server:warn,2019-07-04T11:52:35.809Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:52:35.809Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:52:36.047Z,ns_1@127.0.0.1:ns_audit_cfg<0.362.0>:ns_audit_cfg:write_audit_json:265]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json" : [{descriptors_path,
                                                                                <<"/opt/couchbase/etc/security">>},
                                                                               {version,
                                                                                2},
                                                                               {uuid,
                                                                                <<"18411111">>},
                                                                               {event_states,
                                                                                {[]}},
                                                                               {filtering_enabled,
                                                                                true},
                                                                               {disabled_userids,
                                                                                []},
                                                                               {auditd_enabled,
                                                                                false},
                                                                               {log_path,
                                                                                <<"/opt/couchbase/var/lib/couchbase/logs">>},
                                                                               {rotate_interval,
                                                                                86400},
                                                                               {rotate_size,
                                                                                20971520},
                                                                               {sync,
                                                                                []}]
[ns_server:debug,2019-07-04T11:52:36.055Z,ns_1@127.0.0.1:ns_audit_cfg<0.362.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2019-07-04T11:52:36.270Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[stats:warn,2019-07-04T11:52:36.427Z,ns_1@127.0.0.1:<0.392.0>:base_stats_collector:latest_tick:69](Collector: global_stats_collector) Dropped 1 ticks
[ns_server:debug,2019-07-04T11:52:38.588Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.280.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:52:39.145Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.394.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:52:39.146Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.394.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:52:39.255Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:52:39.255Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.714.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:52:39.256Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-07-04T11:52:39.257Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-07-04T11:52:39.263Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-07-04T11:52:39.266Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:debug,2019-07-04T11:52:39.272Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:apply_changed_memcached_config:163]New memcached config is hot-reloadable.
[ns_server:debug,2019-07-04T11:52:39.273Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[user:info,2019-07-04T11:52:39.317Z,ns_1@127.0.0.1:memcached_config_mgr<0.714.0>:memcached_config_mgr:hot_reload_config:223]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>,
                                                                      <<"xattr_enabled">>]
[ns_server:debug,2019-07-04T11:52:41.576Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {[],wrong_token}
[ns_server:debug,2019-07-04T11:53:04.249Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2019-07-04T11:53:04.249Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:53:04.249Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2019-07-04T11:53:04.249Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:53:05.555Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{4,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.556Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
settings ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2019-07-04T11:53:05.557Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:05.572Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memory_quota ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|994]
[ns_server:debug,2019-07-04T11:53:05.572Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([memory_quota,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2019-07-04T11:53:05.572Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460385}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2019-07-04T11:53:05.572Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{5,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.575Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{6,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.576Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',services} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
 fts,index,kv,n1ql]
[ns_server:debug,2019-07-04T11:53:05.576Z,ns_1@127.0.0.1:ns_audit<0.366.0>:ns_audit:handle_call:110]Audit setup_node_services: [{services,[fts,index,kv,n1ql]},
                            {node,'ns_1@127.0.0.1'},
                            {real_userid,
                                {[{domain,wrong_token},
                                  {user,<<"<ud></ud>">>}]}},
                            {sessionid,<<"98dfbca7bc2c007a9add58c491a812fc">>},
                            {remote,{[{ip,<<"172.24.0.1">>},{port,33716}]}},
                            {timestamp,<<"2019-07-04T11:53:05.575Z">>}]
[ns_server:debug,2019-07-04T11:53:05.577Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {node,'ns_1@127.0.0.1',services}]..)
[ns_server:debug,2019-07-04T11:53:05.599Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{7,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.599Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cluster_name ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
 97,112,112]
[ns_server:debug,2019-07-04T11:53:05.599Z,ns_1@127.0.0.1:ns_audit<0.366.0>:ns_audit:handle_call:110]Audit cluster_settings: [{cluster_name,<<"app">>},
                         {quotas,{[{kv,994},
                                   {index,349},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {sessionid,<<"98dfbca7bc2c007a9add58c491a812fc">>},
                         {remote,{[{ip,<<"172.24.0.1">>},{port,33712}]}},
                         {timestamp,<<"2019-07-04T11:53:05.599Z">>}]
[ns_server:debug,2019-07-04T11:53:05.600Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{3,63729460385}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2019-07-04T11:53:05.600Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{8,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.601Z,ns_1@127.0.0.1:ns_audit<0.366.0>:ns_audit:handle_call:110]Audit modify_index_storage_mode: [{storageMode,<<"forestdb">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {sessionid,
                                      <<"98dfbca7bc2c007a9add58c491a812fc">>},
                                  {remote,
                                      {[{ip,<<"172.24.0.1">>},{port,33716}]}},
                                  {timestamp,<<"2019-07-04T11:53:05.600Z">>}]
[ns_server:debug,2019-07-04T11:53:05.601Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2019-07-04T11:53:05.646Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([rest,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:05.648Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{9,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.648Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest ->
[{port,8091}]
[ns_server:debug,2019-07-04T11:53:05.671Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[6,0],
                                                                {0,3359036101},
                                                                false,[]} to {[6,
                                                                               0],
                                                                              {0,
                                                                               3359036101},
                                                                              true,
                                                                              []}
[ns_server:debug,2019-07-04T11:53:05.671Z,ns_1@127.0.0.1:menelaus_ui_auth<0.303.0>:token_server:handle_cast:202]Purge tokens []
[ns_server:debug,2019-07-04T11:53:05.671Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{10,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.672Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest_creds ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"jkKoQCLgS0hwSaQwbjFuPTPff4IF8pnVfaYqo4gjzGRi9I4+EJ+qkWwBnlE0q7M4oewExLOJOdnRZy8S34iiWg==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"a0jO063Ch2WEsXWFP3cZ2iHE8JeSZPweQjKljT1T4r0=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"X1H9WHt5Rn/HyJ8kJ4/Z07G+iSc=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2019-07-04T11:53:05.672Z,ns_1@127.0.0.1:ns_audit<0.366.0>:ns_audit:handle_call:110]Audit password_change: [{identity,{[{domain,builtin},
                                    {user,<<"<ud>admin</ud>">>}]}},
                        {real_userid,{[{domain,wrong_token},
                                       {user,<<"<ud></ud>">>}]}},
                        {sessionid,<<"98dfbca7bc2c007a9add58c491a812fc">>},
                        {remote,{[{ip,<<"172.24.0.1">>},{port,33716}]}},
                        {timestamp,<<"2019-07-04T11:53:05.671Z">>}]
[ns_server:debug,2019-07-04T11:53:05.673Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{11,63729460385}}]}]
[ns_server:debug,2019-07-04T11:53:05.674Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
uuid ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
 <<"070fee742e7318eb771177d58d7c8507">>]
[ns_server:debug,2019-07-04T11:53:05.674Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([rest_creds,uuid,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:05.676Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:53:05.680Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.236.0>
[ns_server:debug,2019-07-04T11:53:05.680Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:05.681Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.236.0>
[ns_server:debug,2019-07-04T11:53:05.681Z,ns_1@127.0.0.1:memcached_passwords<0.233.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-07-04T11:53:05.687Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2019-07-04T11:53:05.688Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2019-07-04T11:53:05.694Z,ns_1@127.0.0.1:ns_audit<0.366.0>:ns_audit:handle_call:110]Audit login_success: [{roles,[<<"admin">>]},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {sessionid,<<"5ee9af94f116d54e79315fd2c900c3c3">>},
                      {remote,{[{ip,<<"172.24.0.1">>},{port,33716}]}},
                      {timestamp,<<"2019-07-04T11:53:05.694Z">>}]
[error_logger:info,2019-07-04T11:53:05.696Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1360.0>},
                       {name,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:05.698Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:05.708Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2019-07-04T11:53:05.713Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1363.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[error_logger:info,2019-07-04T11:53:05.713Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1363.0>},
                       {name,{service_fts,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,[service_fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:05.714Z,ns_1@127.0.0.1:service_stats_collector-index<0.1366.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[error_logger:info,2019-07-04T11:53:05.714Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1366.0>},
                       {name,{service_index,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:05.717Z,ns_1@127.0.0.1:ns_ports_setup<0.354.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,projector,indexer,query,saslauthd_port,goxdcr,fts]
[ns_server:debug,2019-07-04T11:53:05.730Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.233.0>
[ns_server:debug,2019-07-04T11:53:05.730Z,ns_1@127.0.0.1:memcached_passwords<0.233.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:05.731Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.233.0>
[ns_server:debug,2019-07-04T11:53:05.737Z,ns_1@127.0.0.1:menelaus_cbauth<0.348.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"goxdcr-cbauth",<0.612.0>} needs_update
[error_logger:info,2019-07-04T11:53:05.737Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.1370.0>},
                       {name,{service_agent,fts}},
                       {mfargs,{service_agent,start_link,[fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:05.739Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.1374.0>},
                       {name,{service_agent,index}},
                       {mfargs,{service_agent,start_link,[index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:05.740Z,ns_1@127.0.0.1:menelaus_cbauth<0.348.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"goxdcr-cbauth",<0.612.0>} needs_update
[ns_server:debug,2019-07-04T11:53:05.762Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2019-07-04T11:53:05.763Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1378.0>},
                       {name,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:05.772Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1369.0>},
                       {name,{service_fts,stats_archiver,"@fts"}},
                       {mfargs,{stats_archiver,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:05.773Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1381.0>},
                       {name,{service_fts,stats_reader,"@fts"}},
                       {mfargs,{stats_reader,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:05.800Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[error_logger:info,2019-07-04T11:53:05.810Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.1383.0>},
                       {name,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:05.817Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1382.0>},
                       {name,{service_index,stats_archiver,"@index"}},
                       {mfargs,{stats_archiver,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:05.817Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.1386.0>},
                       {name,{service_index,stats_reader,"@index"}},
                       {mfargs,{stats_reader,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:06.069Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:error,2019-07-04T11:53:06.420Z,ns_1@127.0.0.1:query_stats_collector<0.387.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:error,2019-07-04T11:53:06.481Z,ns_1@127.0.0.1:<0.617.0>:menelaus_pluggable_ui:handle_resp:344]http client error {error,
                      {econnrefused,
                          [{lhttpc_client,send_request,1,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,220}]},
                           {lhttpc_client,execute,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,169}]},
                           {lhttpc_client,request,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,92}]}]}}

[ns_server:debug,2019-07-04T11:53:06.733Z,ns_1@127.0.0.1:service_stats_collector-index<0.1366.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:53:06.733Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1363.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:53:07.408Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:error,2019-07-04T11:53:07.455Z,ns_1@127.0.0.1:query_stats_collector<0.387.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2019-07-04T11:53:07.455Z,ns_1@127.0.0.1:json_rpc_connection-projector-cbauth<0.1442.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.1442.0>
[ns_server:debug,2019-07-04T11:53:07.456Z,ns_1@127.0.0.1:menelaus_cbauth<0.348.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"projector-cbauth",<0.1442.0>} started
[ns_server:debug,2019-07-04T11:53:07.513Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@projector-cbauth",admin}
[ns_server:error,2019-07-04T11:53:07.557Z,ns_1@127.0.0.1:<0.762.0>:menelaus_pluggable_ui:handle_resp:344]http client error {error,
                      {econnrefused,
                          [{lhttpc_client,send_request,1,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,220}]},
                           {lhttpc_client,execute,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,169}]},
                           {lhttpc_client,request,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,92}]}]}}

[ns_server:error,2019-07-04T11:53:07.575Z,ns_1@127.0.0.1:<0.762.0>:menelaus_pluggable_ui:handle_resp:344]http client error {error,
                      {econnrefused,
                          [{lhttpc_client,send_request,1,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,220}]},
                           {lhttpc_client,execute,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,169}]},
                           {lhttpc_client,request,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,92}]}]}}

[ns_server:debug,2019-07-04T11:53:07.594Z,ns_1@127.0.0.1:json_rpc_connection-index-cbauth<0.1452.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-cbauth", handling process <0.1452.0>
[ns_server:debug,2019-07-04T11:53:07.594Z,ns_1@127.0.0.1:menelaus_cbauth<0.348.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"index-cbauth",<0.1452.0>} started
[ns_server:debug,2019-07-04T11:53:07.740Z,ns_1@127.0.0.1:service_stats_collector-index<0.1366.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:53:07.740Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1363.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:53:07.742Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@index-cbauth",admin}
[ns_server:error,2019-07-04T11:53:07.836Z,ns_1@127.0.0.1:<0.617.0>:menelaus_pluggable_ui:handle_resp:344]http client error {error,
                      {econnrefused,
                          [{lhttpc_client,send_request,1,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,220}]},
                           {lhttpc_client,execute,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,169}]},
                           {lhttpc_client,request,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,92}]}]}}

[ns_server:error,2019-07-04T11:53:07.839Z,ns_1@127.0.0.1:<0.762.0>:menelaus_pluggable_ui:handle_resp:344]http client error {error,
                      {econnrefused,
                          [{lhttpc_client,send_request,1,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,220}]},
                           {lhttpc_client,execute,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,169}]},
                           {lhttpc_client,request,9,
                               [{file,
                                    "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                {line,92}]}]}}

[ns_server:debug,2019-07-04T11:53:08.052Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.1480.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-service_api", handling process <0.1480.0>
[ns_server:debug,2019-07-04T11:53:08.052Z,ns_1@127.0.0.1:service_agent-index<0.1374.0>:service_agent:do_handle_connection:324]Observed new json rpc connection for index: <0.1480.0>
[ns_server:debug,2019-07-04T11:53:08.052Z,ns_1@127.0.0.1:<0.1377.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {json_rpc_events,<0.1375.0>} exited with reason normal
[ns_server:debug,2019-07-04T11:53:08.308Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-cbauth<0.1492.0>:json_rpc_connection:init:73]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.1492.0>
[ns_server:debug,2019-07-04T11:53:08.308Z,ns_1@127.0.0.1:menelaus_cbauth<0.348.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"cbq-engine-cbauth",<0.1492.0>} started
[ns_server:debug,2019-07-04T11:53:08.312Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2019-07-04T11:53:08.330Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{12,63729460388}}]}]
[ns_server:debug,2019-07-04T11:53:08.330Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:08.577Z,ns_1@127.0.0.1:json_rpc_connection-fts-cbauth<0.1515.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-cbauth", handling process <0.1515.0>
[ns_server:debug,2019-07-04T11:53:08.578Z,ns_1@127.0.0.1:menelaus_cbauth<0.348.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"fts-cbauth",<0.1515.0>} started
[ns_server:debug,2019-07-04T11:53:08.642Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@fts-cbauth",admin}
[ns_server:debug,2019-07-04T11:53:08.662Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{13,63729460388}}]}]
[ns_server:debug,2019-07-04T11:53:08.662Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
 <<"{\"uuid\":\"4704a37aa982ccf9\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\""...>>]
[ns_server:debug,2019-07-04T11:53:08.662Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-known/2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:08.705Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{14,63729460388}}]}]
[ns_server:debug,2019-07-04T11:53:08.705Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-wanted/2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
 <<"{\"uuid\":\"10e09678eaa2bb72\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\""...>>]
[ns_server:debug,2019-07-04T11:53:08.712Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-wanted/2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:08.744Z,ns_1@127.0.0.1:service_stats_collector-index<0.1366.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:53:08.744Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1363.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:53:08.746Z,ns_1@127.0.0.1:service_stats_collector-index<0.1366.0>:service_stats_collector:check_status:350]Service service_index is started
[ns_server:debug,2019-07-04T11:53:08.788Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/fts/cbgt/cfg/version">>}]..)
[ns_server:debug,2019-07-04T11:53:08.789Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{15,63729460388}}]}]
[ns_server:debug,2019-07-04T11:53:08.789Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/fts/cbgt/cfg/version">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
 <<"5.5.0">>]
[ns_server:debug,2019-07-04T11:53:08.827Z,ns_1@127.0.0.1:json_rpc_connection-fts-service_api<0.1573.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-service_api", handling process <0.1573.0>
[ns_server:debug,2019-07-04T11:53:08.827Z,ns_1@127.0.0.1:service_agent-fts<0.1370.0>:service_agent:do_handle_connection:324]Observed new json rpc connection for fts: <0.1573.0>
[ns_server:debug,2019-07-04T11:53:08.827Z,ns_1@127.0.0.1:<0.1373.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {json_rpc_events,<0.1371.0>} exited with reason normal
[ns_server:debug,2019-07-04T11:53:09.428Z,ns_1@127.0.0.1:cleanup_process<0.1606.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `fts'
[ns_server:debug,2019-07-04T11:53:09.445Z,ns_1@127.0.0.1:service_rebalancer-fts<0.1607.0>:service_agent:wait_for_agents:73]Waiting for the service agents for service fts to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:53:09.445Z,ns_1@127.0.0.1:service_rebalancer-fts<0.1607.0>:service_agent:wait_for_agents_loop:91]All service agents are ready for fts
[rebalance:info,2019-07-04T11:53:09.445Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.1621.0>:service_rebalancer:rebalance:110]Rebalancing service fts with id <<"a26baa76d960a25d7b4208c90c32121e">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2019-07-04T11:53:09.449Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.1621.0>:service_rebalancer:rebalance:115]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2019-07-04T11:53:09.460Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.1621.0>:service_rebalancer:rebalance:124]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2019-07-04T11:53:09.478Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.1621.0>:service_janitor:do_orchestrate_initial_rebalance:101]Initial rebalance progress for `fts': [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2019-07-04T11:53:09.536Z,ns_1@127.0.0.1:service_rebalancer-fts<0.1607.0>:service_rebalancer:run_rebalance:71]Worker terminated: {'EXIT',<0.1621.0>,normal}
[ns_server:debug,2019-07-04T11:53:09.540Z,ns_1@127.0.0.1:cleanup_process<0.1606.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `fts` finished successfully
[ns_server:debug,2019-07-04T11:53:09.541Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {service_map,fts}]..)
[ns_server:debug,2019-07-04T11:53:09.541Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{16,63729460389}}]}]
[ns_server:debug,2019-07-04T11:53:09.545Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,fts} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:53:09.540Z,ns_1@127.0.0.1:cleanup_process<0.1606.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `index'
[ns_server:debug,2019-07-04T11:53:09.546Z,ns_1@127.0.0.1:service_rebalancer-index<0.1660.0>:service_agent:wait_for_agents:73]Waiting for the service agents for service index to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:53:09.548Z,ns_1@127.0.0.1:service_rebalancer-index<0.1660.0>:service_agent:wait_for_agents_loop:91]All service agents are ready for index
[rebalance:info,2019-07-04T11:53:09.548Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.1674.0>:service_rebalancer:rebalance:110]Rebalancing service index with id <<"114b7b811af5701ef1f64dbbd438bdf7">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2019-07-04T11:53:09.550Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.1674.0>:service_rebalancer:rebalance:115]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                    {priority,3},
                    {opaque,null}]}]
[ns_server:debug,2019-07-04T11:53:09.567Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.1674.0>:service_rebalancer:rebalance:124]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2019-07-04T11:53:09.584Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{17,63729460389}}]}]
[ns_server:debug,2019-07-04T11:53:09.584Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460389}}]}|
 <<"{\"MasterId\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"RebalId\":\"114b7b811af5701ef1f64dbbd438bdf7\",\"Source\":0,\"Error\":\"\"}">>]
[ns_server:debug,2019-07-04T11:53:09.585Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2019-07-04T11:53:09.623Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{18,63729460389}}]}]
[ns_server:debug,2019-07-04T11:53:09.623Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:53:09.623Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2019-07-04T11:53:09.650Z,ns_1@127.0.0.1:service_rebalancer-index<0.1660.0>:service_rebalancer:run_rebalance:71]Worker terminated: {'EXIT',<0.1674.0>,normal}
[ns_server:debug,2019-07-04T11:53:09.660Z,ns_1@127.0.0.1:cleanup_process<0.1606.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `index` finished successfully
[ns_server:debug,2019-07-04T11:53:09.662Z,ns_1@127.0.0.1:cleanup_process<0.1606.0>:service_janitor:maybe_init_simple_service:71]Created initial service map for service `n1ql'
[ns_server:debug,2019-07-04T11:53:09.669Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{19,63729460389}}]}]
[ns_server:debug,2019-07-04T11:53:09.670Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,index} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:53:09.670Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{20,63729460389}}]}]
[ns_server:debug,2019-07-04T11:53:09.670Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,n1ql} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:53:09.672Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {service_map,index},
                               {service_map,n1ql}]..)
[ns_server:debug,2019-07-04T11:53:09.749Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1363.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:53:09.751Z,ns_1@127.0.0.1:service_stats_collector-fts<0.1363.0>:service_stats_collector:check_status:350]Service service_fts is started
[ns_server:debug,2019-07-04T11:53:09.815Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{21,63729460389}}]}]
[ns_server:debug,2019-07-04T11:53:09.815Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460389}}]}|
 <<"{\"Version\":3}">>]
[ns_server:debug,2019-07-04T11:53:09.817Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/indexing/info/versionToken">>}]..)
[ns_server:debug,2019-07-04T11:53:23.469Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.469Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.469Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.469Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.469Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.470Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.475Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.475Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.476Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.477Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.477Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.477Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.478Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.478Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.479Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.482Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.482Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.483Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.484Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.484Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.484Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.485Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.485Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.487Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.490Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.490Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.490Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.492Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.492Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.493Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.494Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.494Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.495Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.498Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.498Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.499Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.499Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.499Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.500Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.502Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.502Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.502Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.508Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.509Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.509Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.509Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.509Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.511Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.529Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.529Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.530Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.530Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.530Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.530Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.530Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.530Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.531Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.536Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.536Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.537Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.537Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.537Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.537Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.537Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.538Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.538Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.546Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.546Z,ns_1@127.0.0.1:<0.2158.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.547Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2158.0>
[ns_server:debug,2019-07-04T11:53:23.547Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.547Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.548Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.549Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.549Z,ns_1@127.0.0.1:<0.2160.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.549Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.2160.0>
[ns_server:debug,2019-07-04T11:53:23.554Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:23.554Z,ns_1@127.0.0.1:<0.1689.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:23.554Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1689.0>
[ns_server:debug,2019-07-04T11:53:25.787Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[6,0],
                                                                {0,3359036101},
                                                                true,[]} to {[6,
                                                                              0],
                                                                             {0,
                                                                              3359036101},
                                                                             true,
                                                                             [{"app",
                                                                               <<"5b6ce58456b7220bfb025f341cb20648">>}]}
[ns_server:debug,2019-07-04T11:53:25.787Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{22,63729460405}}]}]
[ns_server:debug,2019-07-04T11:53:25.788Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:25.788Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{4,63729460405}}],
 {configs,[[{map,[]},
            {fastForwardMap,[]},
            {repl_type,dcp},
            {uuid,<<"5b6ce58456b7220bfb025f341cb20648">>},
            {auth_type,sasl},
            {num_replicas,1},
            {replica_index,false},
            {ram_quota,1042284544},
            {autocompaction,false},
            {purge_interval,undefined},
            {flush_enabled,false},
            {num_threads,3},
            {eviction_policy,value_only},
            {conflict_resolution_type,seqno},
            {storage_mode,couchstore},
            {max_ttl,0},
            {compression_mode,off},
            {type,membase},
            {num_vbuckets,1024},
            {replication_topology,star},
            {servers,[]},
            {sasl_password,"*****"}]]}]
[ns_server:debug,2019-07-04T11:53:25.788Z,ns_1@127.0.0.1:ns_audit<0.366.0>:ns_audit:handle_call:110]Audit create_bucket: [{props,{[{compression_mode,off},
                               {max_ttl,0},
                               {storage_mode,couchstore},
                               {conflict_resolution_type,seqno},
                               {eviction_policy,value_only},
                               {num_threads,3},
                               {flush_enabled,false},
                               {purge_interval,undefined},
                               {ram_quota,1042284544},
                               {replica_index,false},
                               {num_replicas,1}]}},
                      {type,membase},
                      {bucket_name,<<"app">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {sessionid,<<"5ee9af94f116d54e79315fd2c900c3c3">>},
                      {remote,{[{ip,<<"172.24.0.1">>},{port,34010}]}},
                      {timestamp,<<"2019-07-04T11:53:25.787Z">>}]
[menelaus:info,2019-07-04T11:53:25.789Z,ns_1@127.0.0.1:<0.1746.0>:menelaus_web_buckets:do_bucket_create:662]Created bucket "app" of type: couchbase
[{num_replicas,1},
 {replica_index,false},
 {ram_quota,1042284544},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,off}]
[ns_server:debug,2019-07-04T11:53:25.812Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2019-07-04T11:53:25.823Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:53:25.825Z,ns_1@127.0.0.1:memcached_passwords<0.233.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-07-04T11:53:25.825Z,ns_1@127.0.0.1:<0.2333.0>:ns_janitor:update_servers:71]janitor decided to update servers list for bucket "app" to ['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:53:25.826Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{23,63729460405}}]}]
[ns_server:debug,2019-07-04T11:53:25.826Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:25.827Z,ns_1@127.0.0.1:ns_bucket_worker<0.371.0>:ns_bucket_sup:update_children:108]Starting new child: {{single_bucket_kv_sup,"app"},
                     {single_bucket_kv_sup,start_link,["app"]},
                     permanent,infinity,supervisor,
                     [single_bucket_kv_sup]}

[ns_server:debug,2019-07-04T11:53:25.826Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{5,63729460405}}],
 {configs,[{"app",
            [{map,[]},
             {fastForwardMap,[]},
             {repl_type,dcp},
             {uuid,<<"5b6ce58456b7220bfb025f341cb20648">>},
             {auth_type,sasl},
             {num_replicas,1},
             {replica_index,false},
             {ram_quota,1042284544},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,off},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[ns_server:debug,2019-07-04T11:53:25.828Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.236.0>
[ns_server:debug,2019-07-04T11:53:25.828Z,ns_1@127.0.0.1:memcached_permissions<0.236.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:25.832Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.236.0>
[ns_server:debug,2019-07-04T11:53:25.835Z,ns_1@127.0.0.1:single_bucket_kv_sup-app<0.2349.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:76]Syncing config to couchdb node
[ns_server:debug,2019-07-04T11:53:25.839Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2019-07-04T11:53:25.839Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2321.0>},
                       {name,{service_fts,stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["@fts-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:25.840Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2019-07-04T11:53:25.844Z,ns_1@127.0.0.1:single_bucket_kv_sup-app<0.2349.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:81]Synced config to couchdb node successfully
[error_logger:info,2019-07-04T11:53:25.844Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2357.0>},
                       {name,{service_fts,stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["@fts-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:25.852Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2019-07-04T11:53:25.858Z,ns_1@127.0.0.1:<0.2362.0>:janitor_agent:query_vbucket_states_loop:96]Exception from query_vbucket_states of "app":'ns_1@127.0.0.1'
{'EXIT',{noproc,{gen_server,call,
                            [{'janitor_agent-app','ns_1@127.0.0.1'},
                             query_vbucket_states,infinity]}}}
[ns_server:debug,2019-07-04T11:53:25.859Z,ns_1@127.0.0.1:<0.2362.0>:janitor_agent:query_vbucket_states_loop_next_step:107]Waiting for "app" on 'ns_1@127.0.0.1'
[stats:error,2019-07-04T11:53:25.871Z,ns_1@127.0.0.1:<0.1401.0>:stats_reader:log_bad_responses:233]Some nodes didn't respond: ['ns_1@127.0.0.1']
[ns_server:warn,2019-07-04T11:53:25.878Z,ns_1@127.0.0.1:kv_monitor<0.1383.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:53:25.879Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.233.0>
[ns_server:debug,2019-07-04T11:53:25.879Z,ns_1@127.0.0.1:memcached_passwords<0.233.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:53:25.881Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.233.0>
[error_logger:info,2019-07-04T11:53:25.887Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2359.0>},
                       {name,{service_index,stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["@index-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.888Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2381.0>},
                       {name,{service_index,stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["@index-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:25.888Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2019-07-04T11:53:25.899Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2019-07-04T11:53:25.903Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2383.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:53:25.904Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.2384.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-07-04T11:53:25.904Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2382.0>,docs_kv_sup}
             started: [{pid,<0.2383.0>},
                       {name,doc_replicator},
                       {mfargs,{capi_ddoc_manager,start_replicator,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.904Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2382.0>,docs_kv_sup}
             started: [{pid,<0.2384.0>},
                       {name,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.918Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.274.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:53:25.925Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2383.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.275.0>
[ns_server:debug,2019-07-04T11:53:25.925Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.2384.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.275.0>
[error_logger:info,2019-07-04T11:53:25.925Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.275.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["app",<0.2383.0>,<0.2384.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.926Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2382.0>,docs_kv_sup}
             started: [{pid,<12396.273.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:53:25.948Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2382.0>,docs_kv_sup}
             started: [{pid,<12396.277.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.968Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2382.0>,docs_kv_sup}
             started: [{pid,<12396.282.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.968Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2382.0>},
                       {name,{docs_kv_sup,"app"}},
                       {mfargs,{docs_kv_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:53:25.978Z,ns_1@127.0.0.1:ns_memcached-app<0.2387.0>:ns_memcached:init:158]Starting ns_memcached
[ns_server:debug,2019-07-04T11:53:25.978Z,ns_1@127.0.0.1:<0.2388.0>:ns_memcached:run_connect_phase:181]Started 'connecting' phase of ns_memcached-app. Parent is <0.2387.0>
[error_logger:info,2019-07-04T11:53:25.978Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2386.0>,ns_memcached_sup}
             started: [{pid,<0.2387.0>},
                       {name,{ns_memcached,"app"}},
                       {mfargs,{ns_memcached,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.995Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2386.0>,ns_memcached_sup}
             started: [{pid,<0.2389.0>},
                       {name,{terse_bucket_info_uploader,"app"}},
                       {mfargs,
                           {terse_bucket_info_uploader,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:25.996Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2386.0>},
                       {name,{ns_memcached_sup,"app"}},
                       {mfargs,{ns_memcached_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:53:26.046Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2391.0>},
                       {name,{dcp_sup,"app"}},
                       {mfargs,{dcp_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[stats:error,2019-07-04T11:53:26.104Z,ns_1@127.0.0.1:<0.2183.0>:stats_reader:log_bad_responses:233]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2019-07-04T11:53:26.111Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2397.0>},
                       {name,{dcp_replication_manager,"app"}},
                       {mfargs,{dcp_replication_manager,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[stats:error,2019-07-04T11:53:26.119Z,ns_1@127.0.0.1:<0.2251.0>:stats_reader:log_bad_responses:233]Some nodes didn't respond: ['ns_1@127.0.0.1']
[stats:error,2019-07-04T11:53:26.121Z,ns_1@127.0.0.1:<0.2249.0>:stats_reader:log_bad_responses:233]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2019-07-04T11:53:26.130Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2401.0>},
                       {name,{replication_manager,"app"}},
                       {mfargs,{replication_manager,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:53:26.132Z,ns_1@127.0.0.1:ns_memcached-app<0.2387.0>:ns_memcached:ensure_bucket:1264]Created bucket "app" with config string "max_size=1042284544;dbname=/opt/couchbase/var/lib/couchbase/data/app;backend=couchdb;couch_bucket=app;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/app/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=5b6ce58456b7220bfb025f341cb20648;conflict_resolution_type=seqno;bucket_type=persistent;item_eviction_policy=value_only;max_ttl=0;ht_locks=47;compression_mode=off;failpartialwarmup=false"
[ns_server:info,2019-07-04T11:53:26.133Z,ns_1@127.0.0.1:ns_memcached-app<0.2387.0>:ns_memcached:handle_cast:647]Main ns_memcached connection established: {ok,#Port<0.7490>}
[ns_server:debug,2019-07-04T11:53:26.209Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@fts-cbauth",admin}
[ns_server:debug,2019-07-04T11:53:26.210Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2019-07-04T11:53:26.211Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:debug,2019-07-04T11:53:26.213Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.280.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "app" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-app-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-fun-1-',3,
                           [{file,"src/ns_heart.erl"},{line,292}]},
                 {lists,foldl,3,[{file,"lists.erl"},{line,1248}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,291}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2019-07-04T11:53:26.215Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2383.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2019-07-04T11:53:26.348Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-app'}
             started: [{pid,<0.2423.0>},
                       {name,rebalance_subprocesses_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               ['rebalance_subprocesses_registry-app',
                                [{terminate_command,kill}]]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:53:26.353Z,ns_1@127.0.0.1:janitor_agent-app<0.2424.0>:janitor_agent:read_flush_counter:918]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2019-07-04T11:53:26.354Z,ns_1@127.0.0.1:janitor_agent-app<0.2424.0>:janitor_agent:read_flush_counter_from_config:925]Initialized flushseq 0 from bucket config
[error_logger:info,2019-07-04T11:53:26.356Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-app'}
             started: [{pid,<0.2424.0>},
                       {name,janitor_agent},
                       {mfargs,{janitor_agent,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.371Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2422.0>},
                       {name,{janitor_agent_sup,"app"}},
                       {mfargs,{janitor_agent_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.399Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2431.0>},
                       {name,{stats_collector,"app"}},
                       {mfargs,{stats_collector,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.433Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2435.0>},
                       {name,{stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.433Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2450.0>},
                       {name,{stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.447Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2451.0>},
                       {name,{goxdcr_stats_collector,"app"}},
                       {mfargs,{goxdcr_stats_collector,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.451Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2453.0>},
                       {name,{goxdcr_stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["@xdcr-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.452Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2455.0>},
                       {name,{goxdcr_stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["@xdcr-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.452Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.2456.0>},
                       {name,{failover_safeness_level,"app"}},
                       {mfargs,{failover_safeness_level,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:53:26.452Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.2349.0>},
                       {name,{single_bucket_kv_sup,"app"}},
                       {mfargs,{single_bucket_kv_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:53:26.620Z,ns_1@127.0.0.1:<0.469.0>:auto_failover:log_down_nodes_reason:382]Node 'ns_1@127.0.0.1' is considered down. Reason:"The data service did not respond for the duration of the auto-failover threshold. Either none of the buckets have warmed up or there is an issue with the data service. "
[ns_server:debug,2019-07-04T11:53:26.620Z,ns_1@127.0.0.1:<0.469.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            0,up,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              0,half_down,false}
[user:info,2019-07-04T11:53:26.677Z,ns_1@127.0.0.1:ns_memcached-app<0.2387.0>:ns_memcached:handle_cast:676]Bucket "app" loaded on node 'ns_1@127.0.0.1' in 0 seconds.
[ns_server:debug,2019-07-04T11:53:26.862Z,ns_1@127.0.0.1:janitor_agent-app<0.2424.0>:dcp_sup:nuke:104]Nuking DCP replicators for bucket "app":
[]
[ns_server:info,2019-07-04T11:53:26.874Z,ns_1@127.0.0.1:<0.2347.0>:ns_janitor:cleanup_with_membase_bucket_check_map:95]janitor decided to generate initial vbucket map
[ns_server:debug,2019-07-04T11:53:26.881Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.280.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "app" stats:
{error,no_samples}

[ns_server:warn,2019-07-04T11:53:26.883Z,ns_1@127.0.0.1:kv_monitor<0.1383.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:53:26.904Z,ns_1@127.0.0.1:<0.2347.0>:mb_map:generate_map_old:378]Natural map score: {1024,0}
[ns_server:debug,2019-07-04T11:53:26.913Z,ns_1@127.0.0.1:<0.2347.0>:mb_map:generate_map_old:385]Rnd maps scores: {1024,0}, {1024,0}
[ns_server:debug,2019-07-04T11:53:26.913Z,ns_1@127.0.0.1:<0.2347.0>:mb_map:generate_map_old:392]Considering 1 maps:
[{1024,0}]
[ns_server:debug,2019-07-04T11:53:26.913Z,ns_1@127.0.0.1:<0.2347.0>:mb_map:generate_map_old:397]Best map score: {1024,0} (true,true,true)
[ns_server:debug,2019-07-04T11:53:26.916Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{24,63729460406}}]}]
[ns_server:debug,2019-07-04T11:53:26.920Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([buckets,vbucket_map_history,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:53:26.928Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2383.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2019-07-04T11:53:26.932Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
vbucket_map_history ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460406}}]},
 {[['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1'|...],
   [...]|...],
  [{replication_topology,star},{tags,undefined},{max_slaves,10}]}]
[ns_server:debug,2019-07-04T11:53:26.932Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{25,63729460406}}]}]
[ns_server:info,2019-07-04T11:53:26.945Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 0 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.945Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.945Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 2 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.945Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 3 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 4 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 5 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 6 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 7 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 8 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{6,63729460406}}],
 {configs,[{"app",
            [{map,[{0,[],['ns_1@127.0.0.1',undefined]},
                   {1,[],['ns_1@127.0.0.1',undefined]},
                   {2,[],['ns_1@127.0.0.1',undefined]},
                   {3,[],['ns_1@127.0.0.1',undefined]},
                   {4,[],['ns_1@127.0.0.1',undefined]},
                   {5,[],['ns_1@127.0.0.1',undefined]},
                   {6,[],['ns_1@127.0.0.1',undefined]},
                   {7,[],['ns_1@127.0.0.1',undefined]},
                   {8,[],['ns_1@127.0.0.1',undefined]},
                   {9,[],['ns_1@127.0.0.1',undefined]},
                   {10,[],['ns_1@127.0.0.1',undefined]},
                   {11,[],['ns_1@127.0.0.1',undefined]},
                   {12,[],['ns_1@127.0.0.1',undefined]},
                   {13,[],['ns_1@127.0.0.1',undefined]},
                   {14,[],['ns_1@127.0.0.1',undefined]},
                   {15,[],['ns_1@127.0.0.1',undefined]},
                   {16,[],['ns_1@127.0.0.1',undefined]},
                   {17,[],['ns_1@127.0.0.1',undefined]},
                   {18,[],['ns_1@127.0.0.1',undefined]},
                   {19,[],['ns_1@127.0.0.1',undefined]},
                   {20,[],['ns_1@127.0.0.1',undefined]},
                   {21,[],['ns_1@127.0.0.1',undefined]},
                   {22,[],['ns_1@127.0.0.1',undefined]},
                   {23,[],['ns_1@127.0.0.1',undefined]},
                   {24,[],['ns_1@127.0.0.1',undefined]},
                   {25,[],['ns_1@127.0.0.1',undefined]},
                   {26,[],['ns_1@127.0.0.1',undefined]},
                   {27,[],['ns_1@127.0.0.1',undefined]},
                   {28,[],['ns_1@127.0.0.1',undefined]},
                   {29,[],['ns_1@127.0.0.1',undefined]},
                   {30,[],['ns_1@127.0.0.1',undefined]},
                   {31,[],['ns_1@127.0.0.1',undefined]},
                   {32,[],['ns_1@127.0.0.1',undefined]},
                   {33,[],['ns_1@127.0.0.1',undefined]},
                   {34,[],['ns_1@127.0.0.1',undefined]},
                   {35,[],['ns_1@127.0.0.1',undefined]},
                   {36,[],['ns_1@127.0.0.1',undefined]},
                   {37,[],['ns_1@127.0.0.1',undefined]},
                   {38,[],['ns_1@127.0.0.1',undefined]},
                   {39,[],['ns_1@127.0.0.1',undefined]},
                   {40,[],['ns_1@127.0.0.1',undefined]},
                   {41,[],['ns_1@127.0.0.1',undefined]},
                   {42,[],['ns_1@127.0.0.1',undefined]},
                   {43,[],['ns_1@127.0.0.1',undefined]},
                   {44,[],['ns_1@127.0.0.1',undefined]},
                   {45,[],['ns_1@127.0.0.1',undefined]},
                   {46,[],['ns_1@127.0.0.1',undefined]},
                   {47,[],['ns_1@127.0.0.1',undefined]},
                   {48,[],['ns_1@127.0.0.1',undefined]},
                   {49,[],['ns_1@127.0.0.1',undefined]},
                   {50,[],['ns_1@127.0.0.1',undefined]},
                   {51,[],['ns_1@127.0.0.1',undefined]},
                   {52,[],['ns_1@127.0.0.1',undefined]},
                   {53,[],['ns_1@127.0.0.1',undefined]},
                   {54,[],['ns_1@127.0.0.1',undefined]},
                   {55,[],['ns_1@127.0.0.1',undefined]},
                   {56,[],['ns_1@127.0.0.1',undefined]},
                   {57,[],['ns_1@127.0.0.1',undefined]},
                   {58,[],['ns_1@127.0.0.1',undefined]},
                   {59,[],['ns_1@127.0.0.1',undefined]},
                   {60,[],['ns_1@127.0.0.1',undefined]},
                   {61,[],['ns_1@127.0.0.1',undefined]},
                   {62,[],['ns_1@127.0.0.1',undefined]},
                   {63,[],['ns_1@127.0.0.1',undefined]},
                   {64,[],['ns_1@127.0.0.1',undefined]},
                   {65,[],['ns_1@127.0.0.1',undefined]},
                   {66,[],['ns_1@127.0.0.1',undefined]},
                   {67,[],['ns_1@127.0.0.1',undefined]},
                   {68,[],['ns_1@127.0.0.1',undefined]},
                   {69,[],['ns_1@127.0.0.1',undefined]},
                   {70,[],['ns_1@127.0.0.1',undefined]},
                   {71,[],['ns_1@127.0.0.1',undefined]},
                   {72,[],['ns_1@127.0.0.1',undefined]},
                   {73,[],['ns_1@127.0.0.1',undefined]},
                   {74,[],['ns_1@127.0.0.1',undefined]},
                   {75,[],['ns_1@127.0.0.1',undefined]},
                   {76,[],['ns_1@127.0.0.1',undefined]},
                   {77,[],['ns_1@127.0.0.1',undefined]},
                   {78,[],['ns_1@127.0.0.1',undefined]},
                   {79,[],['ns_1@127.0.0.1',undefined]},
                   {80,[],['ns_1@127.0.0.1',undefined]},
                   {81,[],['ns_1@127.0.0.1',undefined]},
                   {82,[],['ns_1@127.0.0.1',undefined]},
                   {83,[],['ns_1@127.0.0.1',undefined]},
                   {84,[],['ns_1@127.0.0.1'|...]},
                   {85,[],[...]},
                   {86,[],...},
                   {87,...},
                   {...}|...]},
             {fastForwardMap,[]},
             {repl_type,dcp},
             {uuid,<<"5b6ce58456b7220bfb025f341cb20648">>},
             {auth_type,sasl},
             {num_replicas,1},
             {replica_index,false},
             {ram_quota,1042284544},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,off},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[ns_server:info,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 9 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{26,63729460406}}]}]
[ns_server:info,2019-07-04T11:53:26.946Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 10 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.947Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 11 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.947Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 12 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.947Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 13 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.947Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 14 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.947Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 15 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-07-04T11:53:26.947Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{7,63729460406}}],
 {configs,[{"app",
            [{map,[]},
             {fastForwardMap,[]},
             {repl_type,dcp},
             {uuid,<<"5b6ce58456b7220bfb025f341cb20648">>},
             {auth_type,sasl},
             {num_replicas,1},
             {replica_index,false},
             {ram_quota,1042284544},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,off},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"},
             {map_opts_hash,133465355}]}]}]
[ns_server:info,2019-07-04T11:53:26.947Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 16 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 17 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 18 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 19 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 20 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 21 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 22 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 23 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.948Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 24 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 25 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 26 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 27 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 28 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 29 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 30 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 31 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 32 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 33 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 34 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 35 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 36 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 37 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 38 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 39 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 40 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 41 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 42 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.949Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 43 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 44 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 45 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 46 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 47 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 48 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 49 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 50 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 51 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 52 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.950Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 53 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 54 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 55 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 56 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 57 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 58 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 59 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 60 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 61 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 62 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 63 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 64 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.951Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 65 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 66 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 67 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 68 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 69 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 70 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 71 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 72 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 73 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 74 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 75 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.952Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 76 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.953Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 77 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.953Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 78 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.953Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 79 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.953Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 80 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.953Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 81 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.953Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 82 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.966Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 83 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 84 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 85 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 86 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 87 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 88 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 89 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 90 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 91 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 92 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 93 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 94 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 95 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.967Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 96 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 97 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 98 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 99 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 100 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 101 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 102 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 103 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 104 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 105 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 106 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 107 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.968Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 108 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.969Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 109 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.969Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 110 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.969Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 111 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 112 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 113 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 114 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 115 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 116 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 117 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 118 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 119 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 120 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.970Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 121 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.971Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 122 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.971Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 123 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.971Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 124 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.971Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 125 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.971Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 126 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.971Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 127 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.971Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 128 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.972Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 129 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.972Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 130 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.973Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 131 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.973Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 132 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.974Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 133 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.974Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 134 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.974Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 135 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.974Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 136 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.975Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 137 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.975Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 138 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.975Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 139 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.975Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 140 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.975Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 141 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.975Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 142 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.976Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 143 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.978Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 144 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.978Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 145 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.978Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 146 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.978Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 147 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.978Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 148 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.979Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 149 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.979Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 150 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.979Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 151 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.979Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 152 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.979Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 153 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 154 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 155 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 156 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 157 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 158 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 159 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 160 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 161 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 162 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.980Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 163 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 164 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 165 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 166 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 167 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 168 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 169 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 170 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 171 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 172 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 173 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 174 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 175 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 176 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 177 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.981Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 178 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 179 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 180 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 181 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 182 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 183 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 184 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 185 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 186 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 187 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 188 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 189 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 190 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 191 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 192 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 193 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.982Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 194 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 195 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 196 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 197 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 198 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 199 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 200 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 201 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 202 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 203 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 204 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 205 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.983Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 206 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 207 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 208 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 209 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 210 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 211 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 212 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 213 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 214 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 215 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 216 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 217 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.984Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 218 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 219 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 220 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 221 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 222 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 223 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 224 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 225 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 226 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 227 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 228 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 229 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 230 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 231 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.985Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 232 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 233 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 234 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 235 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 236 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 237 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 238 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 239 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 240 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.986Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 241 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 242 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 243 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 244 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 245 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 246 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 247 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 248 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 249 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.987Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 250 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.988Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 251 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.988Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 252 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.988Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 253 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.988Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 254 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 255 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 256 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 257 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 258 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 259 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 260 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 261 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 262 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 263 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 264 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 265 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 266 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.989Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 267 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 268 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 269 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 270 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 271 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 272 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 273 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 274 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 275 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 276 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.990Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 277 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 278 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 279 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 280 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 281 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 282 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 283 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 284 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.991Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 285 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 286 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 287 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 288 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 289 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 290 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 291 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 292 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 293 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 294 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 295 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 296 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 297 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.992Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 298 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 299 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 300 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 301 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 302 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 303 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 304 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 305 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 306 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 307 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 308 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 309 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 310 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 311 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.993Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 312 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 313 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 314 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 315 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 316 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 317 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 318 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 319 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 320 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 321 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 322 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 323 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 324 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.994Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 325 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 326 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 327 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 328 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 329 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 330 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 331 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 332 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 333 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 334 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 335 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 336 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 337 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 338 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 339 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.995Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 340 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 341 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 342 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 343 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 344 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 345 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 346 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 347 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 348 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 349 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 350 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 351 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 352 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 353 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 354 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.996Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 355 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 356 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 357 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 358 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 359 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 360 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 361 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 362 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 363 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 364 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.997Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 365 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 366 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 367 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 368 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 369 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 370 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 371 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 372 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 373 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 374 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 375 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:26.998Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 376 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 377 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 378 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 379 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 380 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 381 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 382 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 383 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 384 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 385 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 386 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 387 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 388 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 389 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.000Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 390 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 391 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 392 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 393 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 394 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 395 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 396 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 397 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 398 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 399 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 400 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 401 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 402 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 403 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 404 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 405 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.001Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 406 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 407 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 408 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 409 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 410 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 411 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 412 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 413 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 414 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 415 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 416 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 417 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 418 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 419 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 420 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.002Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 421 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 422 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 423 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 424 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 425 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 426 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 427 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 428 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 429 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 430 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 431 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 432 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 433 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 434 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.003Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 435 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 436 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 437 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 438 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 439 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 440 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 441 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 442 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 443 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 444 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 445 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 446 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 447 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 448 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 449 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 450 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 451 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.004Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 452 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 453 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 454 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 455 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 456 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 457 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 458 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 459 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 460 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 461 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 462 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 463 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 464 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 465 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 466 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 467 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 468 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.005Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 469 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 470 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 471 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 472 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 473 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 474 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 475 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 476 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 477 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 478 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 479 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 480 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 481 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 482 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.006Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 483 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 484 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 485 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 486 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 487 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 488 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 489 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 490 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 491 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.007Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 492 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 493 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 494 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 495 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 496 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 497 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 498 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 499 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 500 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.008Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 501 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 502 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 503 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 504 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 505 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 506 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 507 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 508 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 509 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 510 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 511 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 512 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.009Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 513 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 514 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 515 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 516 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 517 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 518 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 519 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 520 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 521 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 522 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 523 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 524 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 525 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 526 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.010Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 527 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 528 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 529 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 530 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 531 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 532 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 533 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 534 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 535 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 536 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 537 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 538 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 539 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 540 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 541 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 542 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.011Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 543 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 544 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 545 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 546 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 547 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 548 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 549 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 550 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 551 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 552 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 553 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 554 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 555 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 556 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 557 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 558 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.012Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 559 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 560 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 561 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 562 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 563 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 564 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 565 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 566 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 567 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 568 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 569 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 570 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 571 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 572 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 573 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 574 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.013Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 575 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 576 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 577 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 578 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 579 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 580 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 581 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 582 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 583 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 584 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.014Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 585 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 586 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 587 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 588 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 589 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 590 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 591 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 592 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 593 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 594 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 595 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 596 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 597 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 598 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 599 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 600 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.015Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 601 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 602 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 603 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 604 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 605 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 606 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 607 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 608 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 609 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 610 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 611 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 612 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 613 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 614 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 615 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 616 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.016Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 617 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 618 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 619 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 620 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 621 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 622 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 623 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 624 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 625 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 626 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 627 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 628 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 629 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.017Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 630 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 631 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 632 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 633 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 634 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 635 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 636 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 637 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 638 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 639 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 640 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 641 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 642 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.018Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 643 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 644 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 645 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 646 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 647 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 648 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 649 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 650 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 651 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 652 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 653 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 654 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 655 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 656 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 657 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 658 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 659 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.019Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 660 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 661 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 662 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 663 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 664 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 665 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 666 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 667 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 668 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 669 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 670 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 671 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 672 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 673 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 674 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 675 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 676 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.020Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 677 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 678 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 679 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 680 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 681 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 682 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 683 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 684 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 685 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 686 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 687 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 688 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 689 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 690 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 691 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.021Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 692 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 693 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 694 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 695 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 696 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 697 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 698 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 699 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 700 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 701 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 702 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 703 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 704 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.022Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 705 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 706 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 707 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 708 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 709 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 710 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 711 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 712 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 713 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 714 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 715 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 716 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 717 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 718 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 719 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 720 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.023Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 721 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 722 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 723 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 724 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 725 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 726 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 727 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 728 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 729 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 730 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 731 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 732 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 733 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 734 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 735 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 736 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 737 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 738 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 739 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 740 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 741 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.024Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 742 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 743 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 744 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 745 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 746 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 747 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 748 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 749 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 750 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 751 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 752 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 753 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 754 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 755 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 756 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 757 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 758 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 759 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 760 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.025Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 761 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 762 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 763 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 764 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 765 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 766 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 767 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 768 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 769 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 770 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 771 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 772 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 773 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 774 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 775 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 776 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 777 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 778 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 779 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 780 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.026Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 781 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 782 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 783 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 784 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 785 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 786 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 787 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 788 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 789 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 790 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 791 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 792 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 793 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 794 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 795 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 796 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 797 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 798 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 799 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 800 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 801 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.027Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 802 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 803 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 804 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 805 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 806 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 807 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 808 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 809 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 810 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 811 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 812 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 813 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 814 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 815 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.028Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 816 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 817 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 818 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 819 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 820 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 821 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 822 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 823 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 824 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 825 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 826 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 827 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 828 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 829 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 830 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 831 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 832 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 833 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.029Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 834 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 835 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 836 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 837 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 838 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 839 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 840 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 841 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 842 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 843 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 844 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 845 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 846 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 847 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 848 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 849 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.030Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 850 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 851 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 852 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 853 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 854 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 855 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 856 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 857 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 858 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 859 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 860 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 861 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 862 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 863 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 864 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 865 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.031Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 866 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 867 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 868 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 869 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 870 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 871 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 872 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 873 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.032Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 874 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.033Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 875 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.033Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 876 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.033Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 877 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.033Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 878 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.033Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 879 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 880 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 881 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 882 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 883 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 884 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 885 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 886 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 887 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 888 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 889 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 890 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 891 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 892 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 893 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 894 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.034Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 895 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 896 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 897 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 898 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 899 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 900 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 901 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 902 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 903 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 904 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 905 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 906 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 907 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 908 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 909 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 910 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.035Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 911 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 912 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 913 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 914 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 915 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 916 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 917 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 918 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 919 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 920 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 921 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 922 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 923 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 924 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 925 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 926 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.036Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 927 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 928 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 929 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 930 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 931 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 932 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 933 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 934 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 935 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 936 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 937 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 938 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.037Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 939 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 940 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 941 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 942 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 943 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 944 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 945 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 946 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 947 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 948 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 949 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 950 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 951 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 952 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 953 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 954 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 955 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.038Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 956 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 957 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 958 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 959 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 960 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 961 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 962 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 963 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 964 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 965 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 966 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 967 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 968 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 969 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 970 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 971 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.039Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 972 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 973 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 974 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 975 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 976 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 977 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 978 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 979 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 980 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 981 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 982 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 983 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 984 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 985 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 986 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.040Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 987 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 988 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 989 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 990 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 991 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 992 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 993 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 994 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 995 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 996 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 997 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.041Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 998 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 999 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1000 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1001 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1002 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1003 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1004 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1005 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1006 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1007 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1008 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1009 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1010 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1011 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1012 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.042Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1013 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1014 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1015 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1016 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1017 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1018 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1019 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1020 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1021 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1022 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.043Z,ns_1@127.0.0.1:<0.2530.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1023 in "app" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-07-04T11:53:27.067Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1023 state to active
[ns_server:info,2019-07-04T11:53:27.067Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1022 state to active
[ns_server:info,2019-07-04T11:53:27.070Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1021 state to active
[ns_server:info,2019-07-04T11:53:27.071Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1020 state to active
[ns_server:info,2019-07-04T11:53:27.073Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1019 state to active
[ns_server:info,2019-07-04T11:53:27.074Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1018 state to active
[ns_server:info,2019-07-04T11:53:27.079Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1017 state to active
[ns_server:info,2019-07-04T11:53:27.080Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1016 state to active
[ns_server:info,2019-07-04T11:53:27.082Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1015 state to active
[ns_server:info,2019-07-04T11:53:27.083Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1014 state to active
[ns_server:info,2019-07-04T11:53:27.084Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1013 state to active
[ns_server:info,2019-07-04T11:53:27.085Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1012 state to active
[ns_server:info,2019-07-04T11:53:27.085Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1011 state to active
[ns_server:info,2019-07-04T11:53:27.086Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1010 state to active
[ns_server:info,2019-07-04T11:53:27.087Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1009 state to active
[ns_server:info,2019-07-04T11:53:27.088Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1008 state to active
[ns_server:info,2019-07-04T11:53:27.089Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1007 state to active
[ns_server:info,2019-07-04T11:53:27.090Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1006 state to active
[ns_server:info,2019-07-04T11:53:27.092Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1005 state to active
[ns_server:info,2019-07-04T11:53:27.092Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1004 state to active
[ns_server:info,2019-07-04T11:53:27.093Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1003 state to active
[ns_server:info,2019-07-04T11:53:27.094Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1002 state to active
[ns_server:info,2019-07-04T11:53:27.095Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1001 state to active
[ns_server:info,2019-07-04T11:53:27.095Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 1000 state to active
[ns_server:info,2019-07-04T11:53:27.096Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 999 state to active
[ns_server:info,2019-07-04T11:53:27.097Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 998 state to active
[ns_server:info,2019-07-04T11:53:27.099Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 997 state to active
[ns_server:info,2019-07-04T11:53:27.103Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 996 state to active
[ns_server:info,2019-07-04T11:53:27.112Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 995 state to active
[ns_server:info,2019-07-04T11:53:27.119Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 994 state to active
[ns_server:info,2019-07-04T11:53:27.120Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 993 state to active
[ns_server:info,2019-07-04T11:53:27.121Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 992 state to active
[ns_server:info,2019-07-04T11:53:27.122Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 991 state to active
[ns_server:info,2019-07-04T11:53:27.123Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 990 state to active
[ns_server:info,2019-07-04T11:53:27.124Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 989 state to active
[ns_server:info,2019-07-04T11:53:27.125Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 988 state to active
[ns_server:info,2019-07-04T11:53:27.127Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 987 state to active
[ns_server:info,2019-07-04T11:53:27.128Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 986 state to active
[ns_server:info,2019-07-04T11:53:27.130Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 985 state to active
[ns_server:info,2019-07-04T11:53:27.138Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 984 state to active
[ns_server:info,2019-07-04T11:53:27.142Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 983 state to active
[ns_server:info,2019-07-04T11:53:27.143Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 982 state to active
[ns_server:info,2019-07-04T11:53:27.144Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 981 state to active
[ns_server:info,2019-07-04T11:53:27.146Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 980 state to active
[ns_server:info,2019-07-04T11:53:27.169Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 979 state to active
[ns_server:info,2019-07-04T11:53:27.176Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 978 state to active
[ns_server:info,2019-07-04T11:53:27.182Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 977 state to active
[ns_server:info,2019-07-04T11:53:27.183Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 976 state to active
[ns_server:info,2019-07-04T11:53:27.184Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 975 state to active
[ns_server:info,2019-07-04T11:53:27.185Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 974 state to active
[ns_server:info,2019-07-04T11:53:27.186Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 973 state to active
[ns_server:info,2019-07-04T11:53:27.187Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 972 state to active
[ns_server:info,2019-07-04T11:53:27.188Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 971 state to active
[ns_server:info,2019-07-04T11:53:27.192Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 970 state to active
[ns_server:info,2019-07-04T11:53:27.193Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 969 state to active
[ns_server:info,2019-07-04T11:53:27.194Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 968 state to active
[ns_server:info,2019-07-04T11:53:27.194Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 967 state to active
[ns_server:info,2019-07-04T11:53:27.195Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 966 state to active
[ns_server:info,2019-07-04T11:53:27.196Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 965 state to active
[ns_server:info,2019-07-04T11:53:27.196Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 964 state to active
[ns_server:info,2019-07-04T11:53:27.197Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 963 state to active
[ns_server:info,2019-07-04T11:53:27.272Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 962 state to active
[ns_server:info,2019-07-04T11:53:27.273Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 961 state to active
[ns_server:info,2019-07-04T11:53:27.274Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 960 state to active
[ns_server:info,2019-07-04T11:53:27.275Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 959 state to active
[ns_server:info,2019-07-04T11:53:27.276Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 958 state to active
[ns_server:info,2019-07-04T11:53:27.277Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 957 state to active
[ns_server:info,2019-07-04T11:53:27.278Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 956 state to active
[ns_server:info,2019-07-04T11:53:27.282Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 955 state to active
[ns_server:info,2019-07-04T11:53:27.284Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 954 state to active
[ns_server:info,2019-07-04T11:53:27.287Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 953 state to active
[ns_server:info,2019-07-04T11:53:27.289Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 952 state to active
[ns_server:info,2019-07-04T11:53:27.290Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 951 state to active
[ns_server:info,2019-07-04T11:53:27.292Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 950 state to active
[ns_server:info,2019-07-04T11:53:27.293Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 949 state to active
[ns_server:info,2019-07-04T11:53:27.294Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 948 state to active
[ns_server:info,2019-07-04T11:53:27.297Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 947 state to active
[ns_server:info,2019-07-04T11:53:27.300Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 946 state to active
[ns_server:info,2019-07-04T11:53:27.303Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 945 state to active
[ns_server:info,2019-07-04T11:53:27.305Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 944 state to active
[ns_server:info,2019-07-04T11:53:27.310Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 943 state to active
[ns_server:info,2019-07-04T11:53:27.311Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 942 state to active
[ns_server:info,2019-07-04T11:53:27.312Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 941 state to active
[ns_server:info,2019-07-04T11:53:27.312Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 940 state to active
[ns_server:info,2019-07-04T11:53:27.313Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 939 state to active
[ns_server:info,2019-07-04T11:53:27.314Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 938 state to active
[ns_server:info,2019-07-04T11:53:27.314Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 937 state to active
[ns_server:info,2019-07-04T11:53:27.315Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 936 state to active
[ns_server:info,2019-07-04T11:53:27.315Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 935 state to active
[ns_server:info,2019-07-04T11:53:27.316Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 934 state to active
[ns_server:info,2019-07-04T11:53:27.318Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 933 state to active
[ns_server:info,2019-07-04T11:53:27.318Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 932 state to active
[ns_server:info,2019-07-04T11:53:27.319Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 931 state to active
[ns_server:info,2019-07-04T11:53:27.320Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 930 state to active
[ns_server:info,2019-07-04T11:53:27.321Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 929 state to active
[ns_server:info,2019-07-04T11:53:27.322Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 928 state to active
[ns_server:info,2019-07-04T11:53:27.322Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 927 state to active
[ns_server:info,2019-07-04T11:53:27.323Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 926 state to active
[ns_server:info,2019-07-04T11:53:27.324Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 925 state to active
[ns_server:info,2019-07-04T11:53:27.324Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 924 state to active
[ns_server:info,2019-07-04T11:53:27.325Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 923 state to active
[ns_server:info,2019-07-04T11:53:27.326Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 922 state to active
[ns_server:info,2019-07-04T11:53:27.346Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 921 state to active
[ns_server:info,2019-07-04T11:53:27.349Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 920 state to active
[ns_server:info,2019-07-04T11:53:27.351Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 919 state to active
[ns_server:info,2019-07-04T11:53:27.352Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 918 state to active
[ns_server:info,2019-07-04T11:53:27.354Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 917 state to active
[ns_server:info,2019-07-04T11:53:27.355Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 916 state to active
[ns_server:info,2019-07-04T11:53:27.356Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 915 state to active
[ns_server:info,2019-07-04T11:53:27.357Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 914 state to active
[ns_server:info,2019-07-04T11:53:27.366Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 913 state to active
[ns_server:info,2019-07-04T11:53:27.418Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 912 state to active
[ns_server:info,2019-07-04T11:53:27.419Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 911 state to active
[ns_server:info,2019-07-04T11:53:27.433Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 910 state to active
[ns_server:info,2019-07-04T11:53:27.436Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 909 state to active
[ns_server:info,2019-07-04T11:53:27.445Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 908 state to active
[ns_server:info,2019-07-04T11:53:27.447Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 907 state to active
[ns_server:info,2019-07-04T11:53:27.452Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 906 state to active
[ns_server:info,2019-07-04T11:53:27.454Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 905 state to active
[ns_server:info,2019-07-04T11:53:27.460Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 904 state to active
[ns_server:info,2019-07-04T11:53:27.461Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 903 state to active
[ns_server:info,2019-07-04T11:53:27.462Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 902 state to active
[ns_server:info,2019-07-04T11:53:27.463Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 901 state to active
[ns_server:info,2019-07-04T11:53:27.464Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 900 state to active
[ns_server:info,2019-07-04T11:53:27.465Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 899 state to active
[ns_server:info,2019-07-04T11:53:27.471Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 898 state to active
[ns_server:info,2019-07-04T11:53:27.472Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 897 state to active
[ns_server:info,2019-07-04T11:53:27.472Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 896 state to active
[ns_server:info,2019-07-04T11:53:27.473Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 895 state to active
[ns_server:info,2019-07-04T11:53:27.474Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 894 state to active
[ns_server:info,2019-07-04T11:53:27.475Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 893 state to active
[ns_server:info,2019-07-04T11:53:27.476Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 892 state to active
[ns_server:info,2019-07-04T11:53:27.480Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 891 state to active
[ns_server:info,2019-07-04T11:53:27.483Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 890 state to active
[ns_server:info,2019-07-04T11:53:27.484Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 889 state to active
[ns_server:info,2019-07-04T11:53:27.486Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 888 state to active
[ns_server:info,2019-07-04T11:53:27.488Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 887 state to active
[ns_server:info,2019-07-04T11:53:27.489Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 886 state to active
[ns_server:info,2019-07-04T11:53:27.491Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 885 state to active
[ns_server:info,2019-07-04T11:53:27.492Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 884 state to active
[ns_server:info,2019-07-04T11:53:27.494Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 883 state to active
[ns_server:info,2019-07-04T11:53:27.496Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 882 state to active
[ns_server:info,2019-07-04T11:53:27.499Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 881 state to active
[ns_server:info,2019-07-04T11:53:27.501Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 880 state to active
[ns_server:info,2019-07-04T11:53:27.509Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 879 state to active
[ns_server:info,2019-07-04T11:53:27.515Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 878 state to active
[ns_server:info,2019-07-04T11:53:27.528Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 877 state to active
[ns_server:info,2019-07-04T11:53:27.529Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 876 state to active
[ns_server:info,2019-07-04T11:53:27.530Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 875 state to active
[ns_server:info,2019-07-04T11:53:27.531Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 874 state to active
[ns_server:info,2019-07-04T11:53:27.532Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 873 state to active
[ns_server:info,2019-07-04T11:53:27.533Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 872 state to active
[ns_server:info,2019-07-04T11:53:27.534Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 871 state to active
[ns_server:info,2019-07-04T11:53:27.534Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 870 state to active
[ns_server:info,2019-07-04T11:53:27.535Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 869 state to active
[ns_server:info,2019-07-04T11:53:27.536Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 868 state to active
[ns_server:info,2019-07-04T11:53:27.552Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 867 state to active
[ns_server:info,2019-07-04T11:53:27.558Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 866 state to active
[ns_server:info,2019-07-04T11:53:27.564Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 865 state to active
[ns_server:info,2019-07-04T11:53:27.567Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 864 state to active
[ns_server:info,2019-07-04T11:53:27.573Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 863 state to active
[ns_server:info,2019-07-04T11:53:27.574Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 862 state to active
[ns_server:info,2019-07-04T11:53:27.575Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 861 state to active
[ns_server:info,2019-07-04T11:53:27.579Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 860 state to active
[ns_server:info,2019-07-04T11:53:27.580Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 859 state to active
[ns_server:info,2019-07-04T11:53:27.585Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 858 state to active
[ns_server:info,2019-07-04T11:53:27.594Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 857 state to active
[ns_server:info,2019-07-04T11:53:27.599Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 856 state to active
[ns_server:info,2019-07-04T11:53:27.637Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 855 state to active
[ns_server:debug,2019-07-04T11:53:27.638Z,ns_1@127.0.0.1:<0.469.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            0,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              1,half_down,false}
[ns_server:info,2019-07-04T11:53:27.638Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 854 state to active
[ns_server:info,2019-07-04T11:53:27.640Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 853 state to active
[ns_server:info,2019-07-04T11:53:27.644Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 852 state to active
[ns_server:info,2019-07-04T11:53:27.646Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 851 state to active
[ns_server:info,2019-07-04T11:53:27.647Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 850 state to active
[ns_server:info,2019-07-04T11:53:27.648Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 849 state to active
[ns_server:info,2019-07-04T11:53:27.648Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 848 state to active
[ns_server:info,2019-07-04T11:53:27.649Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 847 state to active
[ns_server:info,2019-07-04T11:53:27.649Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 846 state to active
[ns_server:info,2019-07-04T11:53:27.649Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 845 state to active
[ns_server:info,2019-07-04T11:53:27.650Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 844 state to active
[ns_server:info,2019-07-04T11:53:27.650Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 843 state to active
[ns_server:info,2019-07-04T11:53:27.651Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 842 state to active
[ns_server:info,2019-07-04T11:53:27.651Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 841 state to active
[ns_server:info,2019-07-04T11:53:27.652Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 840 state to active
[ns_server:info,2019-07-04T11:53:27.652Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 839 state to active
[ns_server:info,2019-07-04T11:53:27.653Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 838 state to active
[ns_server:info,2019-07-04T11:53:27.653Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 837 state to active
[ns_server:info,2019-07-04T11:53:27.654Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 836 state to active
[ns_server:info,2019-07-04T11:53:27.654Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 835 state to active
[ns_server:info,2019-07-04T11:53:27.655Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 834 state to active
[ns_server:info,2019-07-04T11:53:27.655Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 833 state to active
[ns_server:info,2019-07-04T11:53:27.656Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 832 state to active
[ns_server:info,2019-07-04T11:53:27.656Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 831 state to active
[ns_server:info,2019-07-04T11:53:27.656Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 830 state to active
[ns_server:info,2019-07-04T11:53:27.657Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 829 state to active
[ns_server:info,2019-07-04T11:53:27.657Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 828 state to active
[ns_server:info,2019-07-04T11:53:27.660Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 827 state to active
[ns_server:info,2019-07-04T11:53:27.660Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 826 state to active
[ns_server:info,2019-07-04T11:53:27.661Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 825 state to active
[ns_server:info,2019-07-04T11:53:27.661Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 824 state to active
[ns_server:info,2019-07-04T11:53:27.662Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 823 state to active
[ns_server:info,2019-07-04T11:53:27.662Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 822 state to active
[ns_server:info,2019-07-04T11:53:27.663Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 821 state to active
[ns_server:info,2019-07-04T11:53:27.663Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 820 state to active
[ns_server:info,2019-07-04T11:53:27.664Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 819 state to active
[ns_server:info,2019-07-04T11:53:27.664Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 818 state to active
[ns_server:info,2019-07-04T11:53:27.665Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 817 state to active
[ns_server:info,2019-07-04T11:53:27.665Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 816 state to active
[ns_server:info,2019-07-04T11:53:27.665Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 815 state to active
[ns_server:info,2019-07-04T11:53:27.666Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 814 state to active
[ns_server:info,2019-07-04T11:53:27.666Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 813 state to active
[ns_server:info,2019-07-04T11:53:27.667Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 812 state to active
[ns_server:info,2019-07-04T11:53:27.667Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 811 state to active
[ns_server:info,2019-07-04T11:53:27.667Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 810 state to active
[ns_server:info,2019-07-04T11:53:27.668Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 809 state to active
[ns_server:info,2019-07-04T11:53:27.670Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 808 state to active
[ns_server:info,2019-07-04T11:53:27.670Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 807 state to active
[ns_server:info,2019-07-04T11:53:27.670Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 806 state to active
[ns_server:info,2019-07-04T11:53:27.671Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 805 state to active
[ns_server:info,2019-07-04T11:53:27.671Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 804 state to active
[ns_server:info,2019-07-04T11:53:27.672Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 803 state to active
[ns_server:info,2019-07-04T11:53:27.673Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 802 state to active
[ns_server:info,2019-07-04T11:53:27.673Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 801 state to active
[ns_server:info,2019-07-04T11:53:27.674Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 800 state to active
[ns_server:info,2019-07-04T11:53:27.675Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 799 state to active
[ns_server:info,2019-07-04T11:53:27.675Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 798 state to active
[ns_server:info,2019-07-04T11:53:27.676Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 797 state to active
[ns_server:info,2019-07-04T11:53:27.676Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 796 state to active
[ns_server:info,2019-07-04T11:53:27.677Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 795 state to active
[ns_server:info,2019-07-04T11:53:27.677Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 794 state to active
[ns_server:info,2019-07-04T11:53:27.678Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 793 state to active
[ns_server:info,2019-07-04T11:53:27.679Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 792 state to active
[ns_server:info,2019-07-04T11:53:27.693Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 791 state to active
[ns_server:info,2019-07-04T11:53:27.701Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 790 state to active
[ns_server:info,2019-07-04T11:53:27.707Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 789 state to active
[ns_server:info,2019-07-04T11:53:27.708Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 788 state to active
[ns_server:info,2019-07-04T11:53:27.709Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 787 state to active
[ns_server:info,2019-07-04T11:53:27.710Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 786 state to active
[ns_server:info,2019-07-04T11:53:27.710Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 785 state to active
[ns_server:info,2019-07-04T11:53:27.711Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 784 state to active
[ns_server:info,2019-07-04T11:53:27.712Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 783 state to active
[ns_server:info,2019-07-04T11:53:27.712Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 782 state to active
[ns_server:info,2019-07-04T11:53:27.713Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 781 state to active
[ns_server:info,2019-07-04T11:53:27.713Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 780 state to active
[ns_server:info,2019-07-04T11:53:27.714Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 779 state to active
[ns_server:info,2019-07-04T11:53:27.715Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 778 state to active
[ns_server:info,2019-07-04T11:53:27.716Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 777 state to active
[ns_server:info,2019-07-04T11:53:27.716Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 776 state to active
[ns_server:info,2019-07-04T11:53:27.717Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 775 state to active
[ns_server:info,2019-07-04T11:53:27.720Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 774 state to active
[ns_server:info,2019-07-04T11:53:27.722Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 773 state to active
[ns_server:info,2019-07-04T11:53:27.723Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 772 state to active
[ns_server:info,2019-07-04T11:53:27.723Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 771 state to active
[ns_server:info,2019-07-04T11:53:27.724Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 770 state to active
[ns_server:info,2019-07-04T11:53:27.725Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 769 state to active
[ns_server:info,2019-07-04T11:53:27.726Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 768 state to active
[ns_server:info,2019-07-04T11:53:27.727Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 767 state to active
[ns_server:info,2019-07-04T11:53:27.728Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 766 state to active
[ns_server:info,2019-07-04T11:53:27.729Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 765 state to active
[ns_server:info,2019-07-04T11:53:27.730Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 764 state to active
[ns_server:info,2019-07-04T11:53:27.733Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 763 state to active
[ns_server:info,2019-07-04T11:53:27.735Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 762 state to active
[ns_server:info,2019-07-04T11:53:27.739Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 761 state to active
[ns_server:info,2019-07-04T11:53:27.743Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 760 state to active
[ns_server:info,2019-07-04T11:53:27.745Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 759 state to active
[ns_server:info,2019-07-04T11:53:27.747Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 758 state to active
[ns_server:info,2019-07-04T11:53:27.748Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 757 state to active
[ns_server:info,2019-07-04T11:53:27.748Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 756 state to active
[ns_server:info,2019-07-04T11:53:27.749Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 755 state to active
[ns_server:info,2019-07-04T11:53:27.750Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 754 state to active
[ns_server:info,2019-07-04T11:53:27.750Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 753 state to active
[ns_server:info,2019-07-04T11:53:27.751Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 752 state to active
[ns_server:info,2019-07-04T11:53:27.751Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 751 state to active
[ns_server:info,2019-07-04T11:53:27.752Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 750 state to active
[ns_server:info,2019-07-04T11:53:27.752Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 749 state to active
[ns_server:info,2019-07-04T11:53:27.753Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 748 state to active
[ns_server:info,2019-07-04T11:53:27.754Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 747 state to active
[ns_server:info,2019-07-04T11:53:27.755Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 746 state to active
[ns_server:info,2019-07-04T11:53:27.755Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 745 state to active
[ns_server:info,2019-07-04T11:53:27.756Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 744 state to active
[ns_server:info,2019-07-04T11:53:27.766Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 743 state to active
[ns_server:info,2019-07-04T11:53:27.783Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 742 state to active
[ns_server:info,2019-07-04T11:53:27.788Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 741 state to active
[ns_server:info,2019-07-04T11:53:27.789Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 740 state to active
[ns_server:info,2019-07-04T11:53:27.790Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 739 state to active
[ns_server:info,2019-07-04T11:53:27.791Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 738 state to active
[ns_server:info,2019-07-04T11:53:27.792Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 737 state to active
[ns_server:info,2019-07-04T11:53:27.795Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 736 state to active
[ns_server:info,2019-07-04T11:53:27.798Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 735 state to active
[ns_server:info,2019-07-04T11:53:27.800Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 734 state to active
[ns_server:info,2019-07-04T11:53:27.800Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 733 state to active
[ns_server:info,2019-07-04T11:53:27.804Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 732 state to active
[ns_server:info,2019-07-04T11:53:27.805Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 731 state to active
[ns_server:info,2019-07-04T11:53:27.805Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 730 state to active
[ns_server:info,2019-07-04T11:53:27.806Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 729 state to active
[ns_server:info,2019-07-04T11:53:27.806Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 728 state to active
[ns_server:info,2019-07-04T11:53:27.809Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 727 state to active
[ns_server:info,2019-07-04T11:53:27.811Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 726 state to active
[ns_server:info,2019-07-04T11:53:27.812Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 725 state to active
[ns_server:info,2019-07-04T11:53:27.813Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 724 state to active
[ns_server:info,2019-07-04T11:53:27.813Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 723 state to active
[ns_server:info,2019-07-04T11:53:27.814Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 722 state to active
[ns_server:info,2019-07-04T11:53:27.815Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 721 state to active
[ns_server:info,2019-07-04T11:53:27.815Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 720 state to active
[ns_server:info,2019-07-04T11:53:27.816Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 719 state to active
[ns_server:info,2019-07-04T11:53:27.816Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 718 state to active
[ns_server:info,2019-07-04T11:53:27.817Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 717 state to active
[ns_server:info,2019-07-04T11:53:27.817Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 716 state to active
[ns_server:info,2019-07-04T11:53:27.818Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 715 state to active
[ns_server:info,2019-07-04T11:53:27.818Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 714 state to active
[ns_server:info,2019-07-04T11:53:27.819Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 713 state to active
[ns_server:info,2019-07-04T11:53:27.819Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 712 state to active
[ns_server:info,2019-07-04T11:53:27.820Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 711 state to active
[ns_server:info,2019-07-04T11:53:27.822Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 710 state to active
[ns_server:info,2019-07-04T11:53:27.822Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 709 state to active
[ns_server:info,2019-07-04T11:53:27.823Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 708 state to active
[ns_server:info,2019-07-04T11:53:27.827Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 707 state to active
[ns_server:info,2019-07-04T11:53:27.836Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 706 state to active
[ns_server:info,2019-07-04T11:53:27.840Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 705 state to active
[ns_server:info,2019-07-04T11:53:27.842Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 704 state to active
[ns_server:info,2019-07-04T11:53:27.843Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 703 state to active
[ns_server:info,2019-07-04T11:53:27.844Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 702 state to active
[ns_server:info,2019-07-04T11:53:27.845Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 701 state to active
[ns_server:info,2019-07-04T11:53:27.846Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 700 state to active
[ns_server:info,2019-07-04T11:53:27.848Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 699 state to active
[ns_server:info,2019-07-04T11:53:27.850Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 698 state to active
[ns_server:info,2019-07-04T11:53:27.851Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 697 state to active
[ns_server:info,2019-07-04T11:53:27.854Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 696 state to active
[ns_server:info,2019-07-04T11:53:27.857Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 695 state to active
[ns_server:info,2019-07-04T11:53:27.858Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 694 state to active
[ns_server:info,2019-07-04T11:53:27.859Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 693 state to active
[ns_server:info,2019-07-04T11:53:27.860Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 692 state to active
[ns_server:info,2019-07-04T11:53:27.861Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 691 state to active
[ns_server:info,2019-07-04T11:53:27.864Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 690 state to active
[ns_server:info,2019-07-04T11:53:27.865Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 689 state to active
[ns_server:info,2019-07-04T11:53:27.867Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 688 state to active
[ns_server:info,2019-07-04T11:53:27.868Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 687 state to active
[ns_server:info,2019-07-04T11:53:27.869Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 686 state to active
[ns_server:info,2019-07-04T11:53:27.871Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 685 state to active
[ns_server:info,2019-07-04T11:53:27.874Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 684 state to active
[ns_server:info,2019-07-04T11:53:27.875Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 683 state to active
[ns_server:info,2019-07-04T11:53:27.876Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 682 state to active
[ns_server:info,2019-07-04T11:53:27.881Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 681 state to active
[ns_server:info,2019-07-04T11:53:27.883Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 680 state to active
[ns_server:info,2019-07-04T11:53:27.885Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 679 state to active
[ns_server:info,2019-07-04T11:53:27.887Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 678 state to active
[ns_server:info,2019-07-04T11:53:27.888Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 677 state to active
[ns_server:info,2019-07-04T11:53:27.889Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 676 state to active
[ns_server:warn,2019-07-04T11:53:27.892Z,ns_1@127.0.0.1:kv_monitor<0.1383.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:info,2019-07-04T11:53:27.893Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 675 state to active
[ns_server:info,2019-07-04T11:53:27.897Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 674 state to active
[ns_server:info,2019-07-04T11:53:27.898Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 673 state to active
[ns_server:info,2019-07-04T11:53:27.901Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 672 state to active
[ns_server:info,2019-07-04T11:53:27.903Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 671 state to active
[ns_server:info,2019-07-04T11:53:27.913Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 670 state to active
[ns_server:info,2019-07-04T11:53:27.917Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 669 state to active
[ns_server:info,2019-07-04T11:53:27.928Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 668 state to active
[ns_server:info,2019-07-04T11:53:27.930Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 667 state to active
[ns_server:info,2019-07-04T11:53:27.931Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 666 state to active
[ns_server:info,2019-07-04T11:53:27.931Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 665 state to active
[ns_server:info,2019-07-04T11:53:27.933Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 664 state to active
[ns_server:info,2019-07-04T11:53:27.934Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 663 state to active
[ns_server:info,2019-07-04T11:53:27.935Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 662 state to active
[ns_server:info,2019-07-04T11:53:27.936Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 661 state to active
[ns_server:info,2019-07-04T11:53:27.938Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 660 state to active
[ns_server:info,2019-07-04T11:53:27.939Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 659 state to active
[ns_server:info,2019-07-04T11:53:27.940Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 658 state to active
[ns_server:info,2019-07-04T11:53:27.942Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 657 state to active
[ns_server:info,2019-07-04T11:53:27.942Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 656 state to active
[ns_server:info,2019-07-04T11:53:27.943Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 655 state to active
[ns_server:info,2019-07-04T11:53:27.943Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 654 state to active
[ns_server:info,2019-07-04T11:53:27.944Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 653 state to active
[ns_server:info,2019-07-04T11:53:27.946Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 652 state to active
[ns_server:info,2019-07-04T11:53:27.947Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 651 state to active
[ns_server:info,2019-07-04T11:53:27.948Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 650 state to active
[ns_server:info,2019-07-04T11:53:27.948Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 649 state to active
[ns_server:info,2019-07-04T11:53:27.950Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 648 state to active
[ns_server:info,2019-07-04T11:53:27.951Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 647 state to active
[ns_server:info,2019-07-04T11:53:27.952Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 646 state to active
[ns_server:info,2019-07-04T11:53:27.952Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 645 state to active
[ns_server:info,2019-07-04T11:53:27.952Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 644 state to active
[ns_server:info,2019-07-04T11:53:27.953Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 643 state to active
[ns_server:info,2019-07-04T11:53:27.958Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 642 state to active
[ns_server:info,2019-07-04T11:53:27.966Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 641 state to active
[ns_server:info,2019-07-04T11:53:27.978Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 640 state to active
[ns_server:info,2019-07-04T11:53:27.979Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 639 state to active
[ns_server:info,2019-07-04T11:53:27.980Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 638 state to active
[ns_server:info,2019-07-04T11:53:27.981Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 637 state to active
[ns_server:info,2019-07-04T11:53:27.983Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 636 state to active
[ns_server:info,2019-07-04T11:53:27.984Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 635 state to active
[ns_server:info,2019-07-04T11:53:27.985Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 634 state to active
[ns_server:info,2019-07-04T11:53:27.989Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 633 state to active
[ns_server:info,2019-07-04T11:53:27.991Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 632 state to active
[ns_server:info,2019-07-04T11:53:27.993Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 631 state to active
[ns_server:info,2019-07-04T11:53:27.997Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 630 state to active
[ns_server:info,2019-07-04T11:53:27.999Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 629 state to active
[ns_server:info,2019-07-04T11:53:27.999Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 628 state to active
[ns_server:info,2019-07-04T11:53:28.000Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 627 state to active
[ns_server:info,2019-07-04T11:53:28.001Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 626 state to active
[ns_server:info,2019-07-04T11:53:28.002Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 625 state to active
[ns_server:info,2019-07-04T11:53:28.003Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 624 state to active
[ns_server:info,2019-07-04T11:53:28.004Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 623 state to active
[ns_server:info,2019-07-04T11:53:28.004Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 622 state to active
[ns_server:info,2019-07-04T11:53:28.005Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 621 state to active
[ns_server:info,2019-07-04T11:53:28.006Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 620 state to active
[ns_server:info,2019-07-04T11:53:28.006Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 619 state to active
[ns_server:info,2019-07-04T11:53:28.007Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 618 state to active
[ns_server:info,2019-07-04T11:53:28.008Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 617 state to active
[ns_server:info,2019-07-04T11:53:28.009Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 616 state to active
[ns_server:info,2019-07-04T11:53:28.009Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 615 state to active
[ns_server:info,2019-07-04T11:53:28.010Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 614 state to active
[ns_server:info,2019-07-04T11:53:28.011Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 613 state to active
[ns_server:info,2019-07-04T11:53:28.012Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 612 state to active
[ns_server:info,2019-07-04T11:53:28.016Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 611 state to active
[ns_server:info,2019-07-04T11:53:28.017Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 610 state to active
[ns_server:info,2019-07-04T11:53:28.027Z,ns_1@127.0.0.1:<0.2404.0>:ns_memcached:do_handle_call:564]Changed vbucket 609 state to active
[ns_server:info,2019-07-04T11:53:28.028Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 608 state to active
[ns_server:info,2019-07-04T11:53:28.029Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 607 state to active
[ns_server:info,2019-07-04T11:53:28.033Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 606 state to active
[ns_server:info,2019-07-04T11:53:28.038Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 605 state to active
[ns_server:info,2019-07-04T11:53:28.039Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 604 state to active
[ns_server:info,2019-07-04T11:53:28.040Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 603 state to active
[ns_server:info,2019-07-04T11:53:28.041Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 602 state to active
[ns_server:info,2019-07-04T11:53:28.041Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 601 state to active
[ns_server:info,2019-07-04T11:53:28.043Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 600 state to active
[ns_server:info,2019-07-04T11:53:28.044Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 599 state to active
[ns_server:info,2019-07-04T11:53:28.045Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 598 state to active
[ns_server:info,2019-07-04T11:53:28.047Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 597 state to active
[ns_server:info,2019-07-04T11:53:28.050Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 596 state to active
[ns_server:info,2019-07-04T11:53:28.053Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 595 state to active
[ns_server:info,2019-07-04T11:53:28.055Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 594 state to active
[ns_server:info,2019-07-04T11:53:28.059Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 593 state to active
[ns_server:info,2019-07-04T11:53:28.061Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 592 state to active
[ns_server:info,2019-07-04T11:53:28.063Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 591 state to active
[ns_server:info,2019-07-04T11:53:28.069Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 590 state to active
[ns_server:info,2019-07-04T11:53:28.071Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 589 state to active
[ns_server:info,2019-07-04T11:53:28.072Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 588 state to active
[ns_server:info,2019-07-04T11:53:28.073Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 587 state to active
[ns_server:info,2019-07-04T11:53:28.075Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 586 state to active
[ns_server:info,2019-07-04T11:53:28.078Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 585 state to active
[ns_server:info,2019-07-04T11:53:28.080Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 584 state to active
[ns_server:info,2019-07-04T11:53:28.089Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 583 state to active
[ns_server:info,2019-07-04T11:53:28.097Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 582 state to active
[ns_server:info,2019-07-04T11:53:28.099Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 581 state to active
[ns_server:info,2019-07-04T11:53:28.101Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 580 state to active
[ns_server:info,2019-07-04T11:53:28.104Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 579 state to active
[ns_server:info,2019-07-04T11:53:28.105Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 578 state to active
[ns_server:info,2019-07-04T11:53:28.117Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 577 state to active
[ns_server:info,2019-07-04T11:53:28.118Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 576 state to active
[ns_server:info,2019-07-04T11:53:28.118Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 575 state to active
[ns_server:info,2019-07-04T11:53:28.119Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 574 state to active
[ns_server:info,2019-07-04T11:53:28.119Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 573 state to active
[ns_server:info,2019-07-04T11:53:28.120Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 572 state to active
[ns_server:info,2019-07-04T11:53:28.121Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 571 state to active
[ns_server:info,2019-07-04T11:53:28.121Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 570 state to active
[ns_server:info,2019-07-04T11:53:28.122Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 569 state to active
[ns_server:info,2019-07-04T11:53:28.123Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 568 state to active
[ns_server:info,2019-07-04T11:53:28.125Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 567 state to active
[ns_server:info,2019-07-04T11:53:28.126Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 566 state to active
[ns_server:info,2019-07-04T11:53:28.127Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 565 state to active
[ns_server:info,2019-07-04T11:53:28.128Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 564 state to active
[ns_server:info,2019-07-04T11:53:28.131Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 563 state to active
[ns_server:info,2019-07-04T11:53:28.134Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 562 state to active
[ns_server:info,2019-07-04T11:53:28.143Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 561 state to active
[ns_server:info,2019-07-04T11:53:28.145Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 560 state to active
[ns_server:info,2019-07-04T11:53:28.149Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 559 state to active
[ns_server:info,2019-07-04T11:53:28.157Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 558 state to active
[ns_server:info,2019-07-04T11:53:28.167Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 557 state to active
[ns_server:info,2019-07-04T11:53:28.175Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 556 state to active
[ns_server:info,2019-07-04T11:53:28.183Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 555 state to active
[ns_server:info,2019-07-04T11:53:28.192Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 554 state to active
[ns_server:info,2019-07-04T11:53:28.201Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 553 state to active
[ns_server:info,2019-07-04T11:53:28.204Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 552 state to active
[ns_server:info,2019-07-04T11:53:28.205Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 551 state to active
[ns_server:info,2019-07-04T11:53:28.208Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 550 state to active
[ns_server:info,2019-07-04T11:53:28.214Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 549 state to active
[ns_server:info,2019-07-04T11:53:28.220Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 548 state to active
[ns_server:info,2019-07-04T11:53:28.242Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 547 state to active
[ns_server:info,2019-07-04T11:53:28.243Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 546 state to active
[ns_server:info,2019-07-04T11:53:28.243Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 545 state to active
[ns_server:info,2019-07-04T11:53:28.244Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 544 state to active
[ns_server:info,2019-07-04T11:53:28.245Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 543 state to active
[ns_server:info,2019-07-04T11:53:28.245Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 542 state to active
[ns_server:info,2019-07-04T11:53:28.246Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 541 state to active
[ns_server:info,2019-07-04T11:53:28.246Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 540 state to active
[ns_server:info,2019-07-04T11:53:28.248Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 539 state to active
[ns_server:info,2019-07-04T11:53:28.250Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 538 state to active
[ns_server:info,2019-07-04T11:53:28.267Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 537 state to active
[ns_server:info,2019-07-04T11:53:28.271Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 536 state to active
[ns_server:info,2019-07-04T11:53:28.274Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 535 state to active
[ns_server:info,2019-07-04T11:53:28.275Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 534 state to active
[ns_server:info,2019-07-04T11:53:28.276Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 533 state to active
[ns_server:info,2019-07-04T11:53:28.277Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 532 state to active
[ns_server:info,2019-07-04T11:53:28.277Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 531 state to active
[ns_server:info,2019-07-04T11:53:28.279Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 530 state to active
[ns_server:info,2019-07-04T11:53:28.284Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 529 state to active
[ns_server:info,2019-07-04T11:53:28.285Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 528 state to active
[ns_server:info,2019-07-04T11:53:28.287Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 527 state to active
[ns_server:info,2019-07-04T11:53:28.297Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 526 state to active
[ns_server:info,2019-07-04T11:53:28.302Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 525 state to active
[ns_server:info,2019-07-04T11:53:28.304Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 524 state to active
[ns_server:info,2019-07-04T11:53:28.309Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 523 state to active
[ns_server:info,2019-07-04T11:53:28.313Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 522 state to active
[ns_server:info,2019-07-04T11:53:28.318Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 521 state to active
[ns_server:info,2019-07-04T11:53:28.323Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 520 state to active
[ns_server:info,2019-07-04T11:53:28.329Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 519 state to active
[ns_server:info,2019-07-04T11:53:28.332Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 518 state to active
[ns_server:info,2019-07-04T11:53:28.335Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 517 state to active
[ns_server:info,2019-07-04T11:53:28.342Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 516 state to active
[ns_server:info,2019-07-04T11:53:28.348Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 515 state to active
[ns_server:info,2019-07-04T11:53:28.352Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 514 state to active
[ns_server:info,2019-07-04T11:53:28.354Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 513 state to active
[ns_server:info,2019-07-04T11:53:28.356Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 512 state to active
[ns_server:info,2019-07-04T11:53:28.357Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 511 state to active
[ns_server:info,2019-07-04T11:53:28.397Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 510 state to active
[ns_server:info,2019-07-04T11:53:28.398Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 509 state to active
[ns_server:info,2019-07-04T11:53:28.399Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 508 state to active
[ns_server:info,2019-07-04T11:53:28.400Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 507 state to active
[ns_server:info,2019-07-04T11:53:28.400Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 506 state to active
[ns_server:info,2019-07-04T11:53:28.402Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 505 state to active
[ns_server:info,2019-07-04T11:53:28.408Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 504 state to active
[ns_server:info,2019-07-04T11:53:28.426Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 503 state to active
[ns_server:info,2019-07-04T11:53:28.434Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 502 state to active
[ns_server:info,2019-07-04T11:53:28.434Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 501 state to active
[ns_server:info,2019-07-04T11:53:28.436Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 500 state to active
[ns_server:info,2019-07-04T11:53:28.437Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 499 state to active
[ns_server:info,2019-07-04T11:53:28.438Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 498 state to active
[ns_server:info,2019-07-04T11:53:28.439Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 497 state to active
[ns_server:info,2019-07-04T11:53:28.442Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 496 state to active
[ns_server:info,2019-07-04T11:53:28.443Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 495 state to active
[ns_server:info,2019-07-04T11:53:28.444Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 494 state to active
[ns_server:info,2019-07-04T11:53:28.446Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 493 state to active
[ns_server:info,2019-07-04T11:53:28.448Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 492 state to active
[ns_server:info,2019-07-04T11:53:28.449Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 491 state to active
[ns_server:info,2019-07-04T11:53:28.455Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 490 state to active
[ns_server:info,2019-07-04T11:53:28.457Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 489 state to active
[ns_server:info,2019-07-04T11:53:28.460Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 488 state to active
[ns_server:info,2019-07-04T11:53:28.461Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 487 state to active
[ns_server:info,2019-07-04T11:53:28.465Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 486 state to active
[ns_server:info,2019-07-04T11:53:28.468Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 485 state to active
[ns_server:info,2019-07-04T11:53:28.470Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 484 state to active
[ns_server:info,2019-07-04T11:53:28.470Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 483 state to active
[ns_server:info,2019-07-04T11:53:28.471Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 482 state to active
[ns_server:info,2019-07-04T11:53:28.472Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 481 state to active
[ns_server:info,2019-07-04T11:53:28.473Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 480 state to active
[ns_server:info,2019-07-04T11:53:28.475Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 479 state to active
[ns_server:info,2019-07-04T11:53:28.476Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 478 state to active
[ns_server:info,2019-07-04T11:53:28.478Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 477 state to active
[ns_server:info,2019-07-04T11:53:28.479Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 476 state to active
[ns_server:info,2019-07-04T11:53:28.481Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 475 state to active
[ns_server:info,2019-07-04T11:53:28.481Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 474 state to active
[ns_server:info,2019-07-04T11:53:28.482Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 473 state to active
[ns_server:info,2019-07-04T11:53:28.482Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 472 state to active
[ns_server:info,2019-07-04T11:53:28.483Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 471 state to active
[ns_server:info,2019-07-04T11:53:28.484Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 470 state to active
[ns_server:info,2019-07-04T11:53:28.484Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 469 state to active
[ns_server:info,2019-07-04T11:53:28.485Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 468 state to active
[ns_server:info,2019-07-04T11:53:28.487Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 467 state to active
[ns_server:info,2019-07-04T11:53:28.488Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 466 state to active
[ns_server:info,2019-07-04T11:53:28.509Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 465 state to active
[ns_server:info,2019-07-04T11:53:28.512Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 464 state to active
[ns_server:info,2019-07-04T11:53:28.513Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 463 state to active
[ns_server:info,2019-07-04T11:53:28.515Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 462 state to active
[ns_server:info,2019-07-04T11:53:28.516Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 461 state to active
[ns_server:info,2019-07-04T11:53:28.517Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 460 state to active
[ns_server:info,2019-07-04T11:53:28.521Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 459 state to active
[ns_server:info,2019-07-04T11:53:28.538Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 458 state to active
[ns_server:info,2019-07-04T11:53:28.538Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 457 state to active
[ns_server:info,2019-07-04T11:53:28.539Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 456 state to active
[ns_server:info,2019-07-04T11:53:28.540Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 455 state to active
[ns_server:info,2019-07-04T11:53:28.541Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 454 state to active
[ns_server:info,2019-07-04T11:53:28.543Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 453 state to active
[ns_server:info,2019-07-04T11:53:28.545Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 452 state to active
[ns_server:info,2019-07-04T11:53:28.545Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 451 state to active
[ns_server:info,2019-07-04T11:53:28.546Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 450 state to active
[ns_server:info,2019-07-04T11:53:28.547Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 449 state to active
[ns_server:info,2019-07-04T11:53:28.548Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 448 state to active
[ns_server:info,2019-07-04T11:53:28.548Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 447 state to active
[ns_server:info,2019-07-04T11:53:28.549Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 446 state to active
[ns_server:info,2019-07-04T11:53:28.549Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 445 state to active
[ns_server:info,2019-07-04T11:53:28.551Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 444 state to active
[ns_server:info,2019-07-04T11:53:28.558Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 443 state to active
[ns_server:info,2019-07-04T11:53:28.560Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 442 state to active
[ns_server:info,2019-07-04T11:53:28.561Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 441 state to active
[ns_server:info,2019-07-04T11:53:28.561Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 440 state to active
[ns_server:info,2019-07-04T11:53:28.563Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 439 state to active
[ns_server:info,2019-07-04T11:53:28.563Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 438 state to active
[ns_server:info,2019-07-04T11:53:28.564Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 437 state to active
[ns_server:info,2019-07-04T11:53:28.564Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 436 state to active
[ns_server:info,2019-07-04T11:53:28.565Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 435 state to active
[ns_server:info,2019-07-04T11:53:28.565Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 434 state to active
[ns_server:info,2019-07-04T11:53:28.566Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 433 state to active
[ns_server:info,2019-07-04T11:53:28.568Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 432 state to active
[ns_server:info,2019-07-04T11:53:28.570Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 431 state to active
[ns_server:info,2019-07-04T11:53:28.571Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 430 state to active
[ns_server:info,2019-07-04T11:53:28.574Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 429 state to active
[ns_server:info,2019-07-04T11:53:28.580Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 428 state to active
[ns_server:info,2019-07-04T11:53:28.581Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 427 state to active
[ns_server:info,2019-07-04T11:53:28.583Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 426 state to active
[ns_server:info,2019-07-04T11:53:28.594Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 425 state to active
[ns_server:info,2019-07-04T11:53:28.600Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 424 state to active
[ns_server:info,2019-07-04T11:53:28.605Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 423 state to active
[ns_server:debug,2019-07-04T11:53:28.615Z,ns_1@127.0.0.1:<0.469.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            1,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              2,half_down,false}
[ns_server:info,2019-07-04T11:53:28.630Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 422 state to active
[ns_server:info,2019-07-04T11:53:28.637Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 421 state to active
[ns_server:info,2019-07-04T11:53:28.648Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 420 state to active
[ns_server:info,2019-07-04T11:53:28.653Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 419 state to active
[ns_server:info,2019-07-04T11:53:28.665Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 418 state to active
[ns_server:info,2019-07-04T11:53:28.669Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 417 state to active
[ns_server:info,2019-07-04T11:53:28.679Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 416 state to active
[ns_server:info,2019-07-04T11:53:28.683Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 415 state to active
[ns_server:info,2019-07-04T11:53:28.690Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 414 state to active
[ns_server:info,2019-07-04T11:53:28.694Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 413 state to active
[ns_server:info,2019-07-04T11:53:28.706Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 412 state to active
[ns_server:info,2019-07-04T11:53:28.709Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 411 state to active
[ns_server:info,2019-07-04T11:53:28.712Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 410 state to active
[ns_server:info,2019-07-04T11:53:28.716Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 409 state to active
[ns_server:info,2019-07-04T11:53:28.725Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 408 state to active
[ns_server:info,2019-07-04T11:53:28.742Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 407 state to active
[ns_server:info,2019-07-04T11:53:28.757Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 406 state to active
[ns_server:info,2019-07-04T11:53:28.758Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 405 state to active
[ns_server:info,2019-07-04T11:53:28.759Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 404 state to active
[ns_server:info,2019-07-04T11:53:28.759Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 403 state to active
[ns_server:info,2019-07-04T11:53:28.760Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 402 state to active
[ns_server:info,2019-07-04T11:53:28.760Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 401 state to active
[ns_server:info,2019-07-04T11:53:28.761Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 400 state to active
[ns_server:info,2019-07-04T11:53:28.761Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 399 state to active
[ns_server:info,2019-07-04T11:53:28.762Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 398 state to active
[ns_server:info,2019-07-04T11:53:28.762Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 397 state to active
[ns_server:info,2019-07-04T11:53:28.763Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 396 state to active
[ns_server:info,2019-07-04T11:53:28.763Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 395 state to active
[ns_server:info,2019-07-04T11:53:28.764Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 394 state to active
[ns_server:info,2019-07-04T11:53:28.764Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 393 state to active
[ns_server:info,2019-07-04T11:53:28.765Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 392 state to active
[ns_server:info,2019-07-04T11:53:28.765Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 391 state to active
[ns_server:info,2019-07-04T11:53:28.766Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 390 state to active
[ns_server:info,2019-07-04T11:53:28.766Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 389 state to active
[ns_server:info,2019-07-04T11:53:28.767Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 388 state to active
[ns_server:info,2019-07-04T11:53:28.770Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 387 state to active
[ns_server:info,2019-07-04T11:53:28.771Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 386 state to active
[ns_server:info,2019-07-04T11:53:28.772Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 385 state to active
[ns_server:info,2019-07-04T11:53:28.772Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 384 state to active
[ns_server:info,2019-07-04T11:53:28.773Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 383 state to active
[ns_server:info,2019-07-04T11:53:28.775Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 382 state to active
[ns_server:info,2019-07-04T11:53:28.776Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 381 state to active
[ns_server:info,2019-07-04T11:53:28.781Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 380 state to active
[ns_server:info,2019-07-04T11:53:28.782Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 379 state to active
[ns_server:info,2019-07-04T11:53:28.783Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 378 state to active
[ns_server:info,2019-07-04T11:53:28.783Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 377 state to active
[ns_server:info,2019-07-04T11:53:28.784Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 376 state to active
[ns_server:info,2019-07-04T11:53:28.786Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 375 state to active
[ns_server:info,2019-07-04T11:53:28.786Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 374 state to active
[ns_server:info,2019-07-04T11:53:28.787Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 373 state to active
[ns_server:info,2019-07-04T11:53:28.791Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 372 state to active
[ns_server:info,2019-07-04T11:53:28.794Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 371 state to active
[ns_server:info,2019-07-04T11:53:28.804Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 370 state to active
[ns_server:info,2019-07-04T11:53:28.815Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 369 state to active
[ns_server:info,2019-07-04T11:53:28.819Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 368 state to active
[ns_server:info,2019-07-04T11:53:28.822Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 367 state to active
[ns_server:info,2019-07-04T11:53:28.847Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 366 state to active
[ns_server:info,2019-07-04T11:53:28.855Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 365 state to active
[ns_server:info,2019-07-04T11:53:28.863Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 364 state to active
[ns_server:info,2019-07-04T11:53:28.867Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 363 state to active
[ns_server:info,2019-07-04T11:53:28.868Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 362 state to active
[ns_server:info,2019-07-04T11:53:28.870Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 361 state to active
[ns_server:info,2019-07-04T11:53:28.887Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 360 state to active
[ns_server:info,2019-07-04T11:53:28.889Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 359 state to active
[ns_server:warn,2019-07-04T11:53:28.895Z,ns_1@127.0.0.1:kv_monitor<0.1383.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:info,2019-07-04T11:53:28.898Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 358 state to active
[ns_server:info,2019-07-04T11:53:28.903Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 357 state to active
[ns_server:info,2019-07-04T11:53:28.905Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 356 state to active
[ns_server:info,2019-07-04T11:53:28.909Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 355 state to active
[ns_server:info,2019-07-04T11:53:28.914Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 354 state to active
[ns_server:info,2019-07-04T11:53:28.924Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 353 state to active
[ns_server:info,2019-07-04T11:53:28.928Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 352 state to active
[ns_server:info,2019-07-04T11:53:28.932Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 351 state to active
[ns_server:info,2019-07-04T11:53:28.935Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 350 state to active
[ns_server:info,2019-07-04T11:53:28.936Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 349 state to active
[ns_server:info,2019-07-04T11:53:28.939Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 348 state to active
[ns_server:info,2019-07-04T11:53:28.941Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 347 state to active
[ns_server:info,2019-07-04T11:53:28.943Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 346 state to active
[ns_server:info,2019-07-04T11:53:28.945Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 345 state to active
[ns_server:info,2019-07-04T11:53:28.948Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 344 state to active
[ns_server:info,2019-07-04T11:53:28.950Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 343 state to active
[ns_server:info,2019-07-04T11:53:28.953Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 342 state to active
[ns_server:info,2019-07-04T11:53:28.956Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 341 state to active
[ns_server:info,2019-07-04T11:53:28.962Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 340 state to active
[ns_server:info,2019-07-04T11:53:28.968Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 339 state to active
[ns_server:info,2019-07-04T11:53:28.972Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 338 state to active
[ns_server:info,2019-07-04T11:53:28.974Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 337 state to active
[ns_server:info,2019-07-04T11:53:28.974Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 336 state to active
[ns_server:info,2019-07-04T11:53:28.975Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 335 state to active
[ns_server:info,2019-07-04T11:53:28.976Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 334 state to active
[ns_server:info,2019-07-04T11:53:28.977Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 333 state to active
[ns_server:info,2019-07-04T11:53:28.978Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 332 state to active
[ns_server:info,2019-07-04T11:53:28.979Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 331 state to active
[ns_server:info,2019-07-04T11:53:28.980Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 330 state to active
[ns_server:info,2019-07-04T11:53:28.981Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 329 state to active
[ns_server:info,2019-07-04T11:53:28.982Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 328 state to active
[ns_server:info,2019-07-04T11:53:28.983Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 327 state to active
[ns_server:info,2019-07-04T11:53:28.984Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 326 state to active
[ns_server:info,2019-07-04T11:53:28.985Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 325 state to active
[ns_server:info,2019-07-04T11:53:28.986Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 324 state to active
[ns_server:info,2019-07-04T11:53:28.988Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 323 state to active
[ns_server:info,2019-07-04T11:53:28.988Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 322 state to active
[ns_server:info,2019-07-04T11:53:28.989Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 321 state to active
[ns_server:info,2019-07-04T11:53:28.991Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 320 state to active
[ns_server:info,2019-07-04T11:53:28.993Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 319 state to active
[ns_server:info,2019-07-04T11:53:28.994Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 318 state to active
[ns_server:info,2019-07-04T11:53:28.995Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 317 state to active
[ns_server:info,2019-07-04T11:53:28.998Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 316 state to active
[ns_server:info,2019-07-04T11:53:28.999Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 315 state to active
[ns_server:info,2019-07-04T11:53:29.001Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 314 state to active
[ns_server:info,2019-07-04T11:53:29.001Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 313 state to active
[ns_server:info,2019-07-04T11:53:29.006Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 312 state to active
[ns_server:info,2019-07-04T11:53:29.020Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 311 state to active
[ns_server:info,2019-07-04T11:53:29.022Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 310 state to active
[ns_server:info,2019-07-04T11:53:29.024Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 309 state to active
[ns_server:info,2019-07-04T11:53:29.025Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 308 state to active
[ns_server:info,2019-07-04T11:53:29.025Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 307 state to active
[ns_server:info,2019-07-04T11:53:29.027Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 306 state to active
[ns_server:info,2019-07-04T11:53:29.027Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 305 state to active
[ns_server:info,2019-07-04T11:53:29.028Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 304 state to active
[ns_server:info,2019-07-04T11:53:29.029Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 303 state to active
[ns_server:info,2019-07-04T11:53:29.036Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 302 state to active
[ns_server:info,2019-07-04T11:53:29.037Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 301 state to active
[ns_server:info,2019-07-04T11:53:29.043Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 300 state to active
[ns_server:info,2019-07-04T11:53:29.045Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 299 state to active
[ns_server:info,2019-07-04T11:53:29.047Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 298 state to active
[ns_server:info,2019-07-04T11:53:29.048Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 297 state to active
[ns_server:info,2019-07-04T11:53:29.052Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 296 state to active
[ns_server:info,2019-07-04T11:53:29.055Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 295 state to active
[ns_server:info,2019-07-04T11:53:29.058Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 294 state to active
[ns_server:info,2019-07-04T11:53:29.059Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 293 state to active
[ns_server:info,2019-07-04T11:53:29.080Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 292 state to active
[ns_server:info,2019-07-04T11:53:29.082Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 291 state to active
[ns_server:info,2019-07-04T11:53:29.090Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 290 state to active
[ns_server:info,2019-07-04T11:53:29.092Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 289 state to active
[ns_server:info,2019-07-04T11:53:29.095Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 288 state to active
[ns_server:info,2019-07-04T11:53:29.098Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 287 state to active
[ns_server:info,2019-07-04T11:53:29.100Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 286 state to active
[ns_server:info,2019-07-04T11:53:29.101Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 285 state to active
[ns_server:info,2019-07-04T11:53:29.102Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 284 state to active
[ns_server:info,2019-07-04T11:53:29.103Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 283 state to active
[ns_server:info,2019-07-04T11:53:29.104Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 282 state to active
[ns_server:info,2019-07-04T11:53:29.111Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 281 state to active
[ns_server:info,2019-07-04T11:53:29.113Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 280 state to active
[ns_server:info,2019-07-04T11:53:29.115Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 279 state to active
[ns_server:info,2019-07-04T11:53:29.120Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 278 state to active
[ns_server:info,2019-07-04T11:53:29.123Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 277 state to active
[ns_server:info,2019-07-04T11:53:29.127Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 276 state to active
[ns_server:info,2019-07-04T11:53:29.128Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 275 state to active
[ns_server:info,2019-07-04T11:53:29.129Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 274 state to active
[ns_server:info,2019-07-04T11:53:29.129Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 273 state to active
[ns_server:info,2019-07-04T11:53:29.130Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 272 state to active
[ns_server:info,2019-07-04T11:53:29.130Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 271 state to active
[ns_server:info,2019-07-04T11:53:29.131Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 270 state to active
[ns_server:info,2019-07-04T11:53:29.131Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 269 state to active
[ns_server:info,2019-07-04T11:53:29.132Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 268 state to active
[ns_server:info,2019-07-04T11:53:29.132Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 267 state to active
[ns_server:info,2019-07-04T11:53:29.133Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 266 state to active
[ns_server:info,2019-07-04T11:53:29.133Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 265 state to active
[ns_server:info,2019-07-04T11:53:29.134Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 264 state to active
[ns_server:info,2019-07-04T11:53:29.135Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 263 state to active
[ns_server:info,2019-07-04T11:53:29.136Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 262 state to active
[ns_server:info,2019-07-04T11:53:29.136Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 261 state to active
[ns_server:info,2019-07-04T11:53:29.137Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 260 state to active
[ns_server:info,2019-07-04T11:53:29.138Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 259 state to active
[ns_server:info,2019-07-04T11:53:29.140Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 258 state to active
[ns_server:info,2019-07-04T11:53:29.148Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 257 state to active
[ns_server:info,2019-07-04T11:53:29.151Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 256 state to active
[ns_server:info,2019-07-04T11:53:29.152Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 255 state to active
[ns_server:info,2019-07-04T11:53:29.153Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 254 state to active
[ns_server:info,2019-07-04T11:53:29.154Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 253 state to active
[ns_server:info,2019-07-04T11:53:29.156Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 252 state to active
[ns_server:info,2019-07-04T11:53:29.165Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 251 state to active
[ns_server:info,2019-07-04T11:53:29.169Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 250 state to active
[ns_server:info,2019-07-04T11:53:29.171Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 249 state to active
[ns_server:info,2019-07-04T11:53:29.190Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 248 state to active
[ns_server:info,2019-07-04T11:53:29.200Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 247 state to active
[ns_server:info,2019-07-04T11:53:29.250Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 246 state to active
[ns_server:info,2019-07-04T11:53:29.258Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 245 state to active
[ns_server:info,2019-07-04T11:53:29.259Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 244 state to active
[ns_server:info,2019-07-04T11:53:29.259Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 243 state to active
[ns_server:info,2019-07-04T11:53:29.260Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 242 state to active
[ns_server:info,2019-07-04T11:53:29.261Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 241 state to active
[ns_server:info,2019-07-04T11:53:29.265Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 240 state to active
[ns_server:info,2019-07-04T11:53:29.267Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 239 state to active
[ns_server:info,2019-07-04T11:53:29.292Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 238 state to active
[ns_server:info,2019-07-04T11:53:29.298Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 237 state to active
[ns_server:info,2019-07-04T11:53:29.300Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 236 state to active
[ns_server:info,2019-07-04T11:53:29.301Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 235 state to active
[ns_server:info,2019-07-04T11:53:29.301Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 234 state to active
[ns_server:info,2019-07-04T11:53:29.302Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 233 state to active
[ns_server:info,2019-07-04T11:53:29.303Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 232 state to active
[ns_server:info,2019-07-04T11:53:29.303Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 231 state to active
[ns_server:info,2019-07-04T11:53:29.304Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 230 state to active
[ns_server:info,2019-07-04T11:53:29.306Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 229 state to active
[ns_server:info,2019-07-04T11:53:29.317Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 228 state to active
[ns_server:info,2019-07-04T11:53:29.320Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 227 state to active
[ns_server:info,2019-07-04T11:53:29.321Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 226 state to active
[ns_server:info,2019-07-04T11:53:29.321Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 225 state to active
[ns_server:info,2019-07-04T11:53:29.322Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 224 state to active
[ns_server:info,2019-07-04T11:53:29.322Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 223 state to active
[ns_server:info,2019-07-04T11:53:29.323Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 222 state to active
[ns_server:info,2019-07-04T11:53:29.323Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 221 state to active
[ns_server:info,2019-07-04T11:53:29.324Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 220 state to active
[ns_server:info,2019-07-04T11:53:29.325Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 219 state to active
[ns_server:info,2019-07-04T11:53:29.328Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 218 state to active
[ns_server:info,2019-07-04T11:53:29.328Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 217 state to active
[ns_server:info,2019-07-04T11:53:29.329Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 216 state to active
[ns_server:info,2019-07-04T11:53:29.330Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 215 state to active
[ns_server:info,2019-07-04T11:53:29.331Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 214 state to active
[ns_server:info,2019-07-04T11:53:29.331Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 213 state to active
[ns_server:info,2019-07-04T11:53:29.332Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 212 state to active
[ns_server:info,2019-07-04T11:53:29.333Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 211 state to active
[ns_server:info,2019-07-04T11:53:29.333Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 210 state to active
[ns_server:info,2019-07-04T11:53:29.334Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 209 state to active
[ns_server:info,2019-07-04T11:53:29.336Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 208 state to active
[ns_server:info,2019-07-04T11:53:29.336Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 207 state to active
[ns_server:info,2019-07-04T11:53:29.337Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 206 state to active
[ns_server:info,2019-07-04T11:53:29.337Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 205 state to active
[ns_server:info,2019-07-04T11:53:29.338Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 204 state to active
[ns_server:info,2019-07-04T11:53:29.338Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 203 state to active
[ns_server:info,2019-07-04T11:53:29.339Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 202 state to active
[ns_server:info,2019-07-04T11:53:29.339Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 201 state to active
[ns_server:info,2019-07-04T11:53:29.340Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 200 state to active
[ns_server:info,2019-07-04T11:53:29.340Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 199 state to active
[ns_server:info,2019-07-04T11:53:29.342Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 198 state to active
[ns_server:info,2019-07-04T11:53:29.343Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 197 state to active
[ns_server:info,2019-07-04T11:53:29.343Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 196 state to active
[ns_server:info,2019-07-04T11:53:29.344Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 195 state to active
[ns_server:info,2019-07-04T11:53:29.344Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 194 state to active
[ns_server:info,2019-07-04T11:53:29.345Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 193 state to active
[ns_server:info,2019-07-04T11:53:29.345Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 192 state to active
[ns_server:info,2019-07-04T11:53:29.346Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 191 state to active
[ns_server:info,2019-07-04T11:53:29.346Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 190 state to active
[ns_server:info,2019-07-04T11:53:29.347Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 189 state to active
[ns_server:info,2019-07-04T11:53:29.347Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 188 state to active
[ns_server:info,2019-07-04T11:53:29.348Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 187 state to active
[ns_server:info,2019-07-04T11:53:29.350Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 186 state to active
[ns_server:info,2019-07-04T11:53:29.351Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 185 state to active
[ns_server:info,2019-07-04T11:53:29.351Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 184 state to active
[ns_server:info,2019-07-04T11:53:29.352Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 183 state to active
[ns_server:info,2019-07-04T11:53:29.352Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 182 state to active
[ns_server:info,2019-07-04T11:53:29.353Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 181 state to active
[ns_server:info,2019-07-04T11:53:29.354Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 180 state to active
[ns_server:info,2019-07-04T11:53:29.354Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 179 state to active
[ns_server:info,2019-07-04T11:53:29.355Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 178 state to active
[ns_server:info,2019-07-04T11:53:29.355Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 177 state to active
[ns_server:info,2019-07-04T11:53:29.356Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 176 state to active
[ns_server:info,2019-07-04T11:53:29.356Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 175 state to active
[ns_server:info,2019-07-04T11:53:29.357Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 174 state to active
[ns_server:info,2019-07-04T11:53:29.358Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 173 state to active
[ns_server:info,2019-07-04T11:53:29.358Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 172 state to active
[ns_server:info,2019-07-04T11:53:29.359Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 171 state to active
[ns_server:info,2019-07-04T11:53:29.359Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 170 state to active
[ns_server:info,2019-07-04T11:53:29.360Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 169 state to active
[ns_server:info,2019-07-04T11:53:29.360Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 168 state to active
[ns_server:info,2019-07-04T11:53:29.361Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 167 state to active
[ns_server:info,2019-07-04T11:53:29.361Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 166 state to active
[ns_server:info,2019-07-04T11:53:29.366Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 165 state to active
[ns_server:info,2019-07-04T11:53:29.366Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 164 state to active
[ns_server:info,2019-07-04T11:53:29.367Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 163 state to active
[ns_server:info,2019-07-04T11:53:29.371Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 162 state to active
[ns_server:info,2019-07-04T11:53:29.383Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 161 state to active
[ns_server:info,2019-07-04T11:53:29.391Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 160 state to active
[ns_server:info,2019-07-04T11:53:29.397Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 159 state to active
[ns_server:info,2019-07-04T11:53:29.399Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 158 state to active
[ns_server:info,2019-07-04T11:53:29.401Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 157 state to active
[ns_server:info,2019-07-04T11:53:29.403Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 156 state to active
[ns_server:info,2019-07-04T11:53:29.404Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 155 state to active
[ns_server:info,2019-07-04T11:53:29.406Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 154 state to active
[ns_server:info,2019-07-04T11:53:29.408Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 153 state to active
[ns_server:info,2019-07-04T11:53:29.410Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 152 state to active
[ns_server:info,2019-07-04T11:53:29.411Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 151 state to active
[ns_server:info,2019-07-04T11:53:29.412Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 150 state to active
[ns_server:info,2019-07-04T11:53:29.413Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 149 state to active
[ns_server:info,2019-07-04T11:53:29.414Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 148 state to active
[ns_server:info,2019-07-04T11:53:29.414Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 147 state to active
[ns_server:info,2019-07-04T11:53:29.417Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 146 state to active
[ns_server:info,2019-07-04T11:53:29.419Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 145 state to active
[ns_server:info,2019-07-04T11:53:29.435Z,ns_1@127.0.0.1:<0.444.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:info,2019-07-04T11:53:29.440Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 144 state to active
[ns_server:info,2019-07-04T11:53:29.445Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 143 state to active
[ns_server:info,2019-07-04T11:53:29.446Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 142 state to active
[ns_server:info,2019-07-04T11:53:29.457Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 141 state to active
[ns_server:info,2019-07-04T11:53:29.465Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 140 state to active
[ns_server:info,2019-07-04T11:53:29.479Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 139 state to active
[ns_server:info,2019-07-04T11:53:29.487Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 138 state to active
[ns_server:info,2019-07-04T11:53:29.500Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 137 state to active
[ns_server:info,2019-07-04T11:53:29.502Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 136 state to active
[ns_server:info,2019-07-04T11:53:29.503Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 135 state to active
[ns_server:info,2019-07-04T11:53:29.503Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 134 state to active
[ns_server:info,2019-07-04T11:53:29.504Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 133 state to active
[ns_server:info,2019-07-04T11:53:29.505Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 132 state to active
[ns_server:info,2019-07-04T11:53:29.506Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 131 state to active
[ns_server:info,2019-07-04T11:53:29.507Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 130 state to active
[ns_server:info,2019-07-04T11:53:29.508Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 129 state to active
[ns_server:info,2019-07-04T11:53:29.508Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 128 state to active
[ns_server:info,2019-07-04T11:53:29.509Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 127 state to active
[ns_server:info,2019-07-04T11:53:29.511Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 126 state to active
[ns_server:info,2019-07-04T11:53:29.520Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 125 state to active
[ns_server:info,2019-07-04T11:53:29.521Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 124 state to active
[ns_server:info,2019-07-04T11:53:29.523Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 123 state to active
[ns_server:info,2019-07-04T11:53:29.527Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 122 state to active
[ns_server:info,2019-07-04T11:53:29.528Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 121 state to active
[ns_server:info,2019-07-04T11:53:29.529Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 120 state to active
[ns_server:info,2019-07-04T11:53:29.531Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 119 state to active
[ns_server:info,2019-07-04T11:53:29.534Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 118 state to active
[ns_server:info,2019-07-04T11:53:29.535Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 117 state to active
[ns_server:info,2019-07-04T11:53:29.536Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 116 state to active
[ns_server:info,2019-07-04T11:53:29.537Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 115 state to active
[ns_server:info,2019-07-04T11:53:29.538Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 114 state to active
[ns_server:info,2019-07-04T11:53:29.539Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 113 state to active
[ns_server:info,2019-07-04T11:53:29.539Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 112 state to active
[ns_server:info,2019-07-04T11:53:29.540Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 111 state to active
[ns_server:info,2019-07-04T11:53:29.542Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 110 state to active
[ns_server:info,2019-07-04T11:53:29.546Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 109 state to active
[ns_server:info,2019-07-04T11:53:29.548Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 108 state to active
[ns_server:info,2019-07-04T11:53:29.550Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 107 state to active
[ns_server:info,2019-07-04T11:53:29.551Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 106 state to active
[ns_server:info,2019-07-04T11:53:29.555Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 105 state to active
[ns_server:info,2019-07-04T11:53:29.558Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 104 state to active
[ns_server:info,2019-07-04T11:53:29.559Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 103 state to active
[ns_server:info,2019-07-04T11:53:29.560Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 102 state to active
[ns_server:info,2019-07-04T11:53:29.561Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 101 state to active
[ns_server:info,2019-07-04T11:53:29.562Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 100 state to active
[ns_server:info,2019-07-04T11:53:29.562Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 99 state to active
[ns_server:info,2019-07-04T11:53:29.563Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 98 state to active
[ns_server:info,2019-07-04T11:53:29.563Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 97 state to active
[ns_server:info,2019-07-04T11:53:29.564Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 96 state to active
[ns_server:info,2019-07-04T11:53:29.564Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 95 state to active
[ns_server:info,2019-07-04T11:53:29.565Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 94 state to active
[ns_server:info,2019-07-04T11:53:29.565Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 93 state to active
[ns_server:info,2019-07-04T11:53:29.566Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 92 state to active
[ns_server:info,2019-07-04T11:53:29.568Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 91 state to active
[ns_server:info,2019-07-04T11:53:29.568Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 90 state to active
[ns_server:info,2019-07-04T11:53:29.569Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 89 state to active
[ns_server:info,2019-07-04T11:53:29.569Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 88 state to active
[ns_server:info,2019-07-04T11:53:29.570Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 87 state to active
[ns_server:info,2019-07-04T11:53:29.570Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 86 state to active
[ns_server:info,2019-07-04T11:53:29.571Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 85 state to active
[ns_server:info,2019-07-04T11:53:29.571Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 84 state to active
[ns_server:info,2019-07-04T11:53:29.572Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 83 state to active
[ns_server:info,2019-07-04T11:53:29.576Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 82 state to active
[ns_server:info,2019-07-04T11:53:29.577Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 81 state to active
[ns_server:info,2019-07-04T11:53:29.583Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 80 state to active
[ns_server:info,2019-07-04T11:53:29.591Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 79 state to active
[ns_server:info,2019-07-04T11:53:29.596Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 78 state to active
[ns_server:info,2019-07-04T11:53:29.597Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 77 state to active
[ns_server:info,2019-07-04T11:53:29.597Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 76 state to active
[ns_server:info,2019-07-04T11:53:29.598Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 75 state to active
[ns_server:info,2019-07-04T11:53:29.600Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 74 state to active
[ns_server:info,2019-07-04T11:53:29.601Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 73 state to active
[ns_server:info,2019-07-04T11:53:29.603Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 72 state to active
[ns_server:info,2019-07-04T11:53:29.606Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 71 state to active
[ns_server:info,2019-07-04T11:53:29.609Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 70 state to active
[ns_server:info,2019-07-04T11:53:29.611Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 69 state to active
[ns_server:info,2019-07-04T11:53:29.613Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 68 state to active
[ns_server:debug,2019-07-04T11:53:29.613Z,ns_1@127.0.0.1:<0.469.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            2,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              3,half_down,false}
[ns_server:info,2019-07-04T11:53:29.615Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 67 state to active
[ns_server:info,2019-07-04T11:53:29.620Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 66 state to active
[ns_server:info,2019-07-04T11:53:29.627Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 65 state to active
[ns_server:info,2019-07-04T11:53:29.628Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 64 state to active
[ns_server:info,2019-07-04T11:53:29.629Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 63 state to active
[ns_server:info,2019-07-04T11:53:29.648Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 62 state to active
[ns_server:info,2019-07-04T11:53:29.649Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 61 state to active
[ns_server:info,2019-07-04T11:53:29.649Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 60 state to active
[ns_server:info,2019-07-04T11:53:29.650Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 59 state to active
[ns_server:info,2019-07-04T11:53:29.651Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 58 state to active
[ns_server:info,2019-07-04T11:53:29.651Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 57 state to active
[ns_server:info,2019-07-04T11:53:29.652Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 56 state to active
[ns_server:info,2019-07-04T11:53:29.652Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 55 state to active
[ns_server:info,2019-07-04T11:53:29.653Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 54 state to active
[ns_server:info,2019-07-04T11:53:29.654Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 53 state to active
[ns_server:info,2019-07-04T11:53:29.654Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 52 state to active
[ns_server:info,2019-07-04T11:53:29.655Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 51 state to active
[ns_server:info,2019-07-04T11:53:29.655Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 50 state to active
[ns_server:info,2019-07-04T11:53:29.656Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 49 state to active
[ns_server:info,2019-07-04T11:53:29.656Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 48 state to active
[ns_server:info,2019-07-04T11:53:29.660Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 47 state to active
[ns_server:info,2019-07-04T11:53:29.667Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 46 state to active
[ns_server:info,2019-07-04T11:53:29.668Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 45 state to active
[ns_server:info,2019-07-04T11:53:29.669Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 44 state to active
[ns_server:info,2019-07-04T11:53:29.670Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 43 state to active
[ns_server:info,2019-07-04T11:53:29.670Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 42 state to active
[ns_server:info,2019-07-04T11:53:29.671Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 41 state to active
[ns_server:info,2019-07-04T11:53:29.671Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 40 state to active
[ns_server:info,2019-07-04T11:53:29.672Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 39 state to active
[ns_server:info,2019-07-04T11:53:29.672Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 38 state to active
[ns_server:info,2019-07-04T11:53:29.673Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 37 state to active
[ns_server:info,2019-07-04T11:53:29.673Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 36 state to active
[ns_server:info,2019-07-04T11:53:29.674Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 35 state to active
[ns_server:info,2019-07-04T11:53:29.674Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 34 state to active
[ns_server:info,2019-07-04T11:53:29.676Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 33 state to active
[ns_server:info,2019-07-04T11:53:29.681Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 32 state to active
[ns_server:info,2019-07-04T11:53:29.686Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 31 state to active
[ns_server:info,2019-07-04T11:53:29.689Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 30 state to active
[ns_server:info,2019-07-04T11:53:29.690Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 29 state to active
[ns_server:info,2019-07-04T11:53:29.692Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 28 state to active
[ns_server:info,2019-07-04T11:53:29.692Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 27 state to active
[ns_server:info,2019-07-04T11:53:29.693Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 26 state to active
[ns_server:info,2019-07-04T11:53:29.694Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 25 state to active
[ns_server:info,2019-07-04T11:53:29.695Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 24 state to active
[ns_server:info,2019-07-04T11:53:29.695Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 23 state to active
[ns_server:info,2019-07-04T11:53:29.695Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 22 state to active
[ns_server:info,2019-07-04T11:53:29.696Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 21 state to active
[ns_server:info,2019-07-04T11:53:29.697Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 20 state to active
[ns_server:info,2019-07-04T11:53:29.698Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 19 state to active
[ns_server:info,2019-07-04T11:53:29.698Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 18 state to active
[ns_server:info,2019-07-04T11:53:29.699Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 17 state to active
[ns_server:info,2019-07-04T11:53:29.700Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 16 state to active
[ns_server:info,2019-07-04T11:53:29.702Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 15 state to active
[ns_server:info,2019-07-04T11:53:29.704Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 14 state to active
[ns_server:info,2019-07-04T11:53:29.706Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 13 state to active
[ns_server:info,2019-07-04T11:53:29.710Z,ns_1@127.0.0.1:<0.2403.0>:ns_memcached:do_handle_call:564]Changed vbucket 12 state to active
[ns_server:info,2019-07-04T11:53:29.731Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 11 state to active
[ns_server:info,2019-07-04T11:53:29.731Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 10 state to active
[ns_server:info,2019-07-04T11:53:29.732Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 9 state to active
[ns_server:info,2019-07-04T11:53:29.732Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 8 state to active
[ns_server:info,2019-07-04T11:53:29.732Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 7 state to active
[ns_server:info,2019-07-04T11:53:29.733Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 6 state to active
[ns_server:info,2019-07-04T11:53:29.733Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 5 state to active
[ns_server:info,2019-07-04T11:53:29.734Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 4 state to active
[ns_server:info,2019-07-04T11:53:29.734Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 3 state to active
[ns_server:info,2019-07-04T11:53:29.735Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 2 state to active
[ns_server:info,2019-07-04T11:53:29.738Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 1 state to active
[ns_server:info,2019-07-04T11:53:29.739Z,ns_1@127.0.0.1:<0.2405.0>:ns_memcached:do_handle_call:564]Changed vbucket 0 state to active
[ns_server:info,2019-07-04T11:53:29.745Z,ns_1@127.0.0.1:ns_memcached-app<0.2387.0>:ns_memcached:handle_call:298]Enabling traffic to bucket "app"
[ns_server:info,2019-07-04T11:53:29.748Z,ns_1@127.0.0.1:ns_memcached-app<0.2387.0>:ns_memcached:handle_call:302]Bucket "app" marked as warmed in 3 seconds
[ns_server:debug,2019-07-04T11:53:30.613Z,ns_1@127.0.0.1:<0.469.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            3,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              4,half_down,false}
[ns_server:debug,2019-07-04T11:53:31.611Z,ns_1@127.0.0.1:<0.469.0>:auto_failover_logic:log_master_activity:170]Transitioned node {'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>} state half_down -> up
[ns_server:info,2019-07-04T11:53:33.629Z,ns_1@127.0.0.1:ns_doctor<0.287.0>:ns_doctor:update_status:322]The following buckets became ready on node 'ns_1@127.0.0.1': ["app"]
[ns_server:debug,2019-07-04T11:53:34.248Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:debug,2019-07-04T11:53:34.249Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:53:34.249Z,ns_1@127.0.0.1:<0.2849.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:53:34.251Z,ns_1@127.0.0.1:<0.2852.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 223347, disk size is 4241408
[ns_server:debug,2019-07-04T11:53:34.251Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:53:34.251Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2019-07-04T11:53:34.254Z,ns_1@127.0.0.1:<0.2850.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:53:34.255Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:53:34.255Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[menelaus:warn,2019-07-04T11:53:39.319Z,ns_1@127.0.0.1:<0.2816.0>:menelaus_web:log_client_error:805]Client-side error-report for user "admin" on node 'ns_1@127.0.0.1':
User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36
Got unhandled javascript error:
name: TypeError;
message: Cannot read property 'shiftKey' of undefined;
stack: TypeError: Cannot read property 'shiftKey' of undefined
    at ChildScope.dialogScope.select (http://localhost:8091/_p/ui/query/qw_query_controller.js:896:23)
    at fn (eval at compile (http://localhost:8091/ui/libs/angular.js:15651:15), <anonymous>:4:256)
    at callback (http://localhost:8091/ui/libs/angular.js:27475:17)
    at ChildScope.$eval (http://localhost:8091/ui/libs/angular.js:18542:28)
    at ChildScope.$apply (http://localhost:8091/ui/libs/angular.js:18641:25)
    at HTMLSpanElement.<anonymous> (http://localhost:8091/ui/libs/angular.js:27480:23)
    at defaultHandlerWrapper (http://localhost:8091/ui/libs/angular.js:3791:11)
    at HTMLSpanElement.eventHandler (http://localhost:8091/ui/libs/angular.js:3779:9);


[menelaus:warn,2019-07-04T11:53:41.497Z,ns_1@127.0.0.1:<0.2818.0>:menelaus_web:log_client_error:805]Client-side error-report for user "admin" on node 'ns_1@127.0.0.1':
User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36
Got unhandled javascript error:
name: TypeError;
message: Cannot read property 'shiftKey' of undefined;
stack: TypeError: Cannot read property 'shiftKey' of undefined
    at ChildScope.dialogScope.select (http://localhost:8091/_p/ui/query/qw_query_controller.js:896:23)
    at fn (eval at compile (http://localhost:8091/ui/libs/angular.js:15651:15), <anonymous>:4:256)
    at callback (http://localhost:8091/ui/libs/angular.js:27475:17)
    at ChildScope.$eval (http://localhost:8091/ui/libs/angular.js:18542:28)
    at ChildScope.$apply (http://localhost:8091/ui/libs/angular.js:18641:25)
    at HTMLSpanElement.<anonymous> (http://localhost:8091/ui/libs/angular.js:27480:23)
    at defaultHandlerWrapper (http://localhost:8091/ui/libs/angular.js:3791:11)
    at HTMLSpanElement.eventHandler (http://localhost:8091/ui/libs/angular.js:3779:9);


[ns_server:info,2019-07-04T11:53:41.498Z,ns_1@127.0.0.1:ns_log<0.231.0>:ns_log:handle_cast:152]suppressing duplicate log menelaus_web:102([<<"Client-side error-report for user \"admin\" on node 'ns_1@127.0.0.1':\nUser-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\nGot unhandled javascript error:\nname: TypeError;\nmessage: Cannot read property 'shiftKey' of undefined;\nstack: TypeError: Cannot read property 'shiftKey' of undefined\n    at ChildScope.dialogScope.select (http://localhost:8091/_p/ui/query/qw_query_controller.js:896:23)\n    at fn (eval at compile (http://localhost:8091/ui/libs/angular.js:15651:15), <anonymous>:4:256)\n    at callback (http://localhost:8091/ui/libs/angular.js:27475:17)\n    at ChildScope.$eval (http://localhost:8091/ui/libs/angular.js:18542:28)\n    at ChildScope.$apply (http://localhost:8091/ui/libs/angular.js:18641:25)\n    at HTMLSpanElement.<anonymous> (http://localhost:8091/ui/libs/angular.js:27480:23)\n    at defaultHandlerWrapper (http://localhost:8091/ui/libs/angular.js:3791:11)\n    at HTMLSpanElement.eventHandler (http://localhost:8091/ui/libs/angular.js:3779:9);\n\n">>]) because it's been seen 1 times in the past 2.178657 secs (last seen 2.178657 secs ago
[ns_server:debug,2019-07-04T11:53:45.142Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2019-07-04T11:53:45.877Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{27,63729460425}}]}]
[ns_server:debug,2019-07-04T11:53:45.878Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/8479338376212444691/0">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460425}}]}|
 <<"{\"DefnId\":8479338376212444691,\"BucketUUID\":\"5b6ce58456b7220bfb025f341cb20648\",\"Definitions\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":[{\"defnId\":8479338376212444691,\"name\":\"users\",\"using\":\"GSI\",\"bucket\":\"app\",\"isPrimary\":true,\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"residentRatio\":100,\"instanceId\":11114069127010685716,\"partitions\":[0],\"versions\":[0],\"numPartitions\":1}]}}">>]
[ns_server:debug,2019-07-04T11:53:45.890Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/8479338376212444691/0">>}]..)
[ns_server:debug,2019-07-04T11:53:46.337Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2019-07-04T11:53:46.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/8479338376212444691/0">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460426}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:53:46.399Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{28,63729460426}}]}]
[ns_server:debug,2019-07-04T11:53:46.400Z,ns_1@127.0.0.1:ns_config_rep<0.248.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/8479338376212444691/0">>}]..)
[ns_server:debug,2019-07-04T11:54:04.252Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:54:04.252Z,ns_1@127.0.0.1:<0.4482.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:54:04.255Z,ns_1@127.0.0.1:<0.4484.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 223347, disk size is 4241408
[ns_server:debug,2019-07-04T11:54:04.255Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:54:04.255Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:54:04.255Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:54:04.260Z,ns_1@127.0.0.1:<0.4486.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:54:04.261Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:54:04.261Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:54:34.256Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:54:34.257Z,ns_1@127.0.0.1:<0.5937.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:54:34.258Z,ns_1@127.0.0.1:<0.5939.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 223347, disk size is 4241408
[ns_server:debug,2019-07-04T11:54:34.259Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:54:34.259Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:54:34.262Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:54:34.267Z,ns_1@127.0.0.1:<0.5940.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:54:34.267Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:54:34.267Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:55:04.260Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:55:04.260Z,ns_1@127.0.0.1:<0.7399.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:55:04.262Z,ns_1@127.0.0.1:<0.7401.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 223347, disk size is 4241408
[ns_server:debug,2019-07-04T11:55:04.262Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:55:04.262Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:55:04.268Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:55:04.273Z,ns_1@127.0.0.1:<0.7402.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:55:04.274Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:55:04.274Z,ns_1@127.0.0.1:compaction_new_daemon<0.416.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2019-07-04T11:55:53.142Z,nonode@nohost:<0.89.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2019-07-04T11:55:53.155Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10},
 {ipv6,false}]
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.156Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:warn,2019-07-04T11:55:53.157Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter ipv6, which is given from command line
[error_logger:info,2019-07-04T11:55:53.183Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.138.0>},
                       {name,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:53.192Z,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,9,125}
Runtime info: [{otp_release,"R16B03-1"},
               {erl_version,"5.10.4.0.0.1"},
               {erl_version_long,
                   "Erlang R16B03-1 (erts-5.10.4.0.0.1) [source-6d69bef] [64-bit] [smp:2:2] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2019,7,4},{11,55,53}}},
               {memory,
                   [{total,110181144},
                    {processes,9340088},
                    {processes_used,9338304},
                    {system,100841056},
                    {atom,339441},
                    {atom_used,322567},
                    {binary,49280},
                    {code,7796127},
                    {ets,2241552}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,path_config,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',otp_internal,
                    io_lib_fread,ns_log_sink,ale_disk_sink,misc,couch_util,
                    ns_server,filelib,cpu_sup,memsup,disksup,os_mon,io,
                    release_handler,overload,alarm_handler,sasl,timer,
                    tftp_sup,httpd_sup,httpc_handler_sup,httpc_cookie,
                    inets_trace,httpc_manager,httpc,httpc_profile_sup,
                    httpc_sup,ftp_sup,inets_sup,inets_app,ssl,lhttpc_manager,
                    lhttpc_sup,lhttpc,tls_connection_sup,ssl_session_cache,
                    ssl_pkix_db,ssl_manager,ssl_sup,ssl_app,crypto_server,
                    crypto_sup,crypto_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_validator,beam_z,
                    beam_flatten,beam_trim,beam_receive,beam_bsm,beam_peep,
                    beam_dead,beam_split,beam_type,beam_bool,beam_except,
                    beam_clean,beam_utils,beam_block,beam_jump,beam_a,
                    v3_codegen,v3_life,v3_kernel,sys_core_dsetel,erl_bifs,
                    sys_core_fold,cerl_trees,sys_core_inline,core_lib,cerl,
                    v3_core,erl_bits,erl_expand_records,sys_pre_expand,sofs,
                    erl_internal,sets,ordsets,erl_lint,compile,
                    dynamic_compile,ale_utils,io_lib_pretty,io_lib_format,
                    io_lib,ale_codegen,dict,ale,ale_dynamic_sup,ale_sup,
                    ale_app,epp,ns_bootstrap,child_erlang,file_io_server,
                    orddict,erl_eval,file,c,kernel_config,user_io,user_sup,
                    supervisor_bridge,standard_error,code_server,unicode,
                    hipe_unified_loader,gb_sets,ets,binary,code,file_server,
                    net_kernel,global_group,erl_distribution,filename,
                    inet_gethost_native,os,inet_parse,inet,inet_udp,
                    inet_config,inet_db,global,gb_trees,rpc,supervisor,kernel,
                    application_master,sys,application,gen_server,erl_parse,
                    proplists,erl_scan,lists,application_controller,proc_lib,
                    gen,gen_event,error_logger,heart,error_handler,
                    erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {os_mon,"CPO  CXC 138 46","2.2.14"},
                    {public_key,"Public key infrastructure","0.21"},
                    {asn1,"The Erlang ASN1 compiler version 2.0.4","2.0.4"},
                    {kernel,"ERTS  CXC 138 10","2.16.4"},
                    {ale,"Another Logger for Erlang","6.0.0-1693-community"},
                    {inets,"INETS  CXC 138 49","5.9.8"},
                    {ns_server,"Couchbase server","6.0.0-1693-community"},
                    {crypto,"CRYPTO version 2","3.2"},
                    {ssl,"Erlang/OTP SSL application","5.3.3"},
                    {sasl,"SASL  CXC 138 11","2.3.4"},
                    {stdlib,"ERTS  CXC 138 10","1.19.4"}]},
               {pre_loaded,
                   [erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {process_count,105},
               {node,nonode@nohost},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,code_server,ale_stats_events,
                    ns_server_cluster_sup,lhttpc_manager,
                    application_controller,ale,'sink-ns_log',httpd_sup,
                    release_handler,kernel_safe_sup,standard_error,ale_sup,
                    overload,error_logger,'sink-disk_json_rpc',alarm_handler,
                    ale_dynamic_sup,'sink-disk_metakv',timer_server,
                    standard_error_sup,'sink-disk_access_int',
                    'sink-disk_access',crypto_server,'sink-disk_reports',
                    crypto_sup,sasl_safe_sup,'sink-disk_stats',tftp_sup,
                    'sink-disk_xdcr',inet_db,init,os_mon_sup,rex,
                    'sink-disk_debug',tls_connection_sup,user,ssl_sup,
                    kernel_sup,cpu_sup,'sink-disk_error',global_name_server,
                    memsup,disksup,'sink-disk_default',httpc_sup,
                    file_server_2,ssl_manager,local_tasks,global_group,
                    httpc_profile_sup,httpc_manager,httpc_handler_sup,ftp_sup,
                    sasl_sup,erl_prim_loader,inets_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,3}]
[ns_server:info,2019-07-04T11:55:53.203Z,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"d200fc0f68695d4aef1fad5c3c8cc55f8c033014\" upstream=\"refs/tags/0.9.7\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"a5567811193b0cc3571fe94e42fc1b8a6a80bc5b\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"10233dcde760b61f4ffac0479bc3a8cabff73beb\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"faa4390d57116ccbdcfc8f00e3affd3044a890cc\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"9e3465560240ffb242b50a47cb7f19251a12ee42\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"055db35bf221ccdc62363f1c4ad88eaac2b892ab\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"f551b6d4f32bb920a83dd28c705bddd5de0d03b2\" />",
 "  <project name=\"blevex\" path=\"godeps/src/github.com/blevesearch/blevex\" remote=\"blevesearch\" revision=\"4b158bb555a3297565afecf6fae675c74f1e47df\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"523d6077a2ec14038605cf8a1feeecaa29c44deb\" upstream=\"alice\">",
 "    <annotation name=\"RELEASE\" value=\"alice\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"1693\" />",
 "    <annotation name=\"VERSION\" value=\"6.0.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbaselabs/cbas\" revision=\"b1f12f65c27f72f582a291ef5a4b72c7a5bf1af0\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"0f3911da8789ba9436962fac63e6928c60f46e6c\" upstream=\"alice\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"78fd5f8ce545e6082271e74cd9f85aa8b8fbbe0d\" upstream=\"alice\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"0df84c7e3c6d95ff435c12a3c08c6f064db11e97\" />",
 "  <project name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"80d2ad8892d806f5103f602fec0d80adaa4b628f\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"a33ad7b7000a9d8d237ba273c47cc100401a0fb0\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"0a94f40b9080e0ecb11d3b7531a58c5e6a4a4465\" upstream=\"master\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbq-gui\" path=\"goproj/src/github.com/couchbase/cbq-gui\" remote=\"couchbase-priv\" revision=\"19fecfe58921c162a31c156781bd2a512711f14d\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"dbfa1c0d73f0e49f6f04e390f03de8f9a6cee769\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"dcae66272b24600ae0005fa06b511cfae8914d3d\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project name=\"couchbase-cli\" revision=\"d04a2983f3f014442d2ec1132bb505aa6c025dc3\" upstream=\"alice\" />",
 "  <project name=\"couchdb\" revision=\"45731d9f42d8046f8ba9ccb754657eb2996b5c4a\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"c545a8563778bfc40284caf9213c7925488e633a\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"3b4c35d79a35756c26ae547e0759b8ef08aa8438\" upstream=\"vulcan\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"f23ba3a5ee43012fcb4b92e1a2a405a92554f4f2\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"05067021a042a1b63e100a486afd7ebddab4c535\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"e84dd5be6c2b899e77bd3fc24a930cc2bcf9188d\" upstream=\"alice\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"kv\" name=\"forestdb\" revision=\"562366039e50730282548b02c1a30d73f97cba27\" upstream=\"vulcan\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"2a0e73f43451045f157640eec59ced72da18471f\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"9b3739952a0900be7628424082559d41dc3cd0d1\" upstream=\"alice\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"3b0453ce6faae42ab4d8cdb9ac1f93919c9d8d69\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"47fc4e5e9153645da45af6a86a5bce95e63a0f9e\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d2f15a425a9c8e4d8447e5f5b89ce14845f7fa05\" upstream=\"vulcan\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb\" revision=\"699b13a51af5dd4f80ff3deedf41bba60debad32\" upstream=\"refs/tags/v1.3.7\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"710456e087a6d497e87f41d0a9d98d6a75672186\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"a0d26c2d6f5de912499d35a5aba573006e5e036f\" upstream=\"refs/tags/v7.1.7\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"aecdbe5a5a91f0688df7bdf260ca962178c06828\" upstream=\"vulcan\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"0da75df145308b9a4e6704d762ca9d9b77752efc\" upstream=\"vulcan\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"1e3589e665a728ec9a2c64b516fd26f52ac2663a\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"f98adca8eb365032cab838ef4d99453931afa112\" upstream=\"vulcan\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"a465a37b72784b21dde0290235a9066147fbb12f\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"28aa45915e50214577e4a7810a2a508c1d17934b\" upstream=\"alice\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"d0c17cc8a803812c2d2a304479cc3a0b200c9aba\" upstream=\"alice\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"081e8b16b991bf706eb77f8243935c6fba31b895\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"3681c2a912330352991ecdd642f257efe5b85518\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"956632ec1bc3e28276d00ee2f22c3202f06efb12\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"abd3b58b422dbc2e9463a589d0f3d93441726e23\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project groups=\"kv\" name=\"moxi\" revision=\"cd8da46b9b953800d430c8b0aa4667790727ed6f\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"62685c2d7ca23c807425dca88b11a3e2323dab41\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"f3bef3551997be504612a2d05a8b324b3bfdfe1b\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"43a2cac6976489bb79896f09695f2af2d9b53857\" upstream=\"alice\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"96501c57bb0fd61c85cba6f63101aed2bcf41d38\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"30badd9e911df0e6dd28e4eec2949fe144c0235c\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"2fbe5179a2673a9275cd0906daa4b1cab38a3eb5\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"55e45187ca8943aa8910e9ae2b59b41242d14386\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"655cdfa588ea190e901bc5590e65d5621688847c\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"df2438af8bb18ba35e599caa1a7afe2eab2c5137\" upstream=\"alice\" />",
 "  <project name=\"query-ui\" revision=\"15a6461f437fe810e411a6613ec7c143991cd1c6\" upstream=\"alice\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"fe09428be4c233d726797a1380f7438f4f71a31a\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"73353fe6dad8f3d67409feefb9b17f90f6de917b\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"4fba14c79f356ae48d6141c561bf9fd7ba33fabd\" upstream=\"refs/tags/v0.14.0\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"c30c3d4c250e68e81c57aa1e8ae91ffd21243cdb\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"9d4e42a20653790449273b3c85e67d6d8bae6e2e\" />",
 "  <project name=\"testrunner\" revision=\"d8f6c71dd26932f304c281645267c31146ef3e1c\" upstream=\"alice\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"601048ad6acbab6cedd582db09b8c4839ff25b15\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"b277d99e18b0ad625405c4cf1ec79af8c94710c7\" upstream=\"alice\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"0ceea4a37442f76199b9259840baf48d17af3c1a\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"5d12c10af88bec4e655ca358aaa3e6d60193a082\" upstream=\"alice\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2019-07-04T11:55:53.260Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.139.0>},
                       {name,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:53.265Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:86]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2019-07-04T11:55:53.266Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:86]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2019-07-04T11:55:53.267Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:init:163]ip config not found. Looks like we're brand new node
[error_logger:info,2019-07-04T11:55:53.268Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.143.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2019-07-04T11:55:53.269Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.142.0>},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:53.671Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:bringup:215]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2019-07-04T11:55:53.682Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.145.0>},
                       {name,erl_epmd},
                       {mfargs,{erl_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.682Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.146.0>},
                       {name,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.683Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.147.0>},
                       {name,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames]]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.683Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.144.0>},
                       {name,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:53.683Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:configure_net_kernel:259]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2019-07-04T11:55:53.686Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:save_node:147]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2019-07-04T11:55:53.693Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:bringup:229]Attempted to save node name to disk: ok
[ns_server:debug,2019-07-04T11:55:53.694Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:wait_for_node:236]Waiting for connection to node 'babysitter_of_ns_1@127.0.0.1' to be established
[error_logger:info,2019-07-04T11:55:53.694Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:55:53.705Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:wait_for_node:248]Observed node 'babysitter_of_ns_1@127.0.0.1' to come up
[error_logger:info,2019-07-04T11:55:53.706Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.141.0>},
                       {name,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.713Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.152.0>},
                       {name,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.715Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.153.0>},
                       {name,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:53.718Z,ns_1@127.0.0.1:ns_config_sup<0.154.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2019-07-04T11:55:53.719Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.155.0>},
                       {name,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.719Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.156.0>},
                       {name,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:53.785Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1095]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2019-07-04T11:55:53.790Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1109]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2019-07-04T11:55:53.833Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1117]Here's full dynamic config we loaded:
[[{{metakv,<<"/indexing/ddl/commandToken/create/8479338376212444691/0">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460426}}]}|
    '_deleted']},
  {buckets,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{7,63729460406}}]},
    {configs,
     [{"app",
       [{repl_type,dcp},
        {uuid,<<"5b6ce58456b7220bfb025f341cb20648">>},
        {auth_type,sasl},
        {num_replicas,1},
        {replica_index,false},
        {ram_quota,1042284544},
        {autocompaction,false},
        {purge_interval,undefined},
        {flush_enabled,false},
        {num_threads,3},
        {eviction_policy,value_only},
        {conflict_resolution_type,seqno},
        {storage_mode,couchstore},
        {max_ttl,0},
        {compression_mode,off},
        {type,membase},
        {num_vbuckets,1024},
        {replication_topology,star},
        {servers,['ns_1@127.0.0.1']},
        {sasl_password,"*****"},
        {map,
         [['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined],
          ['ns_1@127.0.0.1',undefined]]},
        {map_opts_hash,133465355}]}]}]},
  {vbucket_map_history,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460406}}]},
    {[['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined],
      ['ns_1@127.0.0.1',undefined]],
     [{replication_topology,star},{tags,undefined},{max_slaves,10}]}]},
  {{metakv,<<"/indexing/info/versionToken">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460389}}]}|
    <<"{\"Version\":3}">>]},
  {{service_map,n1ql},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
    'ns_1@127.0.0.1']},
  {{service_map,index},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
    'ns_1@127.0.0.1']},
  {{metakv,<<"/indexing/rebalance/RebalanceToken">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]}|
    '_deleted']},
  {{service_map,fts},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
    'ns_1@127.0.0.1']},
  {{metakv,<<"/fts/cbgt/cfg/version">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
    <<"5.5.0">>]},
  {{metakv,
    <<"/fts/cbgt/cfg/nodeDefs-wanted/2a8ae539a5cab1af4159b9b57e0098ee">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
    <<"{\"uuid\":\"10e09678eaa2bb72\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\"v0.6.0\\\",\\\"version-cbft.lib\\\":\\\"v0.5.5\\\"}\"}},\"implVersion\":\"5.5.0\"}">>]},
  {{metakv,
    <<"/fts/cbgt/cfg/nodeDefs-known/2a8ae539a5cab1af4159b9b57e0098ee">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
    <<"{\"uuid\":\"4704a37aa982ccf9\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\"v0.6.0\\\",\\\"version-cbft.lib\\\":\\\"v0.5.5\\\"}\"}},\"implVersion\":\"5.5.0\"}">>]},
  {{metakv,<<"/query/settings/config">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
    <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>]},
  {uuid,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
    <<"070fee742e7318eb771177d58d7c8507">>]},
  {rest_creds,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
    {"<ud>admin</ud>",
     {auth,
      [{<<"plain">>,"*****"},
       {<<"sha512">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,
           <<"jkKoQCLgS0hwSaQwbjFuPTPff4IF8pnVfaYqo4gjzGRi9I4+EJ+qkWwBnlE0q7M4oewExLOJOdnRZy8S34iiWg==">>},
          {<<"i">>,4000}]}},
       {<<"sha256">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,<<"a0jO063Ch2WEsXWFP3cZ2iHE8JeSZPweQjKljT1T4r0=">>},
          {<<"i">>,4000}]}},
       {<<"sha1">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,<<"X1H9WHt5Rn/HyJ8kJ4/Z07G+iSc=">>},
          {<<"i">>,4000}]}}]}}]},
  {rest,[{port,8091}]},
  {{metakv,<<"/indexing/settings/config">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{3,63729460385}}]}|
    <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"forestdb\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":365953024,\"indexer.settings.compaction.abort_exceed_interval\":false}">>]},
  {cluster_name,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
    97,112,112]},
  {{node,'ns_1@127.0.0.1',services},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
    fts,index,kv,n1ql]},
  {memory_quota,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
    994]},
  {fts_memory_quota,256},
  {settings,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
    {stats,[{send_stats,true}]}]},
  {auto_failover_cfg,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
    {enabled,true},
    {timeout,120},
    {count,0},
    {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
    {failover_server_group,false},
    {max_count,1},
    {failed_over_server_groups,[]}]},
  {audit_decriptors,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
    {20480,
     [{name,<<"opened DCP connection">>},
      {description,<<"opened DCP connection">>},
      {enabled,true},
      {module,memcached}]},
    {20482,
     [{name,<<"external memcached bucket flush">>},
      {description,
       <<"External user flushed the content of a memcached bucket">>},
      {enabled,true},
      {module,memcached}]},
    {20483,
     [{name,<<"invalid packet">>},
      {description,<<"Rejected an invalid packet">>},
      {enabled,true},
      {module,memcached}]},
    {20485,
     [{name,<<"authentication succeeded">>},
      {description,<<"Authentication to the cluster succeeded">>},
      {enabled,false},
      {module,memcached}]},
    {20488,
     [{name,<<"document read">>},
      {description,<<"Document was read">>},
      {enabled,false},
      {module,memcached}]},
    {20489,
     [{name,<<"document locked">>},
      {description,<<"Document was locked">>},
      {enabled,false},
      {module,memcached}]},
    {20490,
     [{name,<<"document modify">>},
      {description,<<"Document was modified">>},
      {enabled,false},
      {module,memcached}]},
    {20491,
     [{name,<<"document delete">>},
      {description,<<"Document was deleted">>},
      {enabled,false},
      {module,memcached}]},
    {28672,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28673,
     [{name,<<"EXPLAIN statement">>},
      {description,<<"A N1QL EXPLAIN statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28674,
     [{name,<<"PREPARE statement">>},
      {description,<<"A N1QL PREPARE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28675,
     [{name,<<"INFER statement">>},
      {description,<<"A N1QL INFER statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28676,
     [{name,<<"INSERT statement">>},
      {description,<<"A N1QL INSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28677,
     [{name,<<"UPSERT statement">>},
      {description,<<"A N1QL UPSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28678,
     [{name,<<"DELETE statement">>},
      {description,<<"A N1QL DELETE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28679,
     [{name,<<"UPDATE statement">>},
      {description,<<"A N1QL UPDATE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28680,
     [{name,<<"MERGE statement">>},
      {description,<<"A N1QL MERGE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28681,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28682,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28683,
     [{name,<<"ALTER INDEX statement">>},
      {description,<<"A N1QL ALTER INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28684,
     [{name,<<"BUILD INDEX statement">>},
      {description,<<"A N1QL BUILD INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28685,
     [{name,<<"GRANT ROLE statement">>},
      {description,<<"A N1QL GRANT ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28686,
     [{name,<<"REVOKE ROLE statement">>},
      {description,<<"A N1QL REVOKE ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28687,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,
       <<"An unrecognized statement was received by the N1QL query engine">>},
      {enabled,false},
      {module,n1ql}]},
    {28688,
     [{name,<<"CREATE PRIMARY INDEX statement">>},
      {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28689,
     [{name,<<"/admin/stats API request">>},
      {description,<<"An HTTP request was made to the API at /admin/stats.">>},
      {enabled,false},
      {module,n1ql}]},
    {28690,
     [{name,<<"/admin/vitals API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/vitals.">>},
      {enabled,false},
      {module,n1ql}]},
    {28691,
     [{name,<<"/admin/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28692,
     [{name,<<"/admin/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28693,
     [{name,<<"/admin/indexes/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28694,
     [{name,<<"/admin/indexes/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28695,
     [{name,<<"/admin/indexes/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28697,
     [{name,<<"/admin/ping API request">>},
      {description,<<"An HTTP request was made to the API at /admin/ping.">>},
      {enabled,false},
      {module,n1ql}]},
    {28698,
     [{name,<<"/admin/config API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/config.">>},
      {enabled,false},
      {module,n1ql}]},
    {28699,
     [{name,<<"/admin/ssl_cert API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
      {enabled,false},
      {module,n1ql}]},
    {28700,
     [{name,<<"/admin/settings API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/settings.">>},
      {enabled,false},
      {module,n1ql}]},
    {28701,
     [{name,<<"/admin/clusters API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/clusters.">>},
      {enabled,false},
      {module,n1ql}]},
    {28702,
     [{name,<<"/admin/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]}]},
  {scramsha_fallback_salt,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
    <<131,49,2,131,220,38,244,3,105,90,168,156>>]},
  {{rbac_upgrade,[5,5]},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
    '_deleted']},
  {{metakv,<<"/eventing/settings/config">>},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
    <<"{\"ram_quota\":256}">>]},
  {client_cert_auth,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
    {state,"disable"},
    {prefixes,[]}]},
  {users_upgrade,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
    '_deleted']},
  {roles_definitions,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
    '_deleted']},
  {cluster_compat_version,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{8,63729460354}}]},
    6,0]},
  {otp,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460353}}]},
    {cookie,{sanitized,<<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}}]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
    {enabled,[]},
    {disabled_users,[]},
    {auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {cbas_memory_quota,1024},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,10},
  {memcached,[]},
  {nodes_wanted,['ns_1@127.0.0.1']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
    'ns_1@127.0.0.1']},
  {read_only_user_creds,
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
    '_deleted']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    {5,5,3}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',ldap_enabled},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',membership},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    active]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
    {port,11210},
    {dedicated_port,11209},
    {ssl_port,undefined},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {maxconn,maxconn},
           {ipv4,<<"required">>},
           {ipv6,<<"optional">>}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {maxconn,dedicated_port_maxconn},
           {ipv4,<<"required">>},
           {ipv6,<<"optional">>}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {maxconn,maxconn},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,<<"required">>},
           {ipv6,<<"optional">>}]}]}},
      {ssl_cipher_list,{"~s",[ssl_cipher_list]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,{memcached_config_mgr,is_enabled,[[5,0]]}},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
    {maxconn,30000},
    {dedicated_port_maxconn,5000},
    {ssl_cipher_list,"HIGH"},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,false},
    {datatype_snappy,true}]},
  {{node,'ns_1@127.0.0.1',moxi},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
    {port,11211},
    {verbosity,[]}]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    undefined]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    <<"2a8ae539a5cab1af4159b9b57e0098ee">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
    false]},
  {{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
   [{'_vclock',
     [{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{28,63729460426}}]}]}]]
[ns_server:info,2019-07-04T11:55:53.863Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1138]Here's full dynamic config we loaded + static & default config:
[{{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{28,63729460426}}]}]},
 {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   <<"2a8ae539a5cab1af4159b9b57e0098ee">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {port,11211},
   {verbosity,[]}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {maxconn,30000},
   {dedicated_port_maxconn,5000},
   {ssl_cipher_list,"HIGH"},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,false},
   {datatype_snappy,true}]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {maxconn,maxconn},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {maxconn,dedicated_port_maxconn},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {maxconn,maxconn},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]}]}},
     {ssl_cipher_list,{"~s",[ssl_cipher_list]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,{memcached_config_mgr,is_enabled,[[5,0]]}},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {port,11210},
   {dedicated_port,11209},
   {ssl_port,undefined},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   active]},
 {{node,'ns_1@127.0.0.1',ldap_enabled},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   {5,5,3}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {secure_headers,[]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {read_only_user_creds,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
   '_deleted']},
 {quorum_nodes,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
   'ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {memcached,[]},
 {max_bucket_count,10},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cbas_memory_quota,1024},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {audit,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
   {enabled,[]},
   {disabled_users,[]},
   {auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {otp,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460353}}]},
   {cookie,{sanitized,<<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}}]},
 {cluster_compat_version,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{8,63729460354}}]},
   6,0]},
 {roles_definitions,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
   '_deleted']},
 {users_upgrade,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
   '_deleted']},
 {client_cert_auth,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
   {state,"disable"},
   {prefixes,[]}]},
 {{metakv,<<"/eventing/settings/config">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
   <<"{\"ram_quota\":256}">>]},
 {{rbac_upgrade,[5,5]},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
   '_deleted']},
 {scramsha_fallback_salt,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
   <<131,49,2,131,220,38,244,3,105,90,168,156>>]},
 {audit_decriptors,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
   {20480,
    [{name,<<"opened DCP connection">>},
     {description,<<"opened DCP connection">>},
     {enabled,true},
     {module,memcached}]},
   {20482,
    [{name,<<"external memcached bucket flush">>},
     {description,
      <<"External user flushed the content of a memcached bucket">>},
     {enabled,true},
     {module,memcached}]},
   {20483,
    [{name,<<"invalid packet">>},
     {description,<<"Rejected an invalid packet">>},
     {enabled,true},
     {module,memcached}]},
   {20485,
    [{name,<<"authentication succeeded">>},
     {description,<<"Authentication to the cluster succeeded">>},
     {enabled,false},
     {module,memcached}]},
   {20488,
    [{name,<<"document read">>},
     {description,<<"Document was read">>},
     {enabled,false},
     {module,memcached}]},
   {20489,
    [{name,<<"document locked">>},
     {description,<<"Document was locked">>},
     {enabled,false},
     {module,memcached}]},
   {20490,
    [{name,<<"document modify">>},
     {description,<<"Document was modified">>},
     {enabled,false},
     {module,memcached}]},
   {20491,
    [{name,<<"document delete">>},
     {description,<<"Document was deleted">>},
     {enabled,false},
     {module,memcached}]},
   {28672,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28673,
    [{name,<<"EXPLAIN statement">>},
     {description,<<"A N1QL EXPLAIN statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28674,
    [{name,<<"PREPARE statement">>},
     {description,<<"A N1QL PREPARE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28675,
    [{name,<<"INFER statement">>},
     {description,<<"A N1QL INFER statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28676,
    [{name,<<"INSERT statement">>},
     {description,<<"A N1QL INSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28677,
    [{name,<<"UPSERT statement">>},
     {description,<<"A N1QL UPSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28678,
    [{name,<<"DELETE statement">>},
     {description,<<"A N1QL DELETE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28679,
    [{name,<<"UPDATE statement">>},
     {description,<<"A N1QL UPDATE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28680,
    [{name,<<"MERGE statement">>},
     {description,<<"A N1QL MERGE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28681,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28682,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28683,
    [{name,<<"ALTER INDEX statement">>},
     {description,<<"A N1QL ALTER INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28684,
    [{name,<<"BUILD INDEX statement">>},
     {description,<<"A N1QL BUILD INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28685,
    [{name,<<"GRANT ROLE statement">>},
     {description,<<"A N1QL GRANT ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28686,
    [{name,<<"REVOKE ROLE statement">>},
     {description,<<"A N1QL REVOKE ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28687,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,
      <<"An unrecognized statement was received by the N1QL query engine">>},
     {enabled,false},
     {module,n1ql}]},
   {28688,
    [{name,<<"CREATE PRIMARY INDEX statement">>},
     {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28689,
    [{name,<<"/admin/stats API request">>},
     {description,<<"An HTTP request was made to the API at /admin/stats.">>},
     {enabled,false},
     {module,n1ql}]},
   {28690,
    [{name,<<"/admin/vitals API request">>},
     {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
     {enabled,false},
     {module,n1ql}]},
   {28691,
    [{name,<<"/admin/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28692,
    [{name,<<"/admin/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28693,
    [{name,<<"/admin/indexes/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28694,
    [{name,<<"/admin/indexes/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28695,
    [{name,<<"/admin/indexes/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28697,
    [{name,<<"/admin/ping API request">>},
     {description,<<"An HTTP request was made to the API at /admin/ping.">>},
     {enabled,false},
     {module,n1ql}]},
   {28698,
    [{name,<<"/admin/config API request">>},
     {description,<<"An HTTP request was made to the API at /admin/config.">>},
     {enabled,false},
     {module,n1ql}]},
   {28699,
    [{name,<<"/admin/ssl_cert API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
     {enabled,false},
     {module,n1ql}]},
   {28700,
    [{name,<<"/admin/settings API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/settings.">>},
     {enabled,false},
     {module,n1ql}]},
   {28701,
    [{name,<<"/admin/clusters API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/clusters.">>},
     {enabled,false},
     {module,n1ql}]},
   {28702,
    [{name,<<"/admin/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]}]},
 {auto_failover_cfg,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
   {enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]}]},
 {settings,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
   {stats,[{send_stats,true}]}]},
 {fts_memory_quota,256},
 {memory_quota,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
   994]},
 {{node,'ns_1@127.0.0.1',services},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
   fts,index,kv,n1ql]},
 {cluster_name,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
   97,112,112]},
 {{metakv,<<"/indexing/settings/config">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{3,63729460385}}]}|
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"forestdb\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":365953024,\"indexer.settings.compaction.abort_exceed_interval\":false}">>]},
 {rest,[{port,8091}]},
 {rest_creds,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
   {"<ud>admin</ud>",
    {auth,
     [{<<"plain">>,"*****"},
      {<<"sha512">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,
          <<"jkKoQCLgS0hwSaQwbjFuPTPff4IF8pnVfaYqo4gjzGRi9I4+EJ+qkWwBnlE0q7M4oewExLOJOdnRZy8S34iiWg==">>},
         {<<"i">>,4000}]}},
      {<<"sha256">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,<<"a0jO063Ch2WEsXWFP3cZ2iHE8JeSZPweQjKljT1T4r0=">>},
         {<<"i">>,4000}]}},
      {<<"sha1">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,<<"X1H9WHt5Rn/HyJ8kJ4/Z07G+iSc=">>},
         {<<"i">>,4000}]}}]}}]},
 {uuid,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
   <<"070fee742e7318eb771177d58d7c8507">>]},
 {{metakv,<<"/query/settings/config">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
   <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>]},
 {{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/2a8ae539a5cab1af4159b9b57e0098ee">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
   <<"{\"uuid\":\"4704a37aa982ccf9\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\"v0.6.0\\\",\\\"version-cbft.lib\\\":\\\"v0.5.5\\\"}\"}},\"implVersion\":\"5.5.0\"}">>]},
 {{metakv,
   <<"/fts/cbgt/cfg/nodeDefs-wanted/2a8ae539a5cab1af4159b9b57e0098ee">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
   <<"{\"uuid\":\"10e09678eaa2bb72\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\"v0.6.0\\\",\\\"version-cbft.lib\\\":\\\"v0.5.5\\\"}\"}},\"implVersion\":\"5.5.0\"}">>]},
 {{metakv,<<"/fts/cbgt/cfg/version">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
   <<"5.5.0">>]},
 {{service_map,fts},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
   'ns_1@127.0.0.1']},
 {{metakv,<<"/indexing/rebalance/RebalanceToken">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]}|
   '_deleted']},
 {{service_map,index},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
   'ns_1@127.0.0.1']},
 {{service_map,n1ql},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
   'ns_1@127.0.0.1']},
 {{metakv,<<"/indexing/info/versionToken">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460389}}]}|
   <<"{\"Version\":3}">>]},
 {vbucket_map_history,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460406}}]},
   {[['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined],
     ['ns_1@127.0.0.1',undefined]],
    [{replication_topology,star},{tags,undefined},{max_slaves,10}]}]},
 {buckets,
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{7,63729460406}}]},
   {configs,
    [{"app",
      [{repl_type,dcp},
       {uuid,<<"5b6ce58456b7220bfb025f341cb20648">>},
       {auth_type,sasl},
       {num_replicas,1},
       {replica_index,false},
       {ram_quota,1042284544},
       {autocompaction,false},
       {purge_interval,undefined},
       {flush_enabled,false},
       {num_threads,3},
       {eviction_policy,value_only},
       {conflict_resolution_type,seqno},
       {storage_mode,couchstore},
       {max_ttl,0},
       {compression_mode,off},
       {type,membase},
       {num_vbuckets,1024},
       {replication_topology,star},
       {servers,['ns_1@127.0.0.1']},
       {sasl_password,"*****"},
       {map,
        [['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined],
         ['ns_1@127.0.0.1',undefined]]},
       {map_opts_hash,133465355}]}]}]},
 {{metakv,<<"/indexing/ddl/commandToken/create/8479338376212444691/0">>},
  [{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460426}}]}|
   '_deleted']}]
[error_logger:info,2019-07-04T11:55:53.884Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.157.0>},
                       {name,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.887Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.163.0>},
                       {name,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.890Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.164.0>},
                       {name,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.891Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.154.0>},
                       {name,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:53.894Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.166.0>},
                       {name,vbucket_filter_changes_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               [vbucket_filter_changes_registry,
                                [{terminate_command,shutdown}]]}},
                       {restart_type,permanent},
                       {shutdown,100},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.897Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.167.0>},
                       {name,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:53.911Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.170.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:53.913Z,ns_1@127.0.0.1:menelaus_barrier<0.171.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2019-07-04T11:55:53.913Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.171.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.914Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.172.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.922Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.173.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.924Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.175.0>},
                       {name,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.937Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.174.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:53.940Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.178.0>},
                       {name,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.958Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.180.0>},
                       {name,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:53.961Z,ns_1@127.0.0.1:users_replicator<0.180.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:55:53.963Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_storage:anounce_startup:69]Announce my startup to <0.180.0>
[ns_server:debug,2019-07-04T11:55:53.964Z,ns_1@127.0.0.1:users_replicator<0.180.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <0.181.0>
[ns_server:debug,2019-07-04T11:55:53.967Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:open:212]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2019-07-04T11:55:53.967Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.181.0>},
                       {name,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:53.967Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.179.0>},
                       {name,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:54.000Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:init:44]Starting versioned cache compiled_roles_cache
[error_logger:info,2019-07-04T11:55:54.001Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.183.0>},
                       {name,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:54.001Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.177.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:54.002Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.186.0>},
                       {name,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:54.003Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.187.0>},
                       {name,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:54.027Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:convert_docs_to_55_in_dets:243]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2019-07-04T11:55:54.028Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,'$1','_','$2'},
                                      [],
                                      [{{'$1','$2'}}]}],
                                    100}
[ns_server:debug,2019-07-04T11:55:54.031Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:init_after_ack:204]Loading 0 items, 299 words took 2ms
[ns_server:debug,2019-07-04T11:55:54.037Z,ns_1@127.0.0.1:users_replicator<0.180.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2019-07-04T11:55:54.040Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.191.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:141]Waiting for ns_couchdb node to start
[error_logger:info,2019-07-04T11:55:54.040Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.190.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:54.040Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:55:54.043Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:55:54.043Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.194.0>,shutdown}}
[error_logger:info,2019-07-04T11:55:54.043Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:54.245Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:54.248Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.197.0>,shutdown}}
[ns_server:debug,2019-07-04T11:55:54.248Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:55:54.248Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:54.451Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:54.454Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.200.0>,shutdown}}
[error_logger:info,2019-07-04T11:55:54.454Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:55:54.454Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:55:54.657Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:54.659Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.203.0>,shutdown}}
[error_logger:info,2019-07-04T11:55:54.659Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:55:54.659Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:55:54.862Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:54.864Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.206.0>,shutdown}}
[error_logger:info,2019-07-04T11:55:54.864Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:55:54.864Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:55:55.068Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:55.070Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.209.0>,shutdown}}
[ns_server:debug,2019-07-04T11:55:55.070Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-07-04T11:55:55.071Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-07-04T11:55:55.273Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-07-04T11:55:55.309Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:55:55.510Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:55:55.711Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:55:55.912Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:55:56.116Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:55:56.317Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-07-04T11:55:56.520Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[error_logger:info,2019-07-04T11:55:57.132Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.221.0>},
                       {name,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:57.333Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: Apache CouchDB v4.5.1-108-g45731d9 (LogLevel=info) is starting.

[ns_server:info,2019-07-04T11:55:57.944Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2019-07-04T11:55:58.083Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.191.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.96617950>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.132Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:ns_storage_conf:setup_db_and_ix_paths:47]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2019-07-04T11:55:58.141Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.224.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.143Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.225.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:58.145Z,ns_1@127.0.0.1:ns_server_sup<0.223.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2019-07-04T11:55:58.148Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.226.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.186Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.227.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.216Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.228.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.217Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.229.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.245Z,ns_1@127.0.0.1:memcached_passwords<0.230.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-07-04T11:55:58.249Z,ns_1@127.0.0.1:memcached_passwords<0.230.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2019-07-04T11:55:58.305Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: 169: Booted. Waiting for shutdown request

[ns_server:debug,2019-07-04T11:55:58.327Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.230.0>
[ns_server:debug,2019-07-04T11:55:58.328Z,ns_1@127.0.0.1:memcached_passwords<0.230.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:55:58.335Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.230.0>
[ns_server:debug,2019-07-04T11:55:58.340Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2019-07-04T11:55:58.340Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.230.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.343Z,ns_1@127.0.0.1:memcached_permissions<0.233.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:55:58.368Z,ns_1@127.0.0.1:memcached_permissions<0.233.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:55:58.373Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.233.0>
[ns_server:debug,2019-07-04T11:55:58.374Z,ns_1@127.0.0.1:memcached_permissions<0.233.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:55:58.375Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.233.0>
[error_logger:info,2019-07-04T11:55:58.378Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.233.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.379Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.236.0>},
                       {name,ns_log_events},
                       {mfargs,{gen_event,start_link,[{local,ns_log_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.382Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.238.0>},
                       {name,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.382Z,ns_1@127.0.0.1:ns_node_disco<0.239.0>:ns_node_disco:init:130]Initting ns_node_disco with []
[ns_server:debug,2019-07-04T11:55:58.382Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[user:info,2019-07-04T11:55:58.382Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:127]Node 'ns_1@127.0.0.1' synchronized otp cookie {sanitized,
                                               <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>} from cluster
[ns_server:debug,2019-07-04T11:55:58.382Z,ns_1@127.0.0.1:<0.240.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:55:58.396Z,ns_1@127.0.0.1:<0.240.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[error_logger:info,2019-07-04T11:55:58.396Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.239.0>},
                       {name,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:55:58.396Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:58.396Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:55:58.397Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2019-07-04T11:55:58.407Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.241.0>},
                       {name,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:55:58.408Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:58.408Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:55:58.427Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.242.0>},
                       {name,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.431Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.243.0>},
                       {name,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.431Z,ns_1@127.0.0.1:ns_config_rep<0.244.0>:ns_config_rep:init:69]init pulling
[ns_server:debug,2019-07-04T11:55:58.431Z,ns_1@127.0.0.1:ns_config_rep<0.244.0>:ns_config_rep:init:71]init pushing
[ns_server:debug,2019-07-04T11:55:58.453Z,ns_1@127.0.0.1:ns_config_rep<0.244.0>:ns_config_rep:init:75]init reannouncing
[ns_server:debug,2019-07-04T11:55:58.453Z,ns_1@127.0.0.1:ns_config_events<0.155.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2019-07-04T11:55:58.453Z,ns_1@127.0.0.1:ns_config_events<0.155.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2019-07-04T11:55:58.453Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[6,
                                                                              0],
                                                                             {0,
                                                                              3890670214},
                                                                             true,
                                                                             [{"app",
                                                                               <<"5b6ce58456b7220bfb025f341cb20648">>}]}
[ns_server:debug,2019-07-04T11:55:58.454Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[ns_server:debug,2019-07-04T11:55:58.454Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[ns_server:debug,2019-07-04T11:55:58.458Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2019-07-04T11:55:58.458Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2019-07-04T11:55:58.458Z,ns_1@127.0.0.1:<0.253.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:55:58.459Z,ns_1@127.0.0.1:<0.253.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:55:58.460Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit_decriptors ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]}]
[ns_server:debug,2019-07-04T11:55:58.460Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]}]
[ns_server:debug,2019-07-04T11:55:58.461Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2019-07-04T11:55:58.461Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:55:58.454Z,ns_1@127.0.0.1:<0.252.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:55:58.467Z,ns_1@127.0.0.1:memcached_permissions<0.233.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-07-04T11:55:58.467Z,ns_1@127.0.0.1:<0.252.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}
[ns_server:debug,2019-07-04T11:55:58.469Z,ns_1@127.0.0.1:memcached_passwords<0.230.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-07-04T11:55:58.486Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.233.0>
[ns_server:debug,2019-07-04T11:55:58.489Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{7,63729460406}}],
 {configs,[[{map,[{0,[],['ns_1@127.0.0.1',undefined]},
                  {1,[],['ns_1@127.0.0.1',undefined]},
                  {2,[],['ns_1@127.0.0.1',undefined]},
                  {3,[],['ns_1@127.0.0.1',undefined]},
                  {4,[],['ns_1@127.0.0.1',undefined]},
                  {5,[],['ns_1@127.0.0.1',undefined]},
                  {6,[],['ns_1@127.0.0.1',undefined]},
                  {7,[],['ns_1@127.0.0.1',undefined]},
                  {8,[],['ns_1@127.0.0.1',undefined]},
                  {9,[],['ns_1@127.0.0.1',undefined]},
                  {10,[],['ns_1@127.0.0.1',undefined]},
                  {11,[],['ns_1@127.0.0.1',undefined]},
                  {12,[],['ns_1@127.0.0.1',undefined]},
                  {13,[],['ns_1@127.0.0.1',undefined]},
                  {14,[],['ns_1@127.0.0.1',undefined]},
                  {15,[],['ns_1@127.0.0.1',undefined]},
                  {16,[],['ns_1@127.0.0.1',undefined]},
                  {17,[],['ns_1@127.0.0.1',undefined]},
                  {18,[],['ns_1@127.0.0.1',undefined]},
                  {19,[],['ns_1@127.0.0.1',undefined]},
                  {20,[],['ns_1@127.0.0.1',undefined]},
                  {21,[],['ns_1@127.0.0.1',undefined]},
                  {22,[],['ns_1@127.0.0.1',undefined]},
                  {23,[],['ns_1@127.0.0.1',undefined]},
                  {24,[],['ns_1@127.0.0.1',undefined]},
                  {25,[],['ns_1@127.0.0.1',undefined]},
                  {26,[],['ns_1@127.0.0.1',undefined]},
                  {27,[],['ns_1@127.0.0.1',undefined]},
                  {28,[],['ns_1@127.0.0.1',undefined]},
                  {29,[],['ns_1@127.0.0.1',undefined]},
                  {30,[],['ns_1@127.0.0.1',undefined]},
                  {31,[],['ns_1@127.0.0.1',undefined]},
                  {32,[],['ns_1@127.0.0.1',undefined]},
                  {33,[],['ns_1@127.0.0.1',undefined]},
                  {34,[],['ns_1@127.0.0.1',undefined]},
                  {35,[],['ns_1@127.0.0.1',undefined]},
                  {36,[],['ns_1@127.0.0.1',undefined]},
                  {37,[],['ns_1@127.0.0.1',undefined]},
                  {38,[],['ns_1@127.0.0.1',undefined]},
                  {39,[],['ns_1@127.0.0.1',undefined]},
                  {40,[],['ns_1@127.0.0.1',undefined]},
                  {41,[],['ns_1@127.0.0.1',undefined]},
                  {42,[],['ns_1@127.0.0.1',undefined]},
                  {43,[],['ns_1@127.0.0.1',undefined]},
                  {44,[],['ns_1@127.0.0.1',undefined]},
                  {45,[],['ns_1@127.0.0.1',undefined]},
                  {46,[],['ns_1@127.0.0.1',undefined]},
                  {47,[],['ns_1@127.0.0.1',undefined]},
                  {48,[],['ns_1@127.0.0.1',undefined]},
                  {49,[],['ns_1@127.0.0.1',undefined]},
                  {50,[],['ns_1@127.0.0.1',undefined]},
                  {51,[],['ns_1@127.0.0.1',undefined]},
                  {52,[],['ns_1@127.0.0.1',undefined]},
                  {53,[],['ns_1@127.0.0.1',undefined]},
                  {54,[],['ns_1@127.0.0.1',undefined]},
                  {55,[],['ns_1@127.0.0.1',undefined]},
                  {56,[],['ns_1@127.0.0.1',undefined]},
                  {57,[],['ns_1@127.0.0.1',undefined]},
                  {58,[],['ns_1@127.0.0.1',undefined]},
                  {59,[],['ns_1@127.0.0.1',undefined]},
                  {60,[],['ns_1@127.0.0.1',undefined]},
                  {61,[],['ns_1@127.0.0.1',undefined]},
                  {62,[],['ns_1@127.0.0.1',undefined]},
                  {63,[],['ns_1@127.0.0.1',undefined]},
                  {64,[],['ns_1@127.0.0.1',undefined]},
                  {65,[],['ns_1@127.0.0.1',undefined]},
                  {66,[],['ns_1@127.0.0.1',undefined]},
                  {67,[],['ns_1@127.0.0.1',undefined]},
                  {68,[],['ns_1@127.0.0.1',undefined]},
                  {69,[],['ns_1@127.0.0.1',undefined]},
                  {70,[],['ns_1@127.0.0.1',undefined]},
                  {71,[],['ns_1@127.0.0.1',undefined]},
                  {72,[],['ns_1@127.0.0.1',undefined]},
                  {73,[],['ns_1@127.0.0.1',undefined]},
                  {74,[],['ns_1@127.0.0.1',undefined]},
                  {75,[],['ns_1@127.0.0.1',undefined]},
                  {76,[],['ns_1@127.0.0.1',undefined]},
                  {77,[],['ns_1@127.0.0.1',undefined]},
                  {78,[],['ns_1@127.0.0.1',undefined]},
                  {79,[],['ns_1@127.0.0.1',undefined]},
                  {80,[],['ns_1@127.0.0.1',undefined]},
                  {81,[],['ns_1@127.0.0.1',undefined]},
                  {82,[],['ns_1@127.0.0.1',undefined]},
                  {83,[],['ns_1@127.0.0.1',undefined]},
                  {84,[],['ns_1@127.0.0.1',undefined]},
                  {85,[],['ns_1@127.0.0.1',undefined]},
                  {86,[],['ns_1@127.0.0.1'|...]},
                  {87,[],[...]},
                  {88,[],...},
                  {89,...},
                  {...}|...]},
            {fastForwardMap,[]},
            {repl_type,dcp},
            {uuid,<<"5b6ce58456b7220bfb025f341cb20648">>},
            {auth_type,sasl},
            {num_replicas,1},
            {replica_index,false},
            {ram_quota,1042284544},
            {autocompaction,false},
            {purge_interval,undefined},
            {flush_enabled,false},
            {num_threads,3},
            {eviction_policy,value_only},
            {conflict_resolution_type,seqno},
            {storage_mode,couchstore},
            {max_ttl,0},
            {compression_mode,off},
            {type,membase},
            {num_vbuckets,1024},
            {replication_topology,star},
            {servers,['ns_1@127.0.0.1']},
            {sasl_password,"*****"},
            {map_opts_hash,133465355}]]}]
[ns_server:debug,2019-07-04T11:55:58.489Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cbas_memory_quota ->
1024
[ns_server:debug,2019-07-04T11:55:58.489Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
client_cert_auth ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2019-07-04T11:55:58.489Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cluster_compat_version ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{8,63729460354}}]},6,0]
[ns_server:debug,2019-07-04T11:55:58.489Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cluster_name ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
 97,112,112]
[ns_server:debug,2019-07-04T11:55:58.489Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2019-07-04T11:55:58.489Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
fts_memory_quota ->
256
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
max_bucket_count ->
10
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memcached ->
[]
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memory_quota ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|994]
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
nodes_wanted ->
['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
otp ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460353}}]},
 {cookie,{sanitized,<<"/05LSHzeZULfh9rXE9Vxot6RR/yAyBkjYVkPvwUz7ZE=">>}}]
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2019-07-04T11:55:58.490Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
quorum_nodes ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
read_only_user_creds ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
remote_clusters ->
[]
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest ->
[{port,8091}]
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest_creds ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"jkKoQCLgS0hwSaQwbjFuPTPff4IF8pnVfaYqo4gjzGRi9I4+EJ+qkWwBnlE0q7M4oewExLOJOdnRZy8S34iiWg==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"a0jO063Ch2WEsXWFP3cZ2iHE8JeSZPweQjKljT1T4r0=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"X1H9WHt5Rn/HyJ8kJ4/Z07G+iSc=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
roles_definitions ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 <<131,49,2,131,220,38,244,3,105,90,168,156>>]
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
secure_headers ->
[]
[ns_server:debug,2019-07-04T11:55:58.491Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2019-07-04T11:55:58.492Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2019-07-04T11:55:58.492Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
settings ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2019-07-04T11:55:58.492Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
users_upgrade ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:55:58.492Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
uuid ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]}|
 <<"070fee742e7318eb771177d58d7c8507">>]
[ns_server:debug,2019-07-04T11:55:58.495Z,ns_1@127.0.0.1:memcached_permissions<0.233.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:55:58.519Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
vbucket_map_history ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460406}}]},
 {[['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1'|...],
   [...]|...],
  [{replication_topology,star},{tags,undefined},{max_slaves,10}]}]
[ns_server:debug,2019-07-04T11:55:58.519Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2019-07-04T11:55:58.519Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{28,63729460426}}]}]
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
 <<"{\"uuid\":\"4704a37aa982ccf9\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\""...>>]
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-wanted/2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
 <<"{\"uuid\":\"10e09678eaa2bb72\",\"nodeDefs\":{\"2a8ae539a5cab1af4159b9b57e0098ee\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"2a8ae539a5cab1af4159b9b57e0098ee\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"features\\\":\\\"leanPlan,indexType:scorch,indexType:upside_down\\\",\\\"nsHostPort\\\":\\\"127.0.0.1:8091\\\",\\\"version-cbft.app\\\":\\\""...>>]
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/fts/cbgt/cfg/version">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460388}}]}|
 <<"5.5.0">>]
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/8479338376212444691/0">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460426}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460389}}]}|
 <<"{\"Version\":3}">>]
[ns_server:debug,2019-07-04T11:55:58.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:55:58.521Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{3,63729460385}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2019-07-04T11:55:58.521Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>]
[ns_server:debug,2019-07-04T11:55:58.521Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{rbac_upgrade,[5,5]} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]}|
 '_deleted']
[ns_server:debug,2019-07-04T11:55:58.521Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2019-07-04T11:55:58.521Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2019-07-04T11:55:58.521Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,fts} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:55:58.521Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,index} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,n1ql} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{2,63729460389}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8092]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9110]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9113]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9112]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9111]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9115]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9114]
[ns_server:debug,2019-07-04T11:55:58.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9116]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|-1]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8095]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9118]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9119]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9121]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9122]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9120]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9117]
[ns_server:debug,2019-07-04T11:55:58.523Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 {5,5,3}]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9140]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8096]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8094]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9100]
[ns_server:debug,2019-07-04T11:55:58.524Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9102]
[ns_server:debug,2019-07-04T11:55:58.525Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:55:58.525Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9101]
[ns_server:debug,2019-07-04T11:55:58.535Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9104]
[ns_server:debug,2019-07-04T11:55:58.555Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9103]
[ns_server:debug,2019-07-04T11:55:58.555Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9105]
[ns_server:debug,2019-07-04T11:55:58.556Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|false]
[ns_server:debug,2019-07-04T11:55:58.556Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2019-07-04T11:55:58.556Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ldap_enabled} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|true]
[ns_server:debug,2019-07-04T11:55:58.556Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 active]
[ns_server:debug,2019-07-04T11:55:58.557Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {port,11210},
 {dedicated_port,11209},
 {ssl_port,undefined},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2019-07-04T11:55:58.558Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {maxconn,maxconn},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {maxconn,dedicated_port_maxconn},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {maxconn,maxconn},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]}]}},
   {ssl_cipher_list,{"~s",[ssl_cipher_list]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,{memcached_config_mgr,is_enabled,[[5,0]]}},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}}]}]
[ns_server:debug,2019-07-04T11:55:58.558Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {maxconn,30000},
 {dedicated_port_maxconn,5000},
 {ssl_cipher_list,"HIGH"},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,false},
 {datatype_snappy,true}]
[ns_server:debug,2019-07-04T11:55:58.558Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {port,11211},
 {verbosity,[]}]
[ns_server:debug,2019-07-04T11:55:58.558Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2019-07-04T11:55:58.558Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}]
[ns_server:debug,2019-07-04T11:55:58.558Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9999]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|8093]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',services} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460385}}]},
 fts,index,kv,n1ql]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 undefined]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|
 <<"2a8ae539a5cab1af4159b9b57e0098ee">>]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|9998]
[ns_server:debug,2019-07-04T11:55:58.559Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460348}}]}|false]
[ns_server:debug,2019-07-04T11:55:58.561Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.233.0>
[ns_server:debug,2019-07-04T11:55:58.565Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:warn,2019-07-04T11:55:58.598Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:58.598Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:55:58.603Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.244.0>},
                       {name,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.603Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.237.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:58.603Z,ns_1@127.0.0.1:ns_config_rep<0.244.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([alert_limits,audit,audit_decriptors,
                               auto_failover_cfg,auto_reprovision_cfg,
                               autocompaction,buckets,cbas_memory_quota,
                               client_cert_auth,cluster_compat_version,
                               cluster_name,drop_request_memory_threshold_mib,
                               email_alerts,fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,
                               read_only_user_creds,remote_clusters,
                               replication,rest,rest_creds,roles_definitions,
                               scramsha_fallback_salt,secure_headers,
                               server_groups,set_view_update_daemon,settings,
                               users_upgrade,uuid,vbucket_map_history,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-known/2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-wanted/2a8ae539a5cab1af4159b9b57e0098ee">>},
                               {metakv,<<"/fts/cbgt/cfg/version">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/8479338376212444691/0">>},
                               {metakv,<<"/indexing/info/versionToken">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {rbac_upgrade,[5,5]},
                               {request_limit,capi},
                               {request_limit,rest},
                               {service_map,fts},
                               {service_map,index},
                               {service_map,n1ql},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port}]..)
[ns_server:debug,2019-07-04T11:55:58.604Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.230.0>
[ns_server:debug,2019-07-04T11:55:58.604Z,ns_1@127.0.0.1:memcached_passwords<0.230.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-07-04T11:55:58.615Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.230.0>
[error_logger:info,2019-07-04T11:55:58.619Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.259.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.625Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2019-07-04T11:55:58.645Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:58.645Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:55:58.645Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.262.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.645Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.265.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.647Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.266.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.671Z,ns_1@127.0.0.1:ns_log_events<0.236.0>:ns_mail_log:init:44]ns_mail_log started up
[error_logger:info,2019-07-04T11:55:58.671Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_mail_sup}
             started: [{pid,<0.268.0>},
                       {name,ns_mail_log},
                       {mfargs,{ns_mail_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.672Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.267.0>},
                       {name,ns_mail_sup},
                       {mfargs,{ns_mail_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:58.672Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.269.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.705Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.270.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.737Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.272.0>},
                       {name,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.739Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.276.0>},
                       {name,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.740Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.271.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:58.741Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2019-07-04T11:55:58.742Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[error_logger:info,2019-07-04T11:55:58.765Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.281.0>},
                       {name,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.838Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.282.0>},
                       {name,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.838Z,ns_1@127.0.0.1:<0.279.0>:restartable:start_child:98]Started child process <0.280.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2019-07-04T11:55:58.838Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.279.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:58.838Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.285.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.855Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.286.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.855Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.287.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.855Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.288.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.856Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.289.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:58.909Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:grab_index_status:380]ignoring failure to get index status: {exit,
                                       {noproc,
                                        {gen_server,call,
                                         ['service_status_keeper-index',
                                          get_status,2000]}}}
[{gen_server,call,3,[{file,"gen_server.erl"},{line,188}]},
 {ns_heart,grab_index_status,0,[{file,"src/ns_heart.erl"},{line,377}]},
 {ns_heart,current_status_slow_inner,0,[{file,"src/ns_heart.erl"},{line,286}]},
 {ns_heart,current_status_slow,1,[{file,"src/ns_heart.erl"},{line,250}]},
 {ns_heart,update_current_status,1,[{file,"src/ns_heart.erl"},{line,187}]},
 {ns_heart,handle_info,2,[{file,"src/ns_heart.erl"},{line,118}]},
 {gen_server,handle_msg,5,[{file,"gen_server.erl"},{line,604}]},
 {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,239}]}]
[ns_server:debug,2019-07-04T11:55:58.909Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "app" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-app-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-fun-1-',3,
                           [{file,"src/ns_heart.erl"},{line,292}]},
                 {lists,foldl,3,[{file,"lists.erl"},{line,1248}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,291}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[error_logger:info,2019-07-04T11:55:58.914Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.292.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.932Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.294.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.933Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.296.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.986Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.298.0>},
                       {name,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.986Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.300.0>},
                       {name,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:58.994Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.302.0>},
                       {name,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.022Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.303.0>},
                       {name,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.055Z,ns_1@127.0.0.1:ns_heart<0.272.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2019-07-04T11:55:59.057Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.304.0>},
                       {name,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.059Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.305.0>},
                       {name,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.077Z,ns_1@127.0.0.1:ns_heart<0.272.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:45]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:info,2019-07-04T11:55:59.093Z,ns_1@127.0.0.1:menelaus_sup<0.297.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts

[ns_server:info,2019-07-04T11:55:59.093Z,ns_1@127.0.0.1:menelaus_sup<0.297.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql

[ns_server:error,2019-07-04T11:55:59.116Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:grab_one_service_status:498]Failed to grab service fts status: {exit,
                                    {noproc,
                                     {gen_server,call,
                                      ['service_agent-fts',get_status,2000]}},
                                    [{gen_server,call,3,
                                      [{file,"gen_server.erl"},{line,188}]},
                                     {ns_heart,grab_one_service_status,1,
                                      [{file,"src/ns_heart.erl"},{line,495}]},
                                     {ns_heart,
                                      '-grab_service_statuses/0-lc$^1/1-1-',1,
                                      [{file,"src/ns_heart.erl"},{line,491}]},
                                     {ns_heart,current_status_slow_inner,0,
                                      [{file,"src/ns_heart.erl"},{line,354}]},
                                     {ns_heart,current_status_slow,1,
                                      [{file,"src/ns_heart.erl"},{line,250}]},
                                     {ns_heart,update_current_status,1,
                                      [{file,"src/ns_heart.erl"},{line,187}]},
                                     {ns_heart,handle_info,2,
                                      [{file,"src/ns_heart.erl"},{line,118}]},
                                     {gen_server,handle_msg,5,
                                      [{file,"gen_server.erl"},{line,604}]}]}
[ns_server:error,2019-07-04T11:55:59.117Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:grab_one_service_status:498]Failed to grab service index status: {exit,
                                      {noproc,
                                       {gen_server,call,
                                        ['service_agent-index',get_status,
                                         2000]}},
                                      [{gen_server,call,3,
                                        [{file,"gen_server.erl"},{line,188}]},
                                       {ns_heart,grab_one_service_status,1,
                                        [{file,"src/ns_heart.erl"},
                                         {line,495}]},
                                       {ns_heart,
                                        '-grab_service_statuses/0-lc$^1/1-1-',
                                        1,
                                        [{file,"src/ns_heart.erl"},
                                         {line,491}]},
                                       {ns_heart,
                                        '-grab_service_statuses/0-lc$^1/1-1-',
                                        1,
                                        [{file,"src/ns_heart.erl"},
                                         {line,491}]},
                                       {ns_heart,current_status_slow_inner,0,
                                        [{file,"src/ns_heart.erl"},
                                         {line,354}]},
                                       {ns_heart,current_status_slow,1,
                                        [{file,"src/ns_heart.erl"},
                                         {line,250}]},
                                       {ns_heart,update_current_status,1,
                                        [{file,"src/ns_heart.erl"},
                                         {line,187}]},
                                       {ns_heart,handle_info,2,
                                        [{file,"src/ns_heart.erl"},
                                         {line,118}]}]}
[error_logger:info,2019-07-04T11:55:59.208Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.307.0>},
                       {name,menelaus_web},
                       {mfargs,{menelaus_web,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.211Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.326.0>},
                       {name,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.214Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.327.0>},
                       {name,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.227Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.328.0>},
                       {name,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.232Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.329.0>},
                       {name,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2019-07-04T11:55:59.233Z,ns_1@127.0.0.1:ns_server_sup<0.223.0>:menelaus_sup:start_link:46]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.0.0-1693-community".
[error_logger:info,2019-07-04T11:55:59.233Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.297.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:59.233Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.335.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.234Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[error_logger:info,2019-07-04T11:55:59.247Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.339.0>},
                       {name,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:59.248Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.340.0>},
                       {name,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.31986353>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.248Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.338.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:59.249Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.342.0>},
                       {name,{service_agent,fts}},
                       {mfargs,{service_agent,start_link,[fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.249Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.346.0>},
                       {name,{service_agent,index}},
                       {mfargs,{service_agent,start_link,[index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:59.257Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: working as port

[ns_server:debug,2019-07-04T11:55:59.326Z,ns_1@127.0.0.1:ns_ports_setup<0.335.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,projector,indexer,query,saslauthd_port,goxdcr,fts]
[error_logger:info,2019-07-04T11:55:59.344Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.350.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.346Z,ns_1@127.0.0.1:ns_audit_cfg<0.351.0>:ns_audit_cfg:write_audit_json:265]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json" : [{descriptors_path,
                                                                                <<"/opt/couchbase/etc/security">>},
                                                                               {version,
                                                                                2},
                                                                               {uuid,
                                                                                <<"18411111">>},
                                                                               {event_states,
                                                                                {[]}},
                                                                               {filtering_enabled,
                                                                                true},
                                                                               {disabled_userids,
                                                                                []},
                                                                               {auditd_enabled,
                                                                                false},
                                                                               {log_path,
                                                                                <<"/opt/couchbase/var/lib/couchbase/logs">>},
                                                                               {rotate_interval,
                                                                                86400},
                                                                               {rotate_size,
                                                                                20971520},
                                                                               {sync,
                                                                                []}]
[ns_server:debug,2019-07-04T11:55:59.360Z,ns_1@127.0.0.1:ns_audit_cfg<0.351.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2019-07-04T11:55:59.361Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.351.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:55:59.363Z,ns_1@127.0.0.1:<0.354.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:55:59.370Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,239}]}]}}

[ns_server:debug,2019-07-04T11:55:59.371Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2019-07-04T11:55:59.371Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:ns_heart:grab_index_status:380]ignoring failure to get index status: {exit,
                                       {noproc,
                                        {gen_server,call,
                                         ['service_status_keeper-index',
                                          get_status,2000]}}}
[{gen_server,call,3,[{file,"gen_server.erl"},{line,188}]},
 {ns_heart,grab_index_status,0,[{file,"src/ns_heart.erl"},{line,377}]},
 {ns_heart,current_status_slow_inner,0,[{file,"src/ns_heart.erl"},{line,286}]},
 {ns_heart,current_status_slow,1,[{file,"src/ns_heart.erl"},{line,250}]},
 {ns_heart,slow_updater_loop,0,[{file,"src/ns_heart.erl"},{line,244}]},
 {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,239}]}]
[ns_server:debug,2019-07-04T11:55:59.372Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "app" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-app-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-fun-1-',3,
                           [{file,"src/ns_heart.erl"},{line,292}]},
                 {lists,foldl,3,[{file,"lists.erl"},{line,1248}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,291}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[error_logger:info,2019-07-04T11:55:59.372Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.355.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.375Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:55:59.375Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:45]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:debug,2019-07-04T11:55:59.376Z,ns_1@127.0.0.1:memcached_config_mgr<0.368.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-07-04T11:55:59.376Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.368.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:55:59.383Z,ns_1@127.0.0.1:<0.370.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2019-07-04T11:55:59.383Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.370.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.397Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.371.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:55:59.399Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:59.399Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:55:59.405Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.373.0>},
                       {name,ns_bucket_worker},
                       {mfargs,{work_queue,start_link,[ns_bucket_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:55:59.427Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:59.427Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:55:59.440Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.376.0>},
                       {name,buckets_observing_subscription},
                       {mfargs,{ns_bucket_sup,subscribe_on_config_events,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.440Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.374.0>},
                       {name,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:59.441Z,ns_1@127.0.0.1:ns_bucket_worker<0.373.0>:ns_bucket_sup:update_children:108]Starting new child: {{single_bucket_kv_sup,"app"},
                     {single_bucket_kv_sup,start_link,["app"]},
                     permanent,infinity,supervisor,
                     [single_bucket_kv_sup]}

[error_logger:info,2019-07-04T11:55:59.441Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.372.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:59.446Z,ns_1@127.0.0.1:single_bucket_kv_sup-app<0.378.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:76]Syncing config to couchdb node
[ns_server:debug,2019-07-04T11:55:59.468Z,ns_1@127.0.0.1:single_bucket_kv_sup-app<0.378.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:81]Synced config to couchdb node successfully
[error_logger:info,2019-07-04T11:55:59.469Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.380.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.474Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.390.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.505Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.393.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.522Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.394.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.522Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.396.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.397.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.399.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.563Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.400.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:55:59.564Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.401.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-07-04T11:55:59.564Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.400.0>},
                       {name,doc_replicator},
                       {mfargs,{capi_ddoc_manager,start_replicator,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.565Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.401.0>},
                       {name,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.582Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.402.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.588Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.404.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.589Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.406.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.599Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.407.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.603Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.409.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.616Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.400.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.236.0>
[ns_server:debug,2019-07-04T11:55:59.616Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.401.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.236.0>
[error_logger:info,2019-07-04T11:55:59.615Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.235.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.617Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.236.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["app",<0.400.0>,<0.401.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.617Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.234.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:59.619Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.409.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:warn,2019-07-04T11:55:59.620Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:59.620Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:55:59.621Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.409.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2019-07-04T11:55:59.638Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.414.0>},
                       {name,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:59.652Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.416.0>},
                       {name,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.653Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.417.0>},
                       {name,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:55:59.655Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:55:59.656Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:error,2019-07-04T11:55:59.665Z,ns_1@127.0.0.1:service_status_keeper_worker<0.416.0>:rest_utils:get_json_local:63]Request to (indexer) getIndexStatus failed: {error,
                                             {econnrefused,
                                              [{lhttpc_client,send_request,1,
                                                [{file,
                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                 {line,220}]},
                                               {lhttpc_client,execute,9,
                                                [{file,
                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                 {line,169}]},
                                               {lhttpc_client,request,9,
                                                [{file,
                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                 {line,92}]}]}}
[ns_server:error,2019-07-04T11:55:59.666Z,ns_1@127.0.0.1:service_status_keeper-index<0.417.0>:service_status_keeper:handle_cast:119]Service service_index returned incorrect status
[error_logger:info,2019-07-04T11:55:59.668Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.421.0>},
                       {name,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:error,2019-07-04T11:55:59.682Z,ns_1@127.0.0.1:service_status_keeper_worker<0.416.0>:rest_utils:get_json_local:63]Request to (fts) api/nsstatus failed: {error,
                                       {econnrefused,
                                        [{lhttpc_client,send_request,1,
                                          [{file,
                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                           {line,220}]},
                                         {lhttpc_client,execute,9,
                                          [{file,
                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                           {line,169}]},
                                         {lhttpc_client,request,9,
                                          [{file,
                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                           {line,92}]}]}}
[ns_server:error,2019-07-04T11:55:59.683Z,ns_1@127.0.0.1:service_status_keeper-fts<0.421.0>:service_status_keeper:handle_cast:119]Service service_fts returned incorrect status
[error_logger:info,2019-07-04T11:55:59.701Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.425.0>},
                       {name,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.702Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.415.0>},
                       {name,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:59.703Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.428.0>},
                       {name,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.41280346>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.705Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.413.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:59.730Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.238.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.791Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.430.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.794Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[error_logger:info,2019-07-04T11:55:59.794Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.431.0>},
                       {name,{service_fts,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,[service_fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.810Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[error_logger:info,2019-07-04T11:55:59.810Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.434.0>},
                       {name,{service_index,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.829Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.437.0>},
                       {name,{service_fts,stats_archiver,"@fts"}},
                       {mfargs,{stats_archiver,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.835Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.439.0>},
                       {name,{service_fts,stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["@fts-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.836Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.441.0>},
                       {name,{service_fts,stats_reader,"@fts"}},
                       {mfargs,{stats_reader,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.836Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.442.0>},
                       {name,{service_fts,stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["@fts-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.842Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.443.0>},
                       {name,{service_index,stats_archiver,"@index"}},
                       {mfargs,{stats_archiver,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.873Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.248.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.873Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.392.0>},
                       {name,{docs_kv_sup,"app"}},
                       {mfargs,{docs_kv_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:55:59.875Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.445.0>},
                       {name,{service_index,stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["@index-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.876Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.449.0>},
                       {name,{service_index,stats_reader,"@index"}},
                       {mfargs,{stats_reader,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.876Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.450.0>},
                       {name,{service_index,stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["@index-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.882Z,ns_1@127.0.0.1:ns_memcached-app<0.452.0>:ns_memcached:init:158]Starting ns_memcached
[ns_server:debug,2019-07-04T11:55:59.882Z,ns_1@127.0.0.1:<0.453.0>:ns_memcached:run_connect_phase:181]Started 'connecting' phase of ns_memcached-app. Parent is <0.452.0>
[error_logger:info,2019-07-04T11:55:59.882Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.451.0>,ns_memcached_sup}
             started: [{pid,<0.452.0>},
                       {name,{ns_memcached,"app"}},
                       {mfargs,{ns_memcached,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:55:59.885Z,ns_1@127.0.0.1:<0.453.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:55:59.922Z,ns_1@127.0.0.1:<0.454.0>:new_concurrency_throttle:init:113]init concurrent throttle process, pid: <0.454.0>, type: kv_throttle# of available token: 1
[error_logger:info,2019-07-04T11:55:59.941Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.451.0>,ns_memcached_sup}
             started: [{pid,<0.455.0>},
                       {name,{terse_bucket_info_uploader,"app"}},
                       {mfargs,
                           {terse_bucket_info_uploader,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:55:59.942Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.451.0>},
                       {name,{ns_memcached_sup,"app"}},
                       {mfargs,{ns_memcached_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:55:59.969Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[error_logger:info,2019-07-04T11:55:59.969Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.446.0>},
                       {name,compaction_new_daemon},
                       {mfargs,{compaction_new_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:55:59.971Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:55:59.971Z,ns_1@127.0.0.1:<0.457.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:55:59.972Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_master) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:55:59.972Z,ns_1@127.0.0.1:<0.461.0>:compaction_new_daemon:spawn_master_db_compactor:849]Start compaction of master db for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:info,2019-07-04T11:55:59.986Z,ns_1@127.0.0.1:<0.460.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:55:59.997Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.400.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2019-07-04T11:55:59.997Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:55:59.997Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2019-07-04T11:56:00.022Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.462.0>},
                       {name,{dcp_sup,"app"}},
                       {mfargs,{dcp_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:00.022Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.463.0>},
                       {name,{dcp_replication_manager,"app"}},
                       {mfargs,{dcp_replication_manager,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.023Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.464.0>},
                       {name,{replication_manager,"app"}},
                       {mfargs,{replication_manager,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.059Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.468.0>},
                       {name,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.060Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.467.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:00.082Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:56:00.082Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 29s
[error_logger:info,2019-07-04T11:56:00.149Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-app'}
             started: [{pid,<0.471.0>},
                       {name,rebalance_subprocesses_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               ['rebalance_subprocesses_registry-app',
                                [{terminate_command,kill}]]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:56:00.168Z,ns_1@127.0.0.1:janitor_agent-app<0.472.0>:janitor_agent:read_flush_counter:918]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2019-07-04T11:56:00.168Z,ns_1@127.0.0.1:janitor_agent-app<0.472.0>:janitor_agent:read_flush_counter_from_config:925]Initialized flushseq 0 from bucket config
[error_logger:info,2019-07-04T11:56:00.168Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-app'}
             started: [{pid,<0.472.0>},
                       {name,janitor_agent},
                       {mfargs,{janitor_agent,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.168Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.470.0>},
                       {name,{janitor_agent_sup,"app"}},
                       {mfargs,{janitor_agent_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.192Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.473.0>},
                       {name,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.326Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.475.0>},
                       {name,{stats_collector,"app"}},
                       {mfargs,{stats_collector,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.354Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.478.0>},
                       {name,{stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.356Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.480.0>},
                       {name,{stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:00.371Z,ns_1@127.0.0.1:<0.354.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:56:00.435Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:00.435Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:00.437Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:00.438Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:56:00.538Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.483.0>},
                       {name,{goxdcr_stats_collector,"app"}},
                       {mfargs,{goxdcr_stats_collector,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.561Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.485.0>},
                       {name,{goxdcr_stats_archiver,"app"}},
                       {mfargs,{stats_archiver,start_link,["@xdcr-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.562Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.487.0>},
                       {name,{goxdcr_stats_reader,"app"}},
                       {mfargs,{stats_reader,start_link,["@xdcr-app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.563Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-app'}
             started: [{pid,<0.488.0>},
                       {name,{failover_safeness_level,"app"}},
                       {mfargs,{failover_safeness_level,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.563Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.378.0>},
                       {name,{single_bucket_kv_sup,"app"}},
                       {mfargs,{single_bucket_kv_sup,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:00.610Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.489.0>},
                       {name,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:00.623Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:00.624Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:00.629Z,ns_1@127.0.0.1:leader_lease_agent<0.490.0>:leader_lease_agent:maybe_recover_persisted_lease:397]Found persisted lease [{node,'ns_1@127.0.0.1'},
                       {uuid,<<"2e0765d644fd2a31b0632a30fdfa32a4">>},
                       {time_left,15000},
                       {status,active}]
[error_logger:info,2019-07-04T11:56:00.629Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.490.0>},
                       {name,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.629Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.482.0>},
                       {name,leader_leases_sup},
                       {mfargs,
                           {leader_services_sup,start_link,
                               [leader_leases_sup]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:00.629Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.492.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:00.676Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.493.0>},
                       {name,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:00.679Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:00.679Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:00.806Z,ns_1@127.0.0.1:leader_registry_sup<0.491.0>:mb_master:check_master_takeover_needed:133]Sending master node question to the following nodes: []
[ns_server:debug,2019-07-04T11:56:00.806Z,ns_1@127.0.0.1:leader_registry_sup<0.491.0>:mb_master:check_master_takeover_needed:135]Got replies: []
[ns_server:debug,2019-07-04T11:56:00.806Z,ns_1@127.0.0.1:leader_registry_sup<0.491.0>:mb_master:check_master_takeover_needed:141]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2019-07-04T11:56:00.815Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:56:00.816Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[user:info,2019-07-04T11:56:00.816Z,ns_1@127.0.0.1:mb_master<0.496.0>:mb_master:init:86]I'm the only node, so I'm the master.
[ns_server:debug,2019-07-04T11:56:00.816Z,ns_1@127.0.0.1:leader_registry<0.493.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[error_logger:info,2019-07-04T11:56:00.896Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.499.0>},
                       {name,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:00.897Z,ns_1@127.0.0.1:<0.453.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:56:00.907Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.501.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[error_logger:info,2019-07-04T11:56:00.907Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.501.0>},
                       {name,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:00.908Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.501.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:warn,2019-07-04T11:56:00.935Z,ns_1@127.0.0.1:<0.508.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@127.0.0.1',
                                                                            <<"2e0765d644fd2a31b0632a30fdfa32a4">>} (valid for 14694ms)
[ns_server:info,2019-07-04T11:56:01.038Z,ns_1@127.0.0.1:mb_master_sup<0.498.0>:misc:start_singleton:756]start_singleton(gen_server, ns_tick, [], []): started as <0.511.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-07-04T11:56:01.038Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.511.0>},
                       {name,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.115Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.515.0>},
                       {name,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:56:01.134Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.513.0>:misc:start_singleton:756]start_singleton(gen_server, auto_reprovision, [], []): started as <0.516.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-07-04T11:56:01.134Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.516.0>},
                       {name,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-07-04T11:56:01.147Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.513.0>:misc:start_singleton:756]start_singleton(gen_fsm, ns_orchestrator, [], []): started as <0.517.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-07-04T11:56:01.150Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.517.0>},
                       {name,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.151Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.513.0>},
                       {name,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:01.159Z,ns_1@127.0.0.1:<0.519.0>:auto_failover:init:211]init auto_failover.
[user:info,2019-07-04T11:56:01.159Z,ns_1@127.0.0.1:<0.519.0>:auto_failover:handle_call:242]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2019-07-04T11:56:01.249Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{29,63729460561}}]}]
[ns_server:info,2019-07-04T11:56:01.250Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.512.0>:misc:start_singleton:756]start_singleton(gen_server, auto_failover, [], []): started as <0.519.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2019-07-04T11:56:01.250Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{1,63729460354}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]}]
[error_logger:info,2019-07-04T11:56:01.250Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.519.0>},
                       {name,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:01.250Z,ns_1@127.0.0.1:ns_config_rep<0.244.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[error_logger:info,2019-07-04T11:56:01.251Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.512.0>},
                       {name,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:01.251Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.496.0>},
                       {name,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:01.251Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.491.0>},
                       {name,leader_registry_sup},
                       {mfargs,
                           {leader_services_sup,start_link,
                               [leader_registry_sup]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:01.256Z,ns_1@127.0.0.1:<0.474.0>:restartable:start_child:98]Started child process <0.481.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2019-07-04T11:56:01.256Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.474.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:01.256Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.529.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.256Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.530.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.377Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.531.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:01.378Z,ns_1@127.0.0.1:<0.354.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2019-07-04T11:56:01.427Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.532.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:01.442Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:01.442Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:01.445Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:01.445Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-07-04T11:56:01.494Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.535.0>},
                       {name,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.495Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.537.0>},
                       {name,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:01.555Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.546.0>},
                       {name,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.563Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.548.0>},
                       {name,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.602Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.550.0>},
                       {name,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:01.603Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[error_logger:info,2019-07-04T11:56:01.604Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.538.0>},
                       {name,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.81875396>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.608Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.558.0>},
                       {name,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.612Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.566.0>},
                       {name,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:01.612Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.534.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:01.612Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[error_logger:info,2019-07-04T11:56:01.612Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.223.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:01.613Z,ns_1@127.0.0.1:menelaus_barrier<0.171.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.169.0>
[ns_server:debug,2019-07-04T11:56:01.613Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2019-07-04T11:56:01.613Z,ns_1@127.0.0.1:<0.168.0>:restartable:start_child:98]Started child process <0.169.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2019-07-04T11:56:01.613Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.168.0>},
                       {name,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:01.614Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2019-07-04T11:56:01.614Z,ns_1@127.0.0.1:<0.2.0>:child_erlang:child_loop:130]135: Entered child_loop
[ns_server:warn,2019-07-04T11:56:01.670Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:01.670Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:01.681Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:01.681Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:01.706Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:debug,2019-07-04T11:56:01.819Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:56:01.818Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:56:01.895Z,ns_1@127.0.0.1:json_rpc_connection-projector-cbauth<0.569.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.569.0>
[ns_server:debug,2019-07-04T11:56:01.896Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"projector-cbauth",<0.569.0>} started
[ns_server:warn,2019-07-04T11:56:01.899Z,ns_1@127.0.0.1:<0.453.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:56:02.000Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2019-07-04T11:56:02.263Z,ns_1@127.0.0.1:<0.519.0>:auto_failover:log_down_nodes_reason:382]Node 'ns_1@127.0.0.1' is considered down. Reason:"The data service did not respond for the duration of the auto-failover threshold. Either none of the buckets have warmed up or there is an issue with the data service. "
[ns_server:debug,2019-07-04T11:56:02.264Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            0,new,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              0,half_down,false}
[ns_server:warn,2019-07-04T11:56:02.311Z,ns_1@127.0.0.1:<0.579.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:error,2019-07-04T11:56:02.311Z,ns_1@127.0.0.1:query_stats_collector<0.402.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2019-07-04T11:56:02.311Z,ns_1@127.0.0.1:<0.483.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:warn,2019-07-04T11:56:02.477Z,ns_1@127.0.0.1:<0.354.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:56:02.478Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:02.479Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:02.483Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:02.483Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:02.607Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:02.665Z,ns_1@127.0.0.1:json_rpc_connection-index-cbauth<0.600.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-cbauth", handling process <0.600.0>
[ns_server:debug,2019-07-04T11:56:02.665Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"index-cbauth",<0.600.0>} started
[ns_server:warn,2019-07-04T11:56:02.782Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:02.782Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:02.786Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:02.786Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:02.839Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:56:02.839Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:56:02.886Z,ns_1@127.0.0.1:ns_ports_setup<0.335.0>:ns_ports_setup:set_children:90]Monitor ns_child_ports_sup <12395.72.0>
[ns_server:debug,2019-07-04T11:56:02.886Z,ns_1@127.0.0.1:memcached_config_mgr<0.368.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:warn,2019-07-04T11:56:02.963Z,ns_1@127.0.0.1:<0.453.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:56:03.007Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2019-07-04T11:56:03.015Z,ns_1@127.0.0.1:memcached_config_mgr<0.368.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:error,2019-07-04T11:56:03.072Z,ns_1@127.0.0.1:query_stats_collector<0.402.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2019-07-04T11:56:03.076Z,ns_1@127.0.0.1:<0.483.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:56:03.164Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            0,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              1,half_down,false}
[ns_server:debug,2019-07-04T11:56:03.170Z,ns_1@127.0.0.1:memcached_config_mgr<0.368.0>:memcached_config_mgr:init:79]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2019-07-04T11:56:03.171Z,ns_1@127.0.0.1:memcached_config_mgr<0.368.0>:memcached_config_mgr:init:82]activated memcached port server
[ns_server:debug,2019-07-04T11:56:03.403Z,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.619.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.619.0>
[ns_server:warn,2019-07-04T11:56:03.404Z,ns_1@127.0.0.1:<0.579.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:56:03.483Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:03.483Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:03.483Z,ns_1@127.0.0.1:<0.354.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:03.484Z,ns_1@127.0.0.1:<0.352.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.351.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_audit_cfg,
                                                                                  notify_memcached,
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_audit_cfg.erl"},
                                                                                   {line,
                                                                                    171}]},
                                                                                 {ns_audit_cfg,
                                                                                  handle_info,
                                                                                  2,
                                                                                  [{file,
                                                                                    "src/ns_audit_cfg.erl"},
                                                                                   {line,
                                                                                    145}]},
                                                                                 {gen_server,
                                                                                  handle_msg,
                                                                                  5,
                                                                                  [{file,
                                                                                    "gen_server.erl"},
                                                                                   {line,
                                                                                    604}]},
                                                                                 {proc_lib,
                                                                                  init_p_do_apply,
                                                                                  3,
                                                                                  [{file,
                                                                                    "proc_lib.erl"},
                                                                                   {line,
                                                                                    239}]}]}
[error_logger:error,2019-07-04T11:56:03.484Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server ns_audit_cfg terminating 
** Last message in was notify_memcached
** When Server state == {state,
                            [{auditd_enabled,false},
                             {disabled,[]},
                             {disabled_users,[]},
                             {enabled,[]},
                             {log_path,
                                 "/opt/couchbase/var/lib/couchbase/logs"},
                             {rotate_interval,86400},
                             {rotate_size,20971520},
                             {sync,[]}],
                            [{uuid,"18411111"},
                             {event_states,{[]}},
                             {filtering_enabled,true},
                             {disabled_userids,[]},
                             {auditd_enabled,false},
                             {disabled,[]},
                             {disabled_users,[]},
                             {enabled,[]},
                             {log_path,
                                 "/opt/couchbase/var/lib/couchbase/logs"},
                             {rotate_interval,86400},
                             {rotate_size,20971520},
                             {sync,[]}]}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_audit_cfg,notify_memcached,1,
                   [{file,"src/ns_audit_cfg.erl"},{line,171}]},
     {ns_audit_cfg,handle_info,2,[{file,"src/ns_audit_cfg.erl"},{line,145}]},
     {gen_server,handle_msg,5,[{file,"gen_server.erl"},{line,604}]},
     {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,239}]}]}

[ns_server:debug,2019-07-04T11:56:03.492Z,ns_1@127.0.0.1:ns_audit_cfg<0.621.0>:ns_audit_cfg:write_audit_json:265]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json" : [{descriptors_path,
                                                                                <<"/opt/couchbase/etc/security">>},
                                                                               {version,
                                                                                2},
                                                                               {uuid,
                                                                                <<"18411111">>},
                                                                               {event_states,
                                                                                {[]}},
                                                                               {filtering_enabled,
                                                                                true},
                                                                               {disabled_userids,
                                                                                []},
                                                                               {auditd_enabled,
                                                                                false},
                                                                               {log_path,
                                                                                <<"/opt/couchbase/var/lib/couchbase/logs">>},
                                                                               {rotate_interval,
                                                                                86400},
                                                                               {rotate_size,
                                                                                20971520},
                                                                               {sync,
                                                                                []}]
[ns_server:error,2019-07-04T11:56:03.484Z,ns_1@127.0.0.1:<0.318.0>:menelaus_web:loop:143]Server error during processing: ["web request failed",
                                 {path,"/pools/default"},
                                 {method,'GET'},
                                 {type,exit},
                                 {what,
                                  {{{{badmatch,
                                      {error,couldnt_connect_to_memcached}},
                                     [{ns_audit_cfg,notify_memcached,1,
                                       [{file,"src/ns_audit_cfg.erl"},
                                        {line,171}]},
                                      {ns_audit_cfg,handle_info,2,
                                       [{file,"src/ns_audit_cfg.erl"},
                                        {line,145}]},
                                      {gen_server,handle_msg,5,
                                       [{file,"gen_server.erl"},{line,604}]},
                                      {proc_lib,init_p_do_apply,3,
                                       [{file,"proc_lib.erl"},{line,239}]}]},
                                    {gen_server,call,[ns_audit_cfg,get_uid]}},
                                   {gen_server,call,
                                    [<0.303.0>,
                                     #Fun<menelaus_web_cache.2.13672906>,
                                     infinity]}}},
                                 {trace,
                                  [{gen_server,call,3,
                                    [{file,"gen_server.erl"},{line,188}]},
                                   {menelaus_web_pools,handle_pool_info,2,
                                    [{file,"src/menelaus_web_pools.erl"},
                                     {line,92}]},
                                   {request_throttler,do_request,3,
                                    [{file,"src/request_throttler.erl"},
                                     {line,59}]},
                                   {menelaus_web,loop,2,
                                    [{file,"src/menelaus_web.erl"},
                                     {line,121}]},
                                   {mochiweb_http,headers,5,
                                    [{file,
                                      "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/mochiweb/mochiweb_http.erl"},
                                     {line,94}]},
                                   {proc_lib,init_p_do_apply,3,
                                    [{file,"proc_lib.erl"},{line,239}]}]}]
[ns_server:warn,2019-07-04T11:56:03.517Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:03.517Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:03.517Z,ns_1@127.0.0.1:ns_audit_cfg<0.621.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:error,2019-07-04T11:56:03.546Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_audit_cfg:init/1
    pid: <0.351.0>
    registered_name: ns_audit_cfg
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_audit_cfg,notify_memcached,1,
                                    [{file,"src/ns_audit_cfg.erl"},
                                     {line,171}]},
                      {ns_audit_cfg,handle_info,2,
                                    [{file,"src/ns_audit_cfg.erl"},
                                     {line,145}]},
                      {gen_server,handle_msg,5,
                                  [{file,"gen_server.erl"},{line,604}]},
                      {proc_lib,init_p_do_apply,3,
                                [{file,"proc_lib.erl"},{line,239}]}]}
      in function  gen_server:terminate/6 (gen_server.erl, line 744)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: [{'$gen_call',{<0.303.0>,#Ref<0.0.0.3195>},get_uid}]
    links: [<0.223.0>,<0.352.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 7509
  neighbours:

[error_logger:error,2019-07-04T11:56:03.546Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_audit_cfg,notify_memcached,1,
                                 [{file,"src/ns_audit_cfg.erl"},{line,171}]},
                   {ns_audit_cfg,handle_info,2,
                                 [{file,"src/ns_audit_cfg.erl"},{line,145}]},
                   {gen_server,handle_msg,5,
                               [{file,"gen_server.erl"},{line,604}]},
                   {proc_lib,init_p_do_apply,3,
                             [{file,"proc_lib.erl"},{line,239}]}]}
     Offender:   [{pid,<0.351.0>},
                  {name,ns_audit_cfg},
                  {mfargs,{ns_audit_cfg,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2019-07-04T11:56:03.546Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server menelaus_web_cache terminating 
** Last message in was #Fun<menelaus_web_cache.2.13672906>
** When Server state == []
** Reason for termination == 
** {{{badmatch,{error,couldnt_connect_to_memcached}},
     [{ns_audit_cfg,notify_memcached,1,
                    [{file,"src/ns_audit_cfg.erl"},{line,171}]},
      {ns_audit_cfg,handle_info,2,[{file,"src/ns_audit_cfg.erl"},{line,145}]},
      {gen_server,handle_msg,5,[{file,"gen_server.erl"},{line,604}]},
      {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,239}]}]},
    {gen_server,call,[ns_audit_cfg,get_uid]}}

[error_logger:error,2019-07-04T11:56:03.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: work_queue:init/1
    pid: <0.303.0>
    registered_name: menelaus_web_cache
    exception exit: {{{badmatch,{error,couldnt_connect_to_memcached}},
                      [{ns_audit_cfg,notify_memcached,1,
                                     [{file,"src/ns_audit_cfg.erl"},
                                      {line,171}]},
                       {ns_audit_cfg,handle_info,2,
                                     [{file,"src/ns_audit_cfg.erl"},
                                      {line,145}]},
                       {gen_server,handle_msg,5,
                                   [{file,"gen_server.erl"},{line,604}]},
                       {proc_lib,init_p_do_apply,3,
                                 [{file,"proc_lib.erl"},{line,239}]}]},
                     {gen_server,call,[ns_audit_cfg,get_uid]}}
      in function  gen_server:terminate/6 (gen_server.erl, line 744)
    ancestors: [menelaus_sup,ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.297.0>]
    dictionary: [{seen_keys_stack,[]}]
    trap_exit: false
    status: running
    heap_size: 75113
    stack_size: 27
    reductions: 29286
  neighbours:

[error_logger:error,2019-07-04T11:56:03.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,menelaus_sup}
     Context:    child_terminated
     Reason:     {{{badmatch,{error,couldnt_connect_to_memcached}},
                   [{ns_audit_cfg,notify_memcached,1,
                                  [{file,"src/ns_audit_cfg.erl"},{line,171}]},
                    {ns_audit_cfg,handle_info,2,
                                  [{file,"src/ns_audit_cfg.erl"},{line,145}]},
                    {gen_server,handle_msg,5,
                                [{file,"gen_server.erl"},{line,604}]},
                    {proc_lib,init_p_do_apply,3,
                              [{file,"proc_lib.erl"},{line,239}]}]},
                  {gen_server,call,[ns_audit_cfg,get_uid]}}
     Offender:   [{pid,<0.303.0>},
                  {name,menelaus_web_cache},
                  {mfargs,{menelaus_web_cache,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,5000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:56:03.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.622.0>},
                       {name,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:03.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.621.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:03.547Z,ns_1@127.0.0.1:<0.625.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:56:03.614Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:warn,2019-07-04T11:56:03.788Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:03.788Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:03.788Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:56:03.856Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:56:03.856Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:warn,2019-07-04T11:56:03.877Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:03.877Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:03.964Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-cbauth<0.655.0>:json_rpc_connection:init:73]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.655.0>
[ns_server:debug,2019-07-04T11:56:03.985Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"cbq-engine-cbauth",<0.655.0>} started
[ns_server:warn,2019-07-04T11:56:03.987Z,ns_1@127.0.0.1:<0.453.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:info,2019-07-04T11:56:03.987Z,ns_1@127.0.0.1:ns_memcached-app<0.452.0>:ns_memcached:handle_cast:670]Failed to establish ns_memcached connection: {error,
                                              couldnt_connect_to_memcached}
[ns_server:debug,2019-07-04T11:56:03.988Z,ns_1@127.0.0.1:ns_memcached-app<0.452.0>:ns_memcached:terminate:739]Dying when socket is not yet connected
[stats:error,2019-07-04T11:56:03.988Z,ns_1@127.0.0.1:<0.475.0>:base_stats_collector:handle_info:109](Collector: stats_collector) Exception in stats collector: {exit,
                                                            {{bad_return_value,
                                                              {stop,
                                                               {error,
                                                                couldnt_connect_to_memcached}}},
                                                             {gen_server,
                                                              call,
                                                              ['ns_memcached-app',
                                                               {stats,<<>>},
                                                               180000]}},
                                                            [{gen_server,
                                                              call,3,
                                                              [{file,
                                                                "gen_server.erl"},
                                                               {line,188}]},
                                                             {ns_memcached,
                                                              do_call,3,
                                                              [{file,
                                                                "src/ns_memcached.erl"},
                                                               {line,1325}]},
                                                             {stats_collector,
                                                              grab_stats,1,
                                                              [{file,
                                                                "src/stats_collector.erl"},
                                                               {line,68}]},
                                                             {base_stats_collector,
                                                              handle_info,2,
                                                              [{file,
                                                                "src/base_stats_collector.erl"},
                                                               {line,89}]},
                                                             {gen_server,
                                                              handle_msg,5,
                                                              [{file,
                                                                "gen_server.erl"},
                                                               {line,604}]},
                                                             {proc_lib,
                                                              init_p_do_apply,
                                                              3,
                                                              [{file,
                                                                "proc_lib.erl"},
                                                               {line,239}]}]}

[ns_server:debug,2019-07-04T11:56:03.989Z,ns_1@127.0.0.1:<0.456.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {bucket_info_cache_invalidations,<0.455.0>} exited with reason shutdown
[ns_server:debug,2019-07-04T11:56:03.989Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@cbq-engine-cbauth",admin}
[user:warn,2019-07-04T11:56:03.988Z,ns_1@127.0.0.1:<0.458.0>:compaction_new_daemon:do_chain_compactors:596]Compactor for database `app` (pid [{type,database},
                                   {important,true},
                                   {name,<<"app">>},
                                   {fa,
                                    {#Fun<compaction_new_daemon.4.106246998>,
                                     [<<"app">>,
                                      {config,
                                       {30,undefined},
                                       {30,undefined},
                                       undefined,false,false,
                                       {daemon_config,30,131072,20971520}},
                                      false,
                                      {[{type,bucket}]}]}}]) terminated unexpectedly: {{bad_return_value,
                                                                                        {stop,
                                                                                         {error,
                                                                                          couldnt_connect_to_memcached}}},
                                                                                       {gen_server,
                                                                                        call,
                                                                                        [{'ns_memcached-app',
                                                                                          'ns_1@127.0.0.1'},
                                                                                         {raw_stats,
                                                                                          <<"diskinfo">>,
                                                                                          #Fun<compaction_new_daemon.18.106246998>,
                                                                                          {<<"0">>,
                                                                                           <<"0">>}},
                                                                                         180000]}}
[stats:error,2019-07-04T11:56:03.990Z,ns_1@127.0.0.1:<0.475.0>:base_stats_collector:handle_info:109](Collector: stats_collector) Exception in stats collector: {exit,
                                                            {noproc,
                                                             {gen_server,
                                                              call,
                                                              ['ns_memcached-app',
                                                               {stats,<<>>},
                                                               180000]}},
                                                            [{gen_server,
                                                              call,3,
                                                              [{file,
                                                                "gen_server.erl"},
                                                               {line,188}]},
                                                             {ns_memcached,
                                                              do_call,3,
                                                              [{file,
                                                                "src/ns_memcached.erl"},
                                                               {line,1325}]},
                                                             {stats_collector,
                                                              grab_stats,1,
                                                              [{file,
                                                                "src/stats_collector.erl"},
                                                               {line,68}]},
                                                             {base_stats_collector,
                                                              handle_info,2,
                                                              [{file,
                                                                "src/base_stats_collector.erl"},
                                                               {line,89}]},
                                                             {gen_server,
                                                              handle_msg,5,
                                                              [{file,
                                                                "gen_server.erl"},
                                                               {line,604}]},
                                                             {proc_lib,
                                                              init_p_do_apply,
                                                              3,
                                                              [{file,
                                                                "proc_lib.erl"},
                                                               {line,239}]}]}

[ns_server:debug,2019-07-04T11:56:03.991Z,ns_1@127.0.0.1:ns_memcached-app<0.658.0>:ns_memcached:init:158]Starting ns_memcached
[ns_server:debug,2019-07-04T11:56:03.991Z,ns_1@127.0.0.1:<0.659.0>:ns_memcached:run_connect_phase:181]Started 'connecting' phase of ns_memcached-app. Parent is <0.658.0>
[error_logger:error,2019-07-04T11:56:03.988Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.452.0> terminating 
** Last message in was {'$gen_cast',
                           {connect_done,4,
                               {error,couldnt_connect_to_memcached}}}
** When Server state == {state,4,0,0,
                            {[{{stats,<<>>},
                               {<0.475.0>,#Ref<0.0.0.2464>},
                               {1562,241362,256723},
                               2},
                              {{set_cluster_config,
                                   <<"{\"rev\":28,\"name\":\"app\",\"uri\":\"/pools/default/buckets/app?bucket_uuid=5b6ce58456b7220bfb025f341cb20648\",\"streamingUri\":\"/pools/default/bucketsStreaming/app?bucket_uuid=5b6ce58456b7220bfb025f341cb20648\",\"nodes\":[{\"couchApiBase\":\"http://$HOST:8092/app%2B5b6ce58456b7220bfb025f341cb20648\",\"hostname\":\"$HOST:8091\",\"ports\":{\"proxy\":11211,\"direct\":11210}}],\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"fts\":8094,\"indexAdmin\":9100,\"indexScan\":9101,\"indexHttp\":9102,\"indexStreamInit\":9103,\"indexStreamCatchup\":9104,\"indexStreamMaint\":9105,\"capi\":8092,\"projector\":9999,\"kv\":11210,\"moxi\":11211,\"n1ql\":8093},\"thisNode\":true}],\"nodeLocator\":\"vbucket\",\"uuid\":\"5b6ce58456b7220bfb025f341cb20648\",\"ddocs\":{\"uri\":\"/pools/default/buckets/app/ddocs\"},\"vBucketServerMap\":{\"hashAlgorithm\":\"CRC\",\"numReplicas\":1,\"serverList\":[\"$HOST:11210\"],\"vBucketMap\":[[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1],[0,-1]]},\"bucketCapabilitiesVer\":\"\",\"bucketCapabilities\":[\"couchapi\",\"xattr\",\"dcp\",\"cbhello\",\"touch\",\"cccp\",\"xdcrCheckpointing\",\"nodesExt\"]}">>},
                               {<0.455.0>,#Ref<0.0.0.1911>},
                               {1562,241360,302317},
                               2}],
                             [{{raw_stats,<<"diskinfo">>,
                                   #Fun<compaction_new_daemon.18.106246998>,
                                   {<<"0">>,<<"0">>}},
                               {<0.459.0>,#Ref<0.0.0.1896>},
                               {1562,241359,973119},
                               2}]},
                            {[],[]},
                            {[],[]},
                            connecting,undefined,"app",still_connecting,
                            undefined,[],[],undefined}
** Reason for termination == 
** {bad_return_value,{stop,{error,couldnt_connect_to_memcached}}}

[error_logger:error,2019-07-04T11:56:03.998Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_memcached:init/1
    pid: <0.452.0>
    registered_name: []
    exception exit: {bad_return_value,
                        {stop,{error,couldnt_connect_to_memcached}}}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [<0.451.0>,'single_bucket_kv_sup-app',ns_bucket_sup,
                  ns_bucket_worker_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.451.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 1598
    stack_size: 27
    reductions: 2267
  neighbours:

[error_logger:error,2019-07-04T11:56:03.998Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.452.0>,
                                         {error,
                                          {bad_return_value,
                                           {stop,
                                            {error,
                                             couldnt_connect_to_memcached}}}}}

[error_logger:error,2019-07-04T11:56:03.999Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {<0.451.0>,ns_memcached_sup}
     Context:    child_terminated
     Reason:     {bad_return_value,
                     {stop,{error,couldnt_connect_to_memcached}}}
     Offender:   [{pid,<0.452.0>},
                  {name,{ns_memcached,"app"}},
                  {mfargs,{ns_memcached,start_link,["app"]}},
                  {restart_type,permanent},
                  {shutdown,86400000},
                  {child_type,worker}]


[error_logger:error,2019-07-04T11:56:03.999Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: compaction_new_daemon:-spawn_dbs_compactor/4-fun-1-/0
    pid: <0.459.0>
    registered_name: []
    exception exit: {{bad_return_value,
                         {stop,{error,couldnt_connect_to_memcached}}},
                     {gen_server,call,
                         [{'ns_memcached-app','ns_1@127.0.0.1'},
                          {raw_stats,<<"diskinfo">>,
                              #Fun<compaction_new_daemon.18.106246998>,
                              {<<"0">>,<<"0">>}},
                          180000]}}
      in function  gen_server:call/3 (gen_server.erl, line 188)
      in call from ns_memcached:do_call/3 (src/ns_memcached.erl, line 1325)
      in call from compaction_new_daemon:aggregated_size_info/1 (src/compaction_new_daemon.erl, line 997)
      in call from compaction_new_daemon:bucket_needs_compaction/3 (src/compaction_new_daemon.erl, line 969)
      in call from compaction_new_daemon:'-spawn_dbs_compactor/4-fun-1-'/4 (src/compaction_new_daemon.erl, line 622)
    ancestors: [<0.458.0>,<0.457.0>,compaction_new_daemon,ns_server_sup,
                  ns_server_nodes_sup,<0.168.0>,ns_server_cluster_sup,
                  <0.89.0>]
    messages: []
    links: [<0.458.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 17731
    stack_size: 27
    reductions: 13187
  neighbours:

[error_logger:info,2019-07-04T11:56:03.999Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.451.0>,ns_memcached_sup}
             started: [{pid,<0.658.0>},
                       {name,{ns_memcached,"app"}},
                       {mfargs,{ns_memcached,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:error,2019-07-04T11:56:04.000Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: compaction_new_daemon:-chain_compactors/1-fun-0-/0
    pid: <0.458.0>
    registered_name: []
    exception exit: {{bad_return_value,
                         {stop,{error,couldnt_connect_to_memcached}}},
                     {gen_server,call,
                         [{'ns_memcached-app','ns_1@127.0.0.1'},
                          {raw_stats,<<"diskinfo">>,
                              #Fun<compaction_new_daemon.18.106246998>,
                              {<<"0">>,<<"0">>}},
                          180000]}}
      in function  compaction_new_daemon:do_chain_compactors/2 (src/compaction_new_daemon.erl, line 600)
    ancestors: [<0.457.0>,compaction_new_daemon,ns_server_sup,
                  ns_server_nodes_sup,<0.168.0>,ns_server_cluster_sup,
                  <0.89.0>]
    messages: []
    links: [<0.457.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 4334
  neighbours:
    neighbour: [{pid,<0.457.0>},
                  {registered_name,[]},
                  {initial_call,
                      {compaction_new_daemon,
                          '-spawn_scheduled_kv_compactor/2-fun-0-',[]}},
                  {current_function,{misc,wait_for_process,2}},
                  {ancestors,
                      [compaction_new_daemon,ns_server_sup,
                       ns_server_nodes_sup,<0.168.0>,ns_server_cluster_sup,
                       <0.89.0>]},
                  {messages,[]},
                  {links,[<0.446.0>,<0.458.0>]},
                  {dictionary,[]},
                  {trap_exit,false},
                  {status,waiting},
                  {heap_size,6772},
                  {stack_size,7},
                  {reductions,1862}]

[ns_server:error,2019-07-04T11:56:04.000Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:log_compactors_exit:1326]Compactor <0.457.0> exited unexpectedly: {{bad_return_value,
                                           {stop,
                                            {error,
                                             couldnt_connect_to_memcached}}},
                                          {gen_server,call,
                                           [{'ns_memcached-app',
                                             'ns_1@127.0.0.1'},
                                            {raw_stats,<<"diskinfo">>,
                                             #Fun<compaction_new_daemon.18.106246998>,
                                             {<<"0">>,<<"0">>}},
                                            180000]}}. Moving to the next bucket.
[ns_server:debug,2019-07-04T11:56:04.001Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:56:04.001Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 26s
[error_logger:info,2019-07-04T11:56:04.001Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.451.0>,ns_memcached_sup}
             started: [{pid,<0.660.0>},
                       {name,{terse_bucket_info_uploader,"app"}},
                       {mfargs,
                           {terse_bucket_info_uploader,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:04.041Z,ns_1@127.0.0.1:<0.659.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:56:04.042Z,ns_1@127.0.0.1:<0.483.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:error,2019-07-04T11:56:04.042Z,ns_1@127.0.0.1:query_stats_collector<0.402.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2019-07-04T11:56:04.164Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            1,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              2,half_down,false}
[ns_server:debug,2019-07-04T11:56:04.387Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:warn,2019-07-04T11:56:04.444Z,ns_1@127.0.0.1:<0.579.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:56:04.509Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:04.514Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:04.524Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:04.524Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:04.559Z,ns_1@127.0.0.1:<0.625.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:56:04.603Z,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.696.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.696.0>
[ns_server:debug,2019-07-04T11:56:04.603Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"goxdcr-cbauth",<0.696.0>} started
[ns_server:warn,2019-07-04T11:56:04.627Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:04.640Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2019-07-04T11:56:04.644Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.409.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:56:04.668Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.409.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:error,2019-07-04T11:56:04.677Z,ns_1@127.0.0.1:service_status_keeper_worker<0.416.0>:rest_utils:get_json_local:63]Request to (indexer) getIndexStatus failed: {error,
                                             {econnrefused,
                                              [{lhttpc_client,send_request,1,
                                                [{file,
                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                 {line,220}]},
                                               {lhttpc_client,execute,9,
                                                [{file,
                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                 {line,169}]},
                                               {lhttpc_client,request,9,
                                                [{file,
                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                 {line,92}]}]}}
[ns_server:error,2019-07-04T11:56:04.677Z,ns_1@127.0.0.1:service_status_keeper-index<0.417.0>:service_status_keeper:handle_cast:119]Service service_index returned incorrect status
[ns_server:error,2019-07-04T11:56:04.697Z,ns_1@127.0.0.1:service_status_keeper_worker<0.416.0>:rest_utils:get_json_local:63]Request to (fts) api/nsstatus failed: {error,
                                       {econnrefused,
                                        [{lhttpc_client,send_request,1,
                                          [{file,
                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                           {line,220}]},
                                         {lhttpc_client,execute,9,
                                          [{file,
                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                           {line,169}]},
                                         {lhttpc_client,request,9,
                                          [{file,
                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                           {line,92}]}]}}
[ns_server:error,2019-07-04T11:56:04.697Z,ns_1@127.0.0.1:service_status_keeper-fts<0.421.0>:service_status_keeper:handle_cast:119]Service service_fts returned incorrect status
[ns_server:warn,2019-07-04T11:56:04.837Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:04.837Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:04.880Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:56:04.880Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:warn,2019-07-04T11:56:04.884Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:04.884Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:05.043Z,ns_1@127.0.0.1:<0.659.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:error,2019-07-04T11:56:05.044Z,ns_1@127.0.0.1:query_stats_collector<0.402.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2019-07-04T11:56:05.044Z,ns_1@127.0.0.1:<0.483.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:56:05.050Z,ns_1@127.0.0.1:json_rpc_connection-fts-cbauth<0.727.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-cbauth", handling process <0.727.0>
[ns_server:debug,2019-07-04T11:56:05.050Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"fts-cbauth",<0.727.0>} started
[ns_server:debug,2019-07-04T11:56:05.188Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            2,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              3,half_down,false}
[ns_server:debug,2019-07-04T11:56:05.190Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@fts-cbauth",admin}
[ns_server:warn,2019-07-04T11:56:05.459Z,ns_1@127.0.0.1:<0.579.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:56:05.536Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:05.537Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:05.543Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:05.543Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-07-04T11:56:05.576Z,ns_1@127.0.0.1:<0.625.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-07-04T11:56:05.711Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:warn,2019-07-04T11:56:05.846Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:05.846Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:05.862Z,ns_1@127.0.0.1:json_rpc_connection-fts-service_api<0.762.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-service_api", handling process <0.762.0>
[ns_server:debug,2019-07-04T11:56:05.863Z,ns_1@127.0.0.1:service_agent-fts<0.342.0>:service_agent:do_handle_connection:324]Observed new json rpc connection for fts: <0.762.0>
[ns_server:debug,2019-07-04T11:56:05.863Z,ns_1@127.0.0.1:<0.345.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {json_rpc_events,<0.343.0>} exited with reason normal
[ns_server:debug,2019-07-04T11:56:05.885Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:warn,2019-07-04T11:56:05.890Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-07-04T11:56:05.891Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-07-04T11:56:05.911Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:346]Checking if service service_fts is started...
[ns_server:debug,2019-07-04T11:56:06.053Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:service_stats_collector:check_status:350]Service service_fts is started
[ns_server:warn,2019-07-04T11:56:06.068Z,ns_1@127.0.0.1:<0.659.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-07-04T11:56:06.069Z,ns_1@127.0.0.1:<0.483.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:error,2019-07-04T11:56:06.069Z,ns_1@127.0.0.1:query_stats_collector<0.402.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:info,2019-07-04T11:56:06.136Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:debug,2019-07-04T11:56:06.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            3,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              4,half_down,false}
[stats:warn,2019-07-04T11:56:06.476Z,ns_1@127.0.0.1:<0.407.0>:base_stats_collector:latest_tick:69](Collector: global_stats_collector) Dropped 4 ticks
[ns_server:debug,2019-07-04T11:56:06.552Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:warn,2019-07-04T11:56:06.712Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:06.752Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"2a8ae539a5cab1af4159b9b57e0098ee">>} ->
[{'_vclock',[{<<"2a8ae539a5cab1af4159b9b57e0098ee">>,{30,63729460566}}]}]
[ns_server:debug,2019-07-04T11:56:06.755Z,ns_1@127.0.0.1:ns_config_rep<0.244.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"2a8ae539a5cab1af4159b9b57e0098ee">>}]..)
[ns_server:debug,2019-07-04T11:56:06.905Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:56:07.047Z,ns_1@127.0.0.1:<0.483.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:56:07.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            4,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              5,half_down,false}
[ns_server:debug,2019-07-04T11:56:07.316Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.276.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-07-04T11:56:07.909Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-07-04T11:56:07.910Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:service_stats_collector:check_status:350]Service service_index is started
[ns_server:debug,2019-07-04T11:56:08.055Z,ns_1@127.0.0.1:<0.483.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:warn,2019-07-04T11:56:08.215Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:08.217Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            5,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              6,half_down,false}
[ns_server:debug,2019-07-04T11:56:09.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            6,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              7,half_down,false}
[ns_server:warn,2019-07-04T11:56:09.719Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:10.160Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            7,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              8,half_down,false}
[ns_server:info,2019-07-04T11:56:11.146Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:warn,2019-07-04T11:56:11.230Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:11.233Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            8,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              9,half_down,false}
[ns_server:debug,2019-07-04T11:56:11.644Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2019-07-04T11:56:11.645Z,ns_1@127.0.0.1:ns_audit<0.355.0>:ns_audit:handle_call:110]Audit login_success: [{roles,[<<"admin">>]},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {sessionid,<<"5b3305ecdc1bb28d4bb16b2ac4170dd3">>},
                      {remote,{[{ip,<<"172.24.0.1">>},{port,35038}]}},
                      {timestamp,<<"2019-07-04T11:56:11.645Z">>}]
[ns_server:debug,2019-07-04T11:56:12.160Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            9,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              10,half_down,false}
[ns_server:warn,2019-07-04T11:56:12.821Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:13.172Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            10,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              11,half_down,false}
[ns_server:warn,2019-07-04T11:56:14.334Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:14.346Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            11,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              12,half_down,false}
[ns_server:debug,2019-07-04T11:56:15.165Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            12,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              13,half_down,false}
[ns_server:debug,2019-07-04T11:56:15.631Z,ns_1@127.0.0.1:leader_lease_agent<0.490.0>:leader_lease_agent:handle_lease_expired:284]Lease held by {lease_holder,<<"2e0765d644fd2a31b0632a30fdfa32a4">>,
                            'ns_1@127.0.0.1'} expired. Starting expirer.
[ns_server:warn,2019-07-04T11:56:15.632Z,ns_1@127.0.0.1:<0.508.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@127.0.0.1',
                                                                            <<"2e0765d644fd2a31b0632a30fdfa32a4">>} (valid for 0ms)
[ns_server:debug,2019-07-04T11:56:15.639Z,ns_1@127.0.0.1:leader_lease_agent<0.490.0>:leader_lease_agent:do_handle_acquire_lease:147]Granting lease to {lease_holder,<<"366f62545d6698c8c51333285a57d49d">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:info,2019-07-04T11:56:15.657Z,ns_1@127.0.0.1:<0.508.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"366f62545d6698c8c51333285a57d49d">>)
[ns_server:warn,2019-07-04T11:56:15.857Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:info,2019-07-04T11:56:17.044Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[stats:warn,2019-07-04T11:56:17.065Z,ns_1@127.0.0.1:system_stats_collector<0.380.0>:base_stats_collector:latest_tick:69](Collector: system_stats_collector) Dropped 1 ticks
[stats:warn,2019-07-04T11:56:17.069Z,ns_1@127.0.0.1:<0.407.0>:base_stats_collector:latest_tick:69](Collector: global_stats_collector) Dropped 1 ticks
[stats:warn,2019-07-04T11:56:17.073Z,ns_1@127.0.0.1:query_stats_collector<0.402.0>:base_stats_collector:latest_tick:69](Collector: query_stats_collector) Dropped 1 ticks
[stats:warn,2019-07-04T11:56:17.081Z,ns_1@127.0.0.1:service_stats_collector-index<0.434.0>:base_stats_collector:latest_tick:69](Collector: service_stats_collector) Dropped 1 ticks
[stats:warn,2019-07-04T11:56:17.235Z,ns_1@127.0.0.1:service_stats_collector-fts<0.431.0>:base_stats_collector:latest_tick:69](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:warn,2019-07-04T11:56:17.301Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:handle_info:116]Dropped 1 heartbeats
[ns_server:warn,2019-07-04T11:56:17.575Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:17.585Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            13,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              14,half_down,false}
[ns_server:debug,2019-07-04T11:56:17.585Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            14,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              15,half_down,false}
[error_logger:error,2019-07-04T11:56:18.051Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: misc:turn_into_gen_server/4
    pid: <12396.238.0>
    registered_name: 'capi_set_view_manager-app'
    exception exit: {error,
                        {select_bucket_failed,
                            {memcached_error,key_enoent,undefined}}}
      in function  capi_set_view_manager:wait_for_bucket_to_start/2 (src/capi_set_view_manager.erl, line 200)
      in call from capi_set_view_manager:init/1 (src/capi_set_view_manager.erl, line 161)
      in call from misc:turn_into_gen_server/4 (src/misc.erl, line 379)
    ancestors: [<0.392.0>,'single_bucket_kv_sup-app',ns_bucket_sup,
                  ns_bucket_worker_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.392.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 19202
  neighbours:

[error_logger:error,2019-07-04T11:56:18.052Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {<0.392.0>,docs_kv_sup}
     Context:    child_terminated
     Reason:     {error,
                     {select_bucket_failed,
                         {memcached_error,key_enoent,undefined}}}
     Offender:   [{pid,<12396.238.0>},
                  {name,capi_set_view_manager},
                  {mfargs,
                      {capi_set_view_manager,start_link_remote,
                          ['couchdb_ns_1@127.0.0.1',"app"]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:56:18.057Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.1487.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:56:18.058Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.1488.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-07-04T11:56:18.058Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.1487.0>},
                       {name,doc_replicator},
                       {mfargs,{capi_ddoc_manager,start_replicator,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:18.058Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.1488.0>},
                       {name,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:18.063Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.306.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:18.064Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.307.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["app",<0.1487.0>,<0.1488.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:18.064Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.305.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:18.064Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.1487.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.307.0>
[ns_server:debug,2019-07-04T11:56:18.064Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.1487.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2019-07-04T11:56:18.064Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.1488.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.307.0>
[error_logger:info,2019-07-04T11:56:18.065Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.310.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:18.089Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.313.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:18.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            15,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              16,half_down,false}
[ns_server:warn,2019-07-04T11:56:19.085Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:19.170Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            16,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              17,half_down,false}
[ns_server:warn,2019-07-04T11:56:20.588Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:20.589Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            17,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              18,half_down,false}
[ns_server:error,2019-07-04T11:56:20.703Z,ns_1@127.0.0.1:<0.1399.0>:janitor_agent:query_states_details:200]Failed to query vbucket states from some nodes:
[{'ns_1@127.0.0.1',timeout}]
[ns_server:info,2019-07-04T11:56:20.704Z,ns_1@127.0.0.1:<0.1399.0>:ns_janitor:cleanup_with_states:130]Bucket "app" not yet ready on ['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:56:21.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            18,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              19,half_down,false}
[ns_server:warn,2019-07-04T11:56:22.092Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:22.167Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            19,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              20,half_down,false}
[ns_server:warn,2019-07-04T11:56:23.566Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:23.567Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            20,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              21,half_down,false}
[ns_server:debug,2019-07-04T11:56:24.139Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            21,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              22,half_down,false}
[ns_server:warn,2019-07-04T11:56:25.095Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:25.148Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            22,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              23,half_down,false}
[ns_server:info,2019-07-04T11:56:26.132Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:error,2019-07-04T11:56:26.145Z,ns_1@127.0.0.1:<0.1668.0>:janitor_agent:query_states_details:200]Failed to query vbucket states from some nodes:
[{'ns_1@127.0.0.1',timeout}]
[ns_server:info,2019-07-04T11:56:26.145Z,ns_1@127.0.0.1:<0.1668.0>:ns_janitor:cleanup_with_states:130]Bucket "app" not yet ready on ['ns_1@127.0.0.1']
[ns_server:warn,2019-07-04T11:56:26.608Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:26.610Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            23,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              24,half_down,false}
[ns_server:debug,2019-07-04T11:56:27.158Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            24,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              25,half_down,false}
[ns_server:warn,2019-07-04T11:56:27.301Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:handle_info:116]Dropped 1 heartbeats
[ns_server:warn,2019-07-04T11:56:28.127Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:28.160Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            25,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              26,half_down,false}
[error_logger:error,2019-07-04T11:56:28.225Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: misc:turn_into_gen_server/4
    pid: <12396.310.0>
    registered_name: 'capi_set_view_manager-app'
    exception exit: {error,
                        {select_bucket_failed,
                            {memcached_error,key_enoent,undefined}}}
      in function  capi_set_view_manager:wait_for_bucket_to_start/2 (src/capi_set_view_manager.erl, line 200)
      in call from capi_set_view_manager:init/1 (src/capi_set_view_manager.erl, line 161)
      in call from misc:turn_into_gen_server/4 (src/misc.erl, line 379)
    ancestors: [<0.392.0>,'single_bucket_kv_sup-app',ns_bucket_sup,
                  ns_bucket_worker_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.392.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 19340
  neighbours:

[error_logger:error,2019-07-04T11:56:28.225Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {<0.392.0>,docs_kv_sup}
     Context:    child_terminated
     Reason:     {error,
                     {select_bucket_failed,
                         {memcached_error,key_enoent,undefined}}}
     Offender:   [{pid,<12396.310.0>},
                  {name,capi_set_view_manager},
                  {mfargs,
                      {capi_set_view_manager,start_link_remote,
                          ['couchdb_ns_1@127.0.0.1',"app"]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:56:28.232Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.1981.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:56:28.233Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.1982.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-07-04T11:56:28.233Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.1981.0>},
                       {name,doc_replicator},
                       {mfargs,{capi_ddoc_manager,start_replicator,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:28.233Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.1982.0>},
                       {name,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:28.236Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.1981.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.358.0>
[ns_server:debug,2019-07-04T11:56:28.236Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.1981.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2019-07-04T11:56:28.236Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.1982.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.358.0>
[error_logger:info,2019-07-04T11:56:28.236Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.357.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:28.236Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.358.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["app",<0.1981.0>,<0.1982.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:28.237Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.356.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-07-04T11:56:28.239Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.360.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:28.242Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.363.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:29.088Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:56:29.097Z,ns_1@127.0.0.1:<0.2007.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:56:29.098Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:56:29.098Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:warn,2019-07-04T11:56:29.632Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:29.638Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            26,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              27,half_down,false}
[ns_server:debug,2019-07-04T11:56:30.003Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:56:30.003Z,ns_1@127.0.0.1:<0.2055.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:56:30.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            27,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              28,half_down,false}
[ns_server:warn,2019-07-04T11:56:31.152Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:31.164Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            28,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              29,half_down,false}
[ns_server:warn,2019-07-04T11:56:32.664Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:32.665Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            29,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              30,half_down,false}
[ns_server:debug,2019-07-04T11:56:33.162Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            30,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              31,half_down,false}
[ns_server:warn,2019-07-04T11:56:34.167Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:34.169Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            31,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              32,half_down,false}
[ns_server:debug,2019-07-04T11:56:35.162Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            32,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              33,half_down,false}
[ns_server:warn,2019-07-04T11:56:35.670Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:info,2019-07-04T11:56:36.138Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:debug,2019-07-04T11:56:36.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            33,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              34,half_down,false}
[ns_server:error,2019-07-04T11:56:36.186Z,ns_1@127.0.0.1:<0.2143.0>:janitor_agent:query_states_details:200]Failed to query vbucket states from some nodes:
[{'ns_1@127.0.0.1',timeout}]
[ns_server:info,2019-07-04T11:56:36.186Z,ns_1@127.0.0.1:<0.2143.0>:ns_janitor:cleanup_with_states:130]Bucket "app" not yet ready on ['ns_1@127.0.0.1']
[ns_server:warn,2019-07-04T11:56:37.173Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:37.174Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            34,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              35,half_down,false}
[ns_server:warn,2019-07-04T11:56:37.309Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:handle_info:116]Dropped 1 heartbeats
[ns_server:debug,2019-07-04T11:56:38.166Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            35,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              36,half_down,false}
[error_logger:error,2019-07-04T11:56:38.362Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: misc:turn_into_gen_server/4
    pid: <12396.360.0>
    registered_name: 'capi_set_view_manager-app'
    exception exit: {error,
                        {select_bucket_failed,
                            {memcached_error,key_enoent,undefined}}}
      in function  capi_set_view_manager:wait_for_bucket_to_start/2 (src/capi_set_view_manager.erl, line 200)
      in call from capi_set_view_manager:init/1 (src/capi_set_view_manager.erl, line 161)
      in call from misc:turn_into_gen_server/4 (src/misc.erl, line 379)
    ancestors: [<0.392.0>,'single_bucket_kv_sup-app',ns_bucket_sup,
                  ns_bucket_worker_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.392.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 19340
  neighbours:

[error_logger:error,2019-07-04T11:56:38.362Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {<0.392.0>,docs_kv_sup}
     Context:    child_terminated
     Reason:     {error,
                     {select_bucket_failed,
                         {memcached_error,key_enoent,undefined}}}
     Offender:   [{pid,<12396.360.0>},
                  {name,capi_set_view_manager},
                  {mfargs,
                      {capi_set_view_manager,start_link_remote,
                          ['couchdb_ns_1@127.0.0.1',"app"]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:56:38.369Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2472.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:56:38.370Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.2474.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-07-04T11:56:38.370Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.2472.0>},
                       {name,doc_replicator},
                       {mfargs,{capi_ddoc_manager,start_replicator,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:38.370Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.2474.0>},
                       {name,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:38.375Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.409.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:38.375Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.410.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["app",<0.2472.0>,<0.2474.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:38.378Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.408.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:38.379Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2472.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.410.0>
[ns_server:debug,2019-07-04T11:56:38.382Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.2474.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.410.0>
[ns_server:debug,2019-07-04T11:56:38.391Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2472.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2019-07-04T11:56:38.395Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.412.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:38.397Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.415.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:38.677Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:39.164Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            36,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              37,half_down,false}
[ns_server:warn,2019-07-04T11:56:40.183Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:40.185Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            37,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              38,half_down,false}
[ns_server:debug,2019-07-04T11:56:41.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            38,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              39,half_down,false}
[ns_server:warn,2019-07-04T11:56:41.687Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:42.212Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            39,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              40,half_down,false}
[ns_server:warn,2019-07-04T11:56:43.190Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:43.191Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            40,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              41,half_down,false}
[ns_server:debug,2019-07-04T11:56:44.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            41,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              42,half_down,false}
[ns_server:warn,2019-07-04T11:56:44.693Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:45.167Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            42,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              43,half_down,false}
[ns_server:info,2019-07-04T11:56:46.139Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:error,2019-07-04T11:56:46.147Z,ns_1@127.0.0.1:<0.2617.0>:janitor_agent:query_states_details:200]Failed to query vbucket states from some nodes:
[{'ns_1@127.0.0.1',timeout}]
[ns_server:info,2019-07-04T11:56:46.147Z,ns_1@127.0.0.1:<0.2617.0>:ns_janitor:cleanup_with_states:130]Bucket "app" not yet ready on ['ns_1@127.0.0.1']
[ns_server:warn,2019-07-04T11:56:46.195Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:46.196Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            43,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              44,half_down,false}
[ns_server:debug,2019-07-04T11:56:47.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            44,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              45,half_down,false}
[ns_server:warn,2019-07-04T11:56:47.347Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:handle_info:116]Dropped 1 heartbeats
[ns_server:warn,2019-07-04T11:56:47.707Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:48.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            45,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              46,half_down,false}
[error_logger:error,2019-07-04T11:56:48.482Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: misc:turn_into_gen_server/4
    pid: <12396.412.0>
    registered_name: 'capi_set_view_manager-app'
    exception exit: {error,
                        {select_bucket_failed,
                            {memcached_error,key_enoent,undefined}}}
      in function  capi_set_view_manager:wait_for_bucket_to_start/2 (src/capi_set_view_manager.erl, line 200)
      in call from capi_set_view_manager:init/1 (src/capi_set_view_manager.erl, line 161)
      in call from misc:turn_into_gen_server/4 (src/misc.erl, line 379)
    ancestors: [<0.392.0>,'single_bucket_kv_sup-app',ns_bucket_sup,
                  ns_bucket_worker_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.392.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 19340
  neighbours:

[error_logger:error,2019-07-04T11:56:48.483Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {<0.392.0>,docs_kv_sup}
     Context:    child_terminated
     Reason:     {error,
                     {select_bucket_failed,
                         {memcached_error,key_enoent,undefined}}}
     Offender:   [{pid,<12396.412.0>},
                  {name,capi_set_view_manager},
                  {mfargs,
                      {capi_set_view_manager,start_link_remote,
                          ['couchdb_ns_1@127.0.0.1',"app"]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:56:48.484Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2935.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:56:48.485Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.2936.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-07-04T11:56:48.485Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.2935.0>},
                       {name,doc_replicator},
                       {mfargs,{capi_ddoc_manager,start_replicator,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:48.485Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.2936.0>},
                       {name,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:48.503Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.459.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:48.504Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2935.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.460.0>
[ns_server:debug,2019-07-04T11:56:48.504Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.2936.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.460.0>
[error_logger:info,2019-07-04T11:56:48.506Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.460.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["app",<0.2935.0>,<0.2936.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:48.506Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.458.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:48.507Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.2935.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2019-07-04T11:56:48.507Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.462.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:48.509Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.465.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-07-04T11:56:49.215Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:49.216Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            46,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              47,half_down,false}
[ns_server:debug,2019-07-04T11:56:50.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            47,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              48,half_down,false}
[ns_server:warn,2019-07-04T11:56:50.717Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:51.162Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            48,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              49,half_down,false}
[ns_server:warn,2019-07-04T11:56:52.222Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:52.222Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            49,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              50,half_down,false}
[ns_server:debug,2019-07-04T11:56:53.130Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            50,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              51,half_down,false}
[ns_server:warn,2019-07-04T11:56:53.700Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:54.139Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            51,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              52,half_down,false}
[ns_server:warn,2019-07-04T11:56:55.220Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:55.222Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            52,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              53,half_down,false}
[ns_server:info,2019-07-04T11:56:56.134Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:error,2019-07-04T11:56:56.155Z,ns_1@127.0.0.1:<0.3100.0>:janitor_agent:query_states_details:200]Failed to query vbucket states from some nodes:
[{'ns_1@127.0.0.1',timeout}]
[ns_server:info,2019-07-04T11:56:56.155Z,ns_1@127.0.0.1:<0.3100.0>:ns_janitor:cleanup_with_states:130]Bucket "app" not yet ready on ['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:56:56.160Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            53,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              54,half_down,false}
[ns_server:warn,2019-07-04T11:56:56.766Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:57.160Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            54,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              55,half_down,false}
[ns_server:warn,2019-07-04T11:56:57.350Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:handle_info:116]Dropped 1 heartbeats
[ns_server:warn,2019-07-04T11:56:58.271Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:56:58.272Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            55,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              56,half_down,false}
[error_logger:error,2019-07-04T11:56:58.694Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: misc:turn_into_gen_server/4
    pid: <12396.462.0>
    registered_name: 'capi_set_view_manager-app'
    exception exit: {error,
                        {select_bucket_failed,
                            {memcached_error,key_enoent,undefined}}}
      in function  capi_set_view_manager:wait_for_bucket_to_start/2 (src/capi_set_view_manager.erl, line 200)
      in call from capi_set_view_manager:init/1 (src/capi_set_view_manager.erl, line 161)
      in call from misc:turn_into_gen_server/4 (src/misc.erl, line 379)
    ancestors: [<0.392.0>,'single_bucket_kv_sup-app',ns_bucket_sup,
                  ns_bucket_worker_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.392.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 19337
  neighbours:

[error_logger:error,2019-07-04T11:56:58.695Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {<0.392.0>,docs_kv_sup}
     Context:    child_terminated
     Reason:     {error,
                     {select_bucket_failed,
                         {memcached_error,key_enoent,undefined}}}
     Offender:   [{pid,<12396.462.0>},
                  {name,capi_set_view_manager},
                  {mfargs,
                      {capi_set_view_manager,start_link_remote,
                          ['couchdb_ns_1@127.0.0.1',"app"]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-07-04T11:56:58.696Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.3441.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-07-04T11:56:58.697Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.3442.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-07-04T11:56:58.697Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.3441.0>},
                       {name,doc_replicator},
                       {mfargs,{capi_ddoc_manager,start_replicator,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:58.697Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<0.3442.0>},
                       {name,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:58.699Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.509.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["app"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:58.699Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-app'}
             started: [{pid,<12396.510.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["app",<0.3441.0>,<0.3442.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-07-04T11:56:58.699Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.508.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-07-04T11:56:58.700Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-app<0.3442.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.510.0>
[error_logger:info,2019-07-04T11:56:58.700Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.512.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:58.699Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.3441.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.510.0>
[ns_server:debug,2019-07-04T11:56:58.703Z,ns_1@127.0.0.1:capi_doc_replicator-app<0.3441.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2019-07-04T11:56:58.703Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.392.0>,docs_kv_sup}
             started: [{pid,<12396.515.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"app"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:59.099Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:56:59.106Z,ns_1@127.0.0.1:<0.3464.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:56:59.107Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:56:59.107Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:56:59.176Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            56,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              57,half_down,false}
[ns_server:error,2019-07-04T11:56:59.251Z,ns_1@127.0.0.1:<0.347.0>:service_agent:wait_for_connection_loop:299]No connection with label "index-service_api" after 60000ms. Exiting.
[ns_server:error,2019-07-04T11:56:59.252Z,ns_1@127.0.0.1:service_agent-index<0.346.0>:service_agent:handle_info:231]Linked process <0.347.0> died with reason {no_connection,"index-service_api"}. Terminating
[ns_server:error,2019-07-04T11:56:59.252Z,ns_1@127.0.0.1:service_agent-index<0.346.0>:service_agent:terminate:260]Terminating abnormally
[ns_server:debug,2019-07-04T11:56:59.252Z,ns_1@127.0.0.1:<0.349.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {json_rpc_events,<0.347.0>} exited with reason {no_connection,
                                                                               "index-service_api"}
[ns_server:debug,2019-07-04T11:56:59.252Z,ns_1@127.0.0.1:<0.348.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.346.0>} exited with reason {linked_process_died,
                                                                                <0.347.0>,
                                                                                {no_connection,
                                                                                 "index-service_api"}}
[error_logger:error,2019-07-04T11:56:59.252Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:-spawn_connection_waiter/2-fun-0-/0
    pid: <0.347.0>
    registered_name: []
    exception exit: {no_connection,"index-service_api"}
      in function  service_agent:wait_for_connection_loop/3 (src/service_agent.erl, line 301)
    ancestors: ['service_agent-index',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.346.0>,<0.349.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 1040
  neighbours:

[ns_server:debug,2019-07-04T11:56:59.253Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"cbq-engine-cbauth",<0.655.0>} needs_update
[error_logger:error,2019-07-04T11:56:59.253Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server 'service_agent-index' terminating 
** Last message in was {'EXIT',<0.347.0>,{no_connection,"index-service_api"}}
** When Server state == {state,index,
                         {dict,2,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],
                            [[{node,'ns_1@127.0.0.1'}|
                              <<"2a8ae539a5cab1af4159b9b57e0098ee">>]],
                            [],[],[],[],[],[],[],[],[],[],[],
                            [[{uuid,<<"2a8ae539a5cab1af4159b9b57e0098ee">>}|
                              'ns_1@127.0.0.1']],
                            []}}},
                         undefined,undefined,undefined,undefined,undefined,
                         undefined,undefined,undefined,undefined,undefined,
                         undefined}
** Reason for termination == 
** {linked_process_died,<0.347.0>,{no_connection,"index-service_api"}}

[error_logger:error,2019-07-04T11:56:59.253Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:init/1
    pid: <0.346.0>
    registered_name: 'service_agent-index'
    exception exit: {linked_process_died,<0.347.0>,
                        {no_connection,"index-service_api"}}
      in function  gen_server:terminate/6 (gen_server.erl, line 744)
    ancestors: [service_agent_children_sup,service_agent_sup,ns_server_sup,
                  ns_server_nodes_sup,<0.168.0>,ns_server_cluster_sup,
                  <0.89.0>]
    messages: []
    links: [<0.339.0>,<0.348.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 28690
    stack_size: 27
    reductions: 4580
  neighbours:

[error_logger:error,2019-07-04T11:56:59.254Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,service_agent_children_sup}
     Context:    child_terminated
     Reason:     {linked_process_died,<0.347.0>,
                                      {no_connection,"index-service_api"}}
     Offender:   [{pid,<0.346.0>},
                  {name,{service_agent,index}},
                  {mfargs,{service_agent,start_link,[index]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-07-04T11:56:59.254Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.3477.0>},
                       {name,{service_agent,index}},
                       {mfargs,{service_agent,start_link,[index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-07-04T11:56:59.271Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"index-cbauth",<0.600.0>} needs_update
[ns_server:debug,2019-07-04T11:56:59.274Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"projector-cbauth",<0.569.0>} needs_update
[ns_server:debug,2019-07-04T11:56:59.275Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"fts-cbauth",<0.727.0>} needs_update
[ns_server:debug,2019-07-04T11:56:59.277Z,ns_1@127.0.0.1:menelaus_cbauth<0.329.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"goxdcr-cbauth",<0.696.0>} needs_update
[ns_server:warn,2019-07-04T11:56:59.777Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:00.163Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            57,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              58,half_down,false}
[ns_server:warn,2019-07-04T11:57:01.284Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:01.287Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            58,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              59,half_down,false}
[ns_server:debug,2019-07-04T11:57:02.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            59,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              60,half_down,false}
[ns_server:warn,2019-07-04T11:57:02.792Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:03.162Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            60,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              61,half_down,false}
[ns_server:info,2019-07-04T11:57:03.636Z,ns_1@127.0.0.1:ns_memcached-app<0.658.0>:ns_memcached:ensure_bucket:1264]Created bucket "app" with config string "max_size=1042284544;dbname=/opt/couchbase/var/lib/couchbase/data/app;backend=couchdb;couch_bucket=app;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/app/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=5b6ce58456b7220bfb025f341cb20648;conflict_resolution_type=seqno;bucket_type=persistent;item_eviction_policy=value_only;max_ttl=0;ht_locks=47;compression_mode=off;failpartialwarmup=false"
[ns_server:info,2019-07-04T11:57:03.638Z,ns_1@127.0.0.1:ns_memcached-app<0.658.0>:ns_memcached:handle_cast:647]Main ns_memcached connection established: {ok,#Port<0.7256>}
[ns_server:debug,2019-07-04T11:57:03.643Z,ns_1@127.0.0.1:<0.3602.0>:janitor_agent:query_vbucket_states_loop_next_step:107]Waiting for "app" on 'ns_1@127.0.0.1'
[ns_server:debug,2019-07-04T11:57:03.646Z,ns_1@127.0.0.1:<0.2057.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 0, disk size is 0
[ns_server:debug,2019-07-04T11:57:03.646Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:57:03.646Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:57:03.648Z,ns_1@127.0.0.1:<0.3700.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:57:03.654Z,ns_1@127.0.0.1:<0.3702.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 0, disk size is 0
[ns_server:debug,2019-07-04T11:57:03.654Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:57:03.655Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[stats:warn,2019-07-04T11:57:03.661Z,ns_1@127.0.0.1:<0.475.0>:base_stats_collector:latest_tick:69](Collector: stats_collector) Dropped 59 ticks
[ns_server:warn,2019-07-04T11:57:03.793Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:warn,2019-07-04T11:57:03.888Z,ns_1@127.0.0.1:ns_heart<0.272.0>:ns_heart:handle_info:116]Dropped 1 heartbeats
[ns_server:debug,2019-07-04T11:57:04.177Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            61,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              62,half_down,false}
[ns_server:debug,2019-07-04T11:57:04.194Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.3793.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-service_api", handling process <0.3793.0>
[ns_server:debug,2019-07-04T11:57:04.196Z,ns_1@127.0.0.1:service_agent-index<0.3477.0>:service_agent:do_handle_connection:324]Observed new json rpc connection for index: <0.3793.0>
[ns_server:debug,2019-07-04T11:57:04.199Z,ns_1@127.0.0.1:<0.3480.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {json_rpc_events,<0.3478.0>} exited with reason normal
[ns_server:debug,2019-07-04T11:57:04.644Z,ns_1@127.0.0.1:<0.3602.0>:janitor_agent:query_vbucket_states_loop_next_step:107]Waiting for "app" on 'ns_1@127.0.0.1'
[ns_server:warn,2019-07-04T11:57:04.817Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:05.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            62,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              63,half_down,false}
[ns_server:debug,2019-07-04T11:57:05.692Z,ns_1@127.0.0.1:<0.3602.0>:janitor_agent:query_vbucket_states_loop_next_step:107]Waiting for "app" on 'ns_1@127.0.0.1'
[ns_server:warn,2019-07-04T11:57:05.833Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:info,2019-07-04T11:57:06.161Z,ns_1@127.0.0.1:<0.517.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:error,2019-07-04T11:57:06.161Z,ns_1@127.0.0.1:<0.3599.0>:janitor_agent:query_states_details:200]Failed to query vbucket states from some nodes:
[{'ns_1@127.0.0.1',warming_up}]
[ns_server:info,2019-07-04T11:57:06.161Z,ns_1@127.0.0.1:<0.3599.0>:ns_janitor:cleanup_with_states:130]Bucket "app" not yet ready on ['ns_1@127.0.0.1']
[ns_server:debug,2019-07-04T11:57:06.170Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            63,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              64,half_down,false}
[ns_server:warn,2019-07-04T11:57:06.835Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:07.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            64,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              65,half_down,false}
[ns_server:warn,2019-07-04T11:57:07.836Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:08.163Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            65,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              66,half_down,false}
[ns_server:warn,2019-07-04T11:57:08.839Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:09.162Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            66,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              67,half_down,false}
[ns_server:warn,2019-07-04T11:57:09.840Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:10.162Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            67,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              68,half_down,false}
[ns_server:warn,2019-07-04T11:57:10.843Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:11.150Z,ns_1@127.0.0.1:<0.4181.0>:janitor_agent:query_vbucket_states_loop_next_step:107]Waiting for "app" on 'ns_1@127.0.0.1'
[ns_server:debug,2019-07-04T11:57:11.162Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            68,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              69,half_down,false}
[ns_server:warn,2019-07-04T11:57:11.846Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:12.157Z,ns_1@127.0.0.1:<0.4181.0>:janitor_agent:query_vbucket_states_loop_next_step:107]Waiting for "app" on 'ns_1@127.0.0.1'
[user:info,2019-07-04T11:57:12.159Z,ns_1@127.0.0.1:ns_memcached-app<0.658.0>:ns_memcached:handle_cast:676]Bucket "app" loaded on node 'ns_1@127.0.0.1' in 8 seconds.
[ns_server:debug,2019-07-04T11:57:12.161Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            69,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              70,half_down,false}
[ns_server:warn,2019-07-04T11:57:12.891Z,ns_1@127.0.0.1:kv_monitor<0.550.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["app"]
[ns_server:debug,2019-07-04T11:57:13.165Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            70,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              71,half_down,false}
[ns_server:debug,2019-07-04T11:57:13.172Z,ns_1@127.0.0.1:janitor_agent-app<0.472.0>:dcp_sup:nuke:104]Nuking DCP replicators for bucket "app":
[]
[ns_server:info,2019-07-04T11:57:13.469Z,ns_1@127.0.0.1:ns_memcached-app<0.658.0>:ns_memcached:handle_call:298]Enabling traffic to bucket "app"
[ns_server:info,2019-07-04T11:57:13.470Z,ns_1@127.0.0.1:ns_memcached-app<0.658.0>:ns_memcached:handle_call:302]Bucket "app" marked as warmed in 9 seconds
[ns_server:info,2019-07-04T11:57:13.800Z,ns_1@127.0.0.1:ns_doctor<0.282.0>:ns_doctor:update_status:322]The following buckets became ready on node 'ns_1@127.0.0.1': ["app"]
[ns_server:debug,2019-07-04T11:57:14.181Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
            71,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>},
              72,half_down,false}
[ns_server:debug,2019-07-04T11:57:15.192Z,ns_1@127.0.0.1:<0.519.0>:auto_failover_logic:log_master_activity:170]Transitioned node {'ns_1@127.0.0.1',<<"2a8ae539a5cab1af4159b9b57e0098ee">>} state half_down -> up
[ns_server:debug,2019-07-04T11:57:29.108Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:57:29.117Z,ns_1@127.0.0.1:<0.5081.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:57:29.118Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:57:29.118Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:57:33.655Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:57:33.655Z,ns_1@127.0.0.1:<0.5291.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:57:33.657Z,ns_1@127.0.0.1:<0.5293.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 241702, disk size is 8435712
[ns_server:debug,2019-07-04T11:57:33.657Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:57:33.657Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:57:59.119Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:57:59.125Z,ns_1@127.0.0.1:<0.6528.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:57:59.126Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:57:59.126Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:58:03.659Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:58:03.660Z,ns_1@127.0.0.1:<0.6738.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:58:03.661Z,ns_1@127.0.0.1:<0.6740.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 241702, disk size is 8435712
[ns_server:debug,2019-07-04T11:58:03.662Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:58:03.662Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:58:29.128Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:58:29.133Z,ns_1@127.0.0.1:<0.7974.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:58:29.137Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:58:29.137Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:58:33.663Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:58:33.664Z,ns_1@127.0.0.1:<0.8183.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:58:33.666Z,ns_1@127.0.0.1:<0.8185.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 241702, disk size is 8435712
[ns_server:debug,2019-07-04T11:58:33.666Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:58:33.666Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:58:59.138Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:58:59.145Z,ns_1@127.0.0.1:<0.9424.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:58:59.147Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:58:59.147Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:59:03.666Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:59:03.667Z,ns_1@127.0.0.1:<0.9624.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:59:03.669Z,ns_1@127.0.0.1:<0.9626.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 241702, disk size is 8435712
[ns_server:debug,2019-07-04T11:59:03.669Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:59:03.669Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:59:29.147Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:59:29.160Z,ns_1@127.0.0.1:<0.10867.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:59:29.162Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:59:29.162Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:59:33.670Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:59:33.670Z,ns_1@127.0.0.1:<0.11070.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:59:33.672Z,ns_1@127.0.0.1:<0.11072.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 241702, disk size is 8435712
[ns_server:debug,2019-07-04T11:59:33.672Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:59:33.672Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T11:59:59.164Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T11:59:59.171Z,ns_1@127.0.0.1:<0.12318.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T11:59:59.172Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T11:59:59.172Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T12:00:03.673Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T12:00:03.673Z,ns_1@127.0.0.1:<0.12514.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T12:00:03.676Z,ns_1@127.0.0.1:<0.12516.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 241702, disk size is 8435712
[ns_server:debug,2019-07-04T12:00:03.676Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T12:00:03.676Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T12:00:29.174Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T12:00:29.179Z,ns_1@127.0.0.1:<0.13754.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T12:00:29.179Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T12:00:29.179Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T12:00:33.678Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T12:00:33.678Z,ns_1@127.0.0.1:<0.13946.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T12:00:33.680Z,ns_1@127.0.0.1:<0.13948.0>:compaction_new_daemon:bucket_needs_compaction:971]`app` data size is 241702, disk size is 8435712
[ns_server:debug,2019-07-04T12:00:33.681Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T12:00:33.681Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-07-04T12:00:59.180Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"app">>]
[ns_server:info,2019-07-04T12:00:59.187Z,ns_1@127.0.0.1:<0.15200.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket app with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-07-04T12:00:59.188Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-07-04T12:00:59.188Z,ns_1@127.0.0.1:compaction_new_daemon<0.446.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
